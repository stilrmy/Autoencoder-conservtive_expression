{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21abee7a-e86d-4999-a297-36f121b1dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "\n",
    "from sympy import symbols, simplify, derive_by_array\n",
    "from scipy.integrate import solve_ivp\n",
    "from xLSINDy_sp import *\n",
    "from sympy.physics.mechanics import *\n",
    "from sympy import *\n",
    "from Data_generator_py import image_process\n",
    "import sympy\n",
    "import torch\n",
    "import sys\n",
    "import HLsearch as HL\n",
    "import example_pendulum\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3931f0-8bba-44e2-8d56-b263f77bac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "environment = \"server\"\n",
    "sample_size = 10\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e864928-be31-4c73-b320-bfe0a2143ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding noise to the pendulum data\n",
      "noise_type: angle noise\n",
      "(4998, 2)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(r'../../../HLsearch/')\n",
    "#Saving Directory\n",
    "params = {}\n",
    "params['adding_noise'] = True\n",
    "params['noise_type'] = 'angle_noise'\n",
    "params['noiselevel'] = 1e-3\n",
    "if environment == 'laptop':\n",
    "    root_dir =R'C:\\Users\\87106\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'desktop':\n",
    "    root_dir = R'E:\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'server':\n",
    "    root_dir = R'/mnt/ssd1/stilrmy/Autoencoder-conservtive_expression'\n",
    "x,dx,ddx = image_process(sample_size,params)\n",
    "X = []\n",
    "Xdot = []\n",
    "for i in range(len(x)):\n",
    "    temp_list = [float(x[i]),float(dx[i])]\n",
    "    X.append(temp_list)\n",
    "    temp_list = [float(dx[i]),float(ddx[i])]\n",
    "    Xdot.append(temp_list)\n",
    "X = np.vstack(X)\n",
    "print(X.shape)\n",
    "Xdot = np.vstack(Xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8250a64-b76e-48f3-810e-28b181740486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states are: (x0, x0_t)\n",
      "states derivatives are:  (x0_t, x0_tt)\n"
     ]
    }
   ],
   "source": [
    "states_dim = 2\n",
    "states = ()\n",
    "states_dot = ()\n",
    "for i in range(states_dim):\n",
    "    if(i<states_dim//2):\n",
    "        states = states + (symbols('x{}'.format(i)),)\n",
    "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
    "    else:\n",
    "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
    "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
    "print('states are:',states)\n",
    "print('states derivatives are: ', states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c6dddc-ac3c-4e37-91dc-ac6bab075c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn from sympy to str\n",
    "states_sym = states\n",
    "states_dot_sym = states_dot\n",
    "states = list(str(descr) for descr in states)\n",
    "states_dot = list(str(descr) for descr in states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f71e4d-17ce-4af8-9d22-dc28cfc93f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'x0*x0_t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build function expression for the library in str\n",
    "expr= HL.buildFunctionExpressions(2,states_dim,states,use_sine=True)\n",
    "#expr=['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n",
    "\"a list of candidate function\"\n",
    "print(expr)\n",
    "expr.pop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ad60fa-1b07-423b-b96d-be7e84e9f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=True)\n",
    "Eta = Eta.to(device)\n",
    "Zeta = Zeta.to(device)\n",
    "Delta = Delta.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e559dd-c8b0-4041-b1f8-64e6124095be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(len(expr),device=device)\n",
    "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
    "prevxi_L = xi_L.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8134ecd-8d51-42b3-bd18-0a0f50901312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, targ):\n",
    "    loss = torch.mean((pred - targ)**2) \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c008f0b2-21b3-4433-afbf-8353931150dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(w, alpha):\n",
    "    clipped = torch.minimum(w,alpha)\n",
    "    clipped = torch.maximum(clipped,-alpha)\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16edbe1-038d-45e5-b403-07e08b441d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxL1norm(w_hat, alpha):\n",
    "    if(torch.is_tensor(alpha)==False):\n",
    "        alpha = torch.tensor(alpha)\n",
    "    w = w_hat - clip(w_hat,alpha)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8dffb9-95ba-4ef5-8987-6cc6a2e20a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(coef, prevcoef, Zeta, Eta, Delta,xdot, bs, lr, lam):\n",
    "    loss_list = []\n",
    "    tl = xdot.shape[0]\n",
    "    n = xdot.shape[1]\n",
    "    momentum = True\n",
    "    #if(torch.is_tensor(xdot)==False):\n",
    "        #xdot = torch.from_numpy(xdot).to(device).float()\n",
    "    v = coef.clone().detach().to(device).requires_grad_(True)\n",
    "    prev = prevcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    for i in range(tl//bs):\n",
    "        #computing acceleration with momentum\n",
    "        #if (momentum == True):\n",
    "            #vhat = (v + ((i - 1) / (i + 2)) * (v - prev)).clone().detach().requires_grad_(True)\n",
    "       # else:\n",
    "        vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "        prev = v\n",
    "        #Computing loss\n",
    "        zeta = Zeta[:,i*bs:(i+1)*bs]\n",
    "        eta = Eta[:,i*bs:(i+1)*bs]\n",
    "        delta = Delta[:,i*bs:(i+1)*bs]\n",
    "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
    "        #forward\n",
    "        q_tt_pred = lagrangianforward(vhat,zeta,eta,delta,x_t,device)\n",
    "        q_tt_true = xdot[i*bs:(i+1)*bs,n//2:].T\n",
    "        q_tt_true = torch.from_numpy(q_tt_true).to(device).float()\n",
    "        lossval = loss(q_tt_pred, q_tt_true)\n",
    "        lossval.requires_grad_(True)\n",
    "        #Backpropagation\n",
    "        lossval.backward()\n",
    "        with torch.no_grad():\n",
    "            v = vhat - lr * vhat.grad\n",
    "            v = proxL1norm(v, lr * lam)\n",
    "            # Manually zero the gradients after updating weights\n",
    "            vhat.grad = None\n",
    "        loss_list.append(lossval.item())\n",
    "    return v, prev, torch.tensor(loss_list).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9ace20-122c-4072-9ccf-9a3bd554abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 20/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  33.993282318115234\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 40/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  33.650028228759766\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 60/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  33.23601150512695\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 80/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  32.70953369140625\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 100/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  31.984634399414062\n"
     ]
    }
   ],
   "source": [
    "Epoch = 100\n",
    "i = 1\n",
    "lr = 1e-3\n",
    "lam = 0.1\n",
    "temp = 1000\n",
    "while(i<=Epoch):\n",
    "    xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "    if i %20 == 0:\n",
    "        print(\"\\n\")\n",
    "        print(\"Stage 1\")\n",
    "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "        print(\"Learning rate : \", lr)\n",
    "        print(\"Average loss : \" , lossitem)\n",
    "    if(temp <=5e-3):\n",
    "        break\n",
    "    if(temp <=1e-1):\n",
    "        lr = 1e-5\n",
    "    temp = lossitem\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04e39cf-6dac-45ba-8c38-8bfeaf4626b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "-1.27*x0 + -8.94*sin(x0) + 9.77*cos(x0) + 7.75*x0**2 + 4.11*x0_t**2 + -2.54*x0*sin(x0) + 5.63*x0_t*sin(x0) + -6.04*sin(x0)**2 + 7.87*x0*cos(x0) + -7.04*x0_t*cos(x0) + -8.01*sin(x0)*cos(x0) + -0.41*cos(x0)**2 + \n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-2\n",
    "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "## obtaining analytical model\n",
    "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
    "L = HL.generateExpression(xi_Lcpu,expr)\n",
    "print(len(xi_L))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbf84e-74af-4e42-a56c-c701b029dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  39.955589294433594\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  38.01542282104492\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  36.39560317993164\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  34.947471618652344\n",
      "expression length:\t 11\n",
      "Result stage 2: -1.056*x0 + -8.771*sin(x0) + 10.34*cos(x0) + 5.713*x0**2 + 4.757*x0_t**2 + -3.226*x0*sin(x0) + 5.554*x0_t*sin(x0) + -6.363*sin(x0)**2 + 7.815*x0*cos(x0) + -6.959*x0_t*cos(x0) + -7.881*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  33.64518356323242\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  32.34423828125\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  31.01140022277832\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  29.59052848815918\n",
      "expression length:\t 11\n",
      "Result stage 3: -0.93*x0 + -8.687*sin(x0) + 10.922*cos(x0) + 3.962*x0**2 + 4.432*x0_t**2 + -3.911*x0*sin(x0) + 5.554*x0_t*sin(x0) + -6.726*sin(x0)**2 + 7.837*x0*cos(x0) + -6.959*x0_t*cos(x0) + -7.837*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  27.96847152709961\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  26.043611526489258\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  23.60667610168457\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  20.282136917114258\n",
      "expression length:\t 11\n",
      "Result stage 4: -0.657*x0 + -8.487*sin(x0) + 11.474*cos(x0) + 1.965*x0**2 + 2.905*x0_t**2 + -4.546*x0*sin(x0) + 5.476*x0_t*sin(x0) + -7.01*sin(x0)**2 + 7.785*x0*cos(x0) + -6.881*x0_t*cos(x0) + -7.695*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  15.770705223083496\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  12.194419860839844\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  10.19188404083252\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  8.720636367797852\n",
      "expression length:\t 10\n",
      "Result stage 5: -7.987*sin(x0) + 11.996*cos(x0) + -0.243*x0**2 + 1.686*x0_t**2 + -4.903*x0*sin(x0) + 5.398*x0_t*sin(x0) + -6.993*sin(x0)**2 + 7.746*x0*cos(x0) + -6.803*x0_t*cos(x0) + -7.403*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  7.86993932723999\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  7.102006912231445\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  6.472408771514893\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  5.948307514190674\n",
      "expression length:\t 10\n",
      "Result stage 6: -7.467*sin(x0) + 12.25*cos(x0) + -1.542*x0**2 + 1.762*x0_t**2 + -4.922*x0*sin(x0) + 5.32*x0_t*sin(x0) + -6.782*sin(x0)**2 + 7.657*x0*cos(x0) + -6.725*x0_t*cos(x0) + -7.111*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  5.506164073944092\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  5.128710746765137\n"
     ]
    }
   ],
   "source": [
    "for stage in range(100):\n",
    "    \n",
    "    #Redefine computation after thresholding\n",
    "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=False)\n",
    "    Eta = Eta.to(device)\n",
    "    Zeta = Zeta.to(device)\n",
    "    Delta = Delta.to(device)\n",
    "\n",
    "    #Training\n",
    "    Epoch = 200\n",
    "    i = 1\n",
    "    lr = 1e-4\n",
    "    if(stage==1):\n",
    "        lam = 0\n",
    "    else:\n",
    "        lam = 0.1\n",
    "    temp = 1000\n",
    "    while(i<=Epoch):   \n",
    "        xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "        if i %50 == 0:\n",
    "            print(\"\\n\")\n",
    "            print(\"Stage \",stage)\n",
    "            print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "            print(\"Learning rate : \", lr)\n",
    "            print(\"Average loss : \" , lossitem)\n",
    "        temp = lossitem\n",
    "        if(temp <=1e-6):\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    ## Thresholding small indices ##\n",
    "    threshold = 1e-1\n",
    "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "    expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "    prevxi_L = xi_L.clone().detach()\n",
    "    mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "    ## obtaining analytical model\n",
    "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
    "    L = HL.generateExpression(xi_Lcpu,expr)\n",
    "    print(\"expression length:\\t\",len(xi_L))\n",
    "    print(\"Result stage \" + str(stage+2) + \":\" , L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda7f63-7451-4f1f-8bb8-6d994abdedee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8813673-d9ae-4769-9aa8-dc6b89ba4f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
