{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21abee7a-e86d-4999-a297-36f121b1dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "\n",
    "from sympy import symbols, simplify, derive_by_array\n",
    "from scipy.integrate import solve_ivp\n",
    "from xLSINDy_sp import *\n",
    "from sympy.physics.mechanics import *\n",
    "from sympy import *\n",
    "from Data_generator_py import image_process\n",
    "import sympy\n",
    "import torch\n",
    "import sys\n",
    "import HLsearch as HL\n",
    "import example_pendulum\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3931f0-8bba-44e2-8d56-b263f77bac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "environment = \"server\"\n",
    "sample_size = 10\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e864928-be31-4c73-b320-bfe0a2143ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding noise to the pendulum data\n",
      "noise_type: image noise\n",
      "(4998, 2)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(r'../../../HLsearch/')\n",
    "#Saving Directory\n",
    "params = {}\n",
    "params['adding_noise'] = True\n",
    "params['noise_type'] = 'image_noise'\n",
    "params['noiselevel'] = 6e-2\n",
    "params['changing_length'] = False\n",
    "if environment == 'laptop':\n",
    "    root_dir =R'C:\\Users\\87106\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'desktop':\n",
    "    root_dir = R'E:\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'server':\n",
    "    root_dir = R'/mnt/ssd1/stilrmy/Autoencoder-conservtive_expression'\n",
    "x,dx,ddx = image_process(sample_size,params)\n",
    "X = []\n",
    "Xdot = []\n",
    "for i in range(len(x)):\n",
    "    temp_list = [float(x[i]),float(dx[i])]\n",
    "    X.append(temp_list)\n",
    "    temp_list = [float(dx[i]),float(ddx[i])]\n",
    "    Xdot.append(temp_list)\n",
    "X = np.vstack(X)\n",
    "print(X.shape)\n",
    "Xdot = np.vstack(Xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8250a64-b76e-48f3-810e-28b181740486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states are: (x0, x0_t)\n",
      "states derivatives are:  (x0_t, x0_tt)\n"
     ]
    }
   ],
   "source": [
    "states_dim = 2\n",
    "states = ()\n",
    "states_dot = ()\n",
    "for i in range(states_dim):\n",
    "    if(i<states_dim//2):\n",
    "        states = states + (symbols('x{}'.format(i)),)\n",
    "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
    "    else:\n",
    "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
    "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
    "print('states are:',states)\n",
    "print('states derivatives are: ', states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c6dddc-ac3c-4e37-91dc-ac6bab075c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn from sympy to str\n",
    "states_sym = states\n",
    "states_dot_sym = states_dot\n",
    "states = list(str(descr) for descr in states)\n",
    "states_dot = list(str(descr) for descr in states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f71e4d-17ce-4af8-9d22-dc28cfc93f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'x0*x0_t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build function expression for the library in str\n",
    "expr= HL.buildFunctionExpressions(2,states_dim,states,use_sine=True)\n",
    "#expr=['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n",
    "\"a list of candidate function\"\n",
    "print(expr)\n",
    "expr.pop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ad60fa-1b07-423b-b96d-be7e84e9f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=True)\n",
    "Eta = Eta.to(device)\n",
    "Zeta = Zeta.to(device)\n",
    "Delta = Delta.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e559dd-c8b0-4041-b1f8-64e6124095be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(len(expr),device=device)\n",
    "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
    "prevxi_L = xi_L.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8134ecd-8d51-42b3-bd18-0a0f50901312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, targ):\n",
    "    loss = torch.mean((pred - targ)**2) \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c008f0b2-21b3-4433-afbf-8353931150dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(w, alpha):\n",
    "    clipped = torch.minimum(w,alpha)\n",
    "    clipped = torch.maximum(clipped,-alpha)\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16edbe1-038d-45e5-b403-07e08b441d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxL1norm(w_hat, alpha):\n",
    "    if(torch.is_tensor(alpha)==False):\n",
    "        alpha = torch.tensor(alpha)\n",
    "    w = w_hat - clip(w_hat,alpha)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8dffb9-95ba-4ef5-8987-6cc6a2e20a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(coef, prevcoef, Zeta, Eta, Delta,xdot, bs, lr, lam):\n",
    "    loss_list = []\n",
    "    tl = xdot.shape[0]\n",
    "    n = xdot.shape[1]\n",
    "    momentum = True\n",
    "    #if(torch.is_tensor(xdot)==False):\n",
    "        #xdot = torch.from_numpy(xdot).to(device).float()\n",
    "    v = coef.clone().detach().to(device).requires_grad_(True)\n",
    "    prev = prevcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    for i in range(tl//bs):\n",
    "        #computing acceleration with momentum\n",
    "        #if (momentum == True):\n",
    "        vhat = (v + ((i - 1) / (i + 2)) * (v - prev)).clone().detach().requires_grad_(True)\n",
    "       # else:\n",
    "        #vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "        prev = v\n",
    "        #Computing loss\n",
    "        zeta = Zeta[:,i*bs:(i+1)*bs]\n",
    "        eta = Eta[:,i*bs:(i+1)*bs]\n",
    "        delta = Delta[:,i*bs:(i+1)*bs]\n",
    "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
    "        #forward\n",
    "        q_tt_pred = lagrangianforward(vhat,zeta,eta,delta,x_t,device)\n",
    "        q_tt_true = xdot[i*bs:(i+1)*bs,n//2:].T\n",
    "        q_tt_true = torch.from_numpy(q_tt_true).to(device).float()\n",
    "        lossval = loss(q_tt_pred, q_tt_true)\n",
    "        lossval.requires_grad_(True)\n",
    "        #Backpropagation\n",
    "        lossval.backward()\n",
    "        with torch.no_grad():\n",
    "            v = vhat - lr * vhat.grad\n",
    "            v = proxL1norm(v, lr * lam)\n",
    "            # Manually zero the gradients after updating weights\n",
    "            vhat.grad = None\n",
    "        loss_list.append(lossval.item())\n",
    "    return v, prev, torch.tensor(loss_list).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9ace20-122c-4072-9ccf-9a3bd554abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 20/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  33.46943664550781\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 40/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  14.967374801635742\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 60/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  3.551048517227173\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 80/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  1.5058364868164062\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 100/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  0.7048614621162415\n"
     ]
    }
   ],
   "source": [
    "Epoch = 100\n",
    "i = 1\n",
    "lr = 1e-3\n",
    "lam = 0.1\n",
    "temp = 1000\n",
    "while(i<=Epoch):\n",
    "    xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "    if i %20 == 0:\n",
    "        print(\"\\n\")\n",
    "        print(\"Stage 1\")\n",
    "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "        print(\"Learning rate : \", lr)\n",
    "        print(\"Average loss : \" , lossitem)\n",
    "    if(temp <=5e-3):\n",
    "        break\n",
    "    if(temp <=1e-1):\n",
    "        lr = 1e-5\n",
    "    temp = lossitem\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04e39cf-6dac-45ba-8c38-8bfeaf4626b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "2.55*x0 + -6.7*sin(x0) + 15.22*cos(x0) + 0.49*x0_t**2 + -3.21*x0*sin(x0) + -2.53*sin(x0)**2 + 5.86*x0*cos(x0) + -2.4*sin(x0)*cos(x0) + -1.68*cos(x0)**2 + \n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-2\n",
    "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "## obtaining analytical model\n",
    "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
    "L = HL.generateExpression(xi_Lcpu,expr)\n",
    "print(len(xi_L))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbf84e-74af-4e42-a56c-c701b029dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  3.1384382247924805\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.909999132156372\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.700361490249634\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.509204387664795\n",
      "expression length:\t 9\n",
      "Result stage 2: 2.886*x0 + -6.666*sin(x0) + 15.255*cos(x0) + 1.097*x0_t**2 + -3.103*x0*sin(x0) + -2.397*sin(x0)**2 + 5.305*x0*cos(x0) + -2.539*sin(x0)*cos(x0) + -1.74*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.3345205783843994\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.1749770641326904\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.0306146144866943\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.9000821113586426\n",
      "expression length:\t 9\n",
      "Result stage 3: 3.209*x0 + -6.658*sin(x0) + 15.327*cos(x0) + 1.076*x0_t**2 + -3.05*x0*sin(x0) + -2.319*sin(x0)**2 + 4.898*x0*cos(x0) + -2.662*sin(x0)*cos(x0) + -1.818*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.7815039157867432\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.6747723817825317\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.5781705379486084\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.4908510446548462\n",
      "expression length:\t 9\n",
      "Result stage 4: 3.451*x0 + -6.601*sin(x0) + 15.339*cos(x0) + 1.053*x0_t**2 + -2.945*x0*sin(x0) + -2.204*sin(x0)**2 + 4.531*x0*cos(x0) + -2.705*sin(x0)*cos(x0) + -1.853*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.411916971206665\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.3408373594284058\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.276580572128296\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.2189420461654663\n",
      "expression length:\t 9\n",
      "Result stage 5: 3.652*x0 + -6.539*sin(x0) + 15.332*cos(x0) + 1.035*x0_t**2 + -2.839*x0*sin(x0) + -2.098*sin(x0)**2 + 4.231*x0*cos(x0) + -2.713*sin(x0)*cos(x0) + -1.879*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1670074462890625\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1203899383544922\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0782976150512695\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.040579915046692\n",
      "expression length:\t 9\n",
      "Result stage 6: 3.816*x0 + -6.478*sin(x0) + 15.312*cos(x0) + 1.02*x0_t**2 + -2.735*x0*sin(x0) + -2.002*sin(x0)**2 + 3.986*x0*cos(x0) + -2.697*sin(x0)*cos(x0) + -1.895*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.00676691532135\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.976500391960144\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9494109749794006\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9251248240470886\n",
      "expression length:\t 9\n",
      "Result stage 7: 3.945*x0 + -6.42*sin(x0) + 15.283*cos(x0) + 1.008*x0_t**2 + -2.637*x0*sin(x0) + -1.918*sin(x0)**2 + 3.786*x0*cos(x0) + -2.662*sin(x0)*cos(x0) + -1.899*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9032799005508423\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.883686363697052\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8660861253738403\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8503026962280273\n",
      "expression length:\t 9\n",
      "Result stage 8: 4.045*x0 + -6.363*sin(x0) + 15.25*cos(x0) + 0.997*x0_t**2 + -2.546*x0*sin(x0) + -1.843*sin(x0)**2 + 3.62*x0*cos(x0) + -2.614*sin(x0)*cos(x0) + -1.894*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8361607789993286\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8234235048294067\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8121591806411743\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8020401000976562\n",
      "expression length:\t 9\n",
      "Result stage 9: 4.118*x0 + -6.31*sin(x0) + 15.217*cos(x0) + 0.988*x0_t**2 + -2.461*x0*sin(x0) + -1.778*sin(x0)**2 + 3.48*x0*cos(x0) + -2.556*sin(x0)*cos(x0) + -1.879*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7929316163063049\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7846226692199707\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7770897746086121\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7703570127487183\n",
      "expression length:\t 9\n",
      "Result stage 10: 4.17*x0 + -6.261*sin(x0) + 15.184*cos(x0) + 0.98*x0_t**2 + -2.383*x0*sin(x0) + -1.72*sin(x0)**2 + 3.361*x0*cos(x0) + -2.491*sin(x0)*cos(x0) + -1.857*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7642215490341187\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7586765885353088\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7536385655403137\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7489467263221741\n",
      "expression length:\t 9\n",
      "Result stage 11: 4.204*x0 + -6.216*sin(x0) + 15.15*cos(x0) + 0.973*x0_t**2 + -2.312*x0*sin(x0) + -1.67*sin(x0)**2 + 3.258*x0*cos(x0) + -2.422*sin(x0)*cos(x0) + -1.827*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7445986866950989\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7406685948371887\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7370497584342957\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7337284684181213\n",
      "expression length:\t 9\n",
      "Result stage 12: 4.223*x0 + -6.174*sin(x0) + 15.117*cos(x0) + 0.966*x0_t**2 + -2.245*x0*sin(x0) + -1.625*sin(x0)**2 + 3.166*x0*cos(x0) + -2.351*sin(x0)*cos(x0) + -1.792*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7305493354797363\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.727558434009552\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7247233986854553\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7220397591590881\n",
      "expression length:\t 9\n",
      "Result stage 13: 4.231*x0 + -6.134*sin(x0) + 15.083*cos(x0) + 0.96*x0_t**2 + -2.182*x0*sin(x0) + -1.585*sin(x0)**2 + 3.085*x0*cos(x0) + -2.277*sin(x0)*cos(x0) + -1.752*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7194874882698059\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7170402407646179\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7146936058998108\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7124795913696289\n",
      "expression length:\t 9\n",
      "Result stage 14: 4.231*x0 + -6.096*sin(x0) + 15.048*cos(x0) + 0.955*x0_t**2 + -2.123*x0*sin(x0) + -1.549*sin(x0)**2 + 3.01*x0*cos(x0) + -2.203*sin(x0)*cos(x0) + -1.708*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7103350162506104\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7082703709602356\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7062663435935974\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7043168544769287\n",
      "expression length:\t 9\n",
      "Result stage 15: 4.222*x0 + -6.062*sin(x0) + 15.011*cos(x0) + 0.95*x0_t**2 + -2.067*x0*sin(x0) + -1.516*sin(x0)**2 + 2.94*x0*cos(x0) + -2.129*sin(x0)*cos(x0) + -1.661*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7024014592170715\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7005324363708496\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6986891627311707\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6968669295310974\n",
      "expression length:\t 9\n",
      "Result stage 16: 4.206*x0 + -6.028*sin(x0) + 14.974*cos(x0) + 0.945*x0_t**2 + -2.013*x0*sin(x0) + -1.485*sin(x0)**2 + 2.876*x0*cos(x0) + -2.055*sin(x0)*cos(x0) + -1.612*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6950728297233582\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6933001279830933\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6915463805198669\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6898221373558044\n",
      "expression length:\t 9\n",
      "Result stage 17: 4.187*x0 + -5.995*sin(x0) + 14.937*cos(x0) + 0.941*x0_t**2 + -1.962*x0*sin(x0) + -1.458*sin(x0)**2 + 2.814*x0*cos(x0) + -1.981*sin(x0)*cos(x0) + -1.56*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.688122570514679\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.686447262763977\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.684798538684845\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.683161735534668\n",
      "expression length:\t 9\n",
      "Result stage 18: 4.166*x0 + -5.962*sin(x0) + 14.901*cos(x0) + 0.937*x0_t**2 + -1.912*x0*sin(x0) + -1.431*sin(x0)**2 + 2.754*x0*cos(x0) + -1.909*sin(x0)*cos(x0) + -1.507*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6815478205680847\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6799466013908386\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6783730387687683\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6768251061439514\n",
      "expression length:\t 9\n",
      "Result stage 19: 4.143*x0 + -5.93*sin(x0) + 14.864*cos(x0) + 0.932*x0_t**2 + -1.864*x0*sin(x0) + -1.405*sin(x0)**2 + 2.696*x0*cos(x0) + -1.837*sin(x0)*cos(x0) + -1.453*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6752864122390747\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.673762857913971\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6722753047943115\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6707995533943176\n",
      "expression length:\t 9\n",
      "Result stage 20: 4.119*x0 + -5.897*sin(x0) + 14.825*cos(x0) + 0.928*x0_t**2 + -1.817*x0*sin(x0) + -1.38*sin(x0)**2 + 2.641*x0*cos(x0) + -1.766*sin(x0)*cos(x0) + -1.398*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.669343113899231\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6678994297981262\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6664733290672302\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6650645136833191\n",
      "expression length:\t 9\n",
      "Result stage 21: 4.092*x0 + -5.865*sin(x0) + 14.785*cos(x0) + 0.924*x0_t**2 + -1.771*x0*sin(x0) + -1.355*sin(x0)**2 + 2.586*x0*cos(x0) + -1.696*sin(x0)*cos(x0) + -1.342*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6636787056922913\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6623055934906006\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6609510779380798\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6596060991287231\n",
      "expression length:\t 9\n",
      "Result stage 22: 4.064*x0 + -5.834*sin(x0) + 14.746*cos(x0) + 0.92*x0_t**2 + -1.726*x0*sin(x0) + -1.331*sin(x0)**2 + 2.533*x0*cos(x0) + -1.628*sin(x0)*cos(x0) + -1.286*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.65827476978302\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6569642424583435\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6556572318077087\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6543684005737305\n",
      "expression length:\t 9\n",
      "Result stage 23: 4.036*x0 + -5.803*sin(x0) + 14.706*cos(x0) + 0.916*x0_t**2 + -1.682*x0*sin(x0) + -1.308*sin(x0)**2 + 2.48*x0*cos(x0) + -1.56*sin(x0)*cos(x0) + -1.229*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6531087756156921\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6518574357032776\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6506240963935852\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6493995785713196\n",
      "expression length:\t 9\n",
      "Result stage 24: 4.006*x0 + -5.771*sin(x0) + 14.67*cos(x0) + 0.913*x0_t**2 + -1.638*x0*sin(x0) + -1.285*sin(x0)**2 + 2.429*x0*cos(x0) + -1.494*sin(x0)*cos(x0) + -1.172*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6481900215148926\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6470045447349548\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6458345055580139\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.644676685333252\n",
      "expression length:\t 9\n",
      "Result stage 25: 3.975*x0 + -5.74*sin(x0) + 14.633*cos(x0) + 0.909*x0_t**2 + -1.596*x0*sin(x0) + -1.263*sin(x0)**2 + 2.378*x0*cos(x0) + -1.429*sin(x0)*cos(x0) + -1.114*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6435409784317017\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6424120664596558\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.641304612159729\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6402191519737244\n",
      "expression length:\t 9\n",
      "Result stage 26: 3.943*x0 + -5.707*sin(x0) + 14.596*cos(x0) + 0.906*x0_t**2 + -1.554*x0*sin(x0) + -1.241*sin(x0)**2 + 2.328*x0*cos(x0) + -1.365*sin(x0)*cos(x0) + -1.057*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6391499638557434\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6381099820137024\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6370857954025269\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6360725164413452\n",
      "expression length:\t 9\n",
      "Result stage 27: 3.912*x0 + -5.674*sin(x0) + 14.558*cos(x0) + 0.902*x0_t**2 + -1.513*x0*sin(x0) + -1.219*sin(x0)**2 + 2.279*x0*cos(x0) + -1.303*sin(x0)*cos(x0) + -0.999*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.635061502456665\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6340686082839966\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.633100152015686\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6321567893028259\n",
      "expression length:\t 9\n",
      "Result stage 28: 3.881*x0 + -5.64*sin(x0) + 14.519*cos(x0) + 0.899*x0_t**2 + -1.472*x0*sin(x0) + -1.196*sin(x0)**2 + 2.231*x0*cos(x0) + -1.241*sin(x0)*cos(x0) + -0.939*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6312334537506104\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6303304433822632\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6294335722923279\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.628567099571228\n",
      "expression length:\t 9\n",
      "Result stage 29: 3.849*x0 + -5.604*sin(x0) + 14.48*cos(x0) + 0.895*x0_t**2 + -1.431*x0*sin(x0) + -1.173*sin(x0)**2 + 2.182*x0*cos(x0) + -1.181*sin(x0)*cos(x0) + -0.879*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6277304291725159\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.626904308795929\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6260936856269836\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6253030300140381\n",
      "expression length:\t 9\n",
      "Result stage 30: 3.818*x0 + -5.567*sin(x0) + 14.441*cos(x0) + 0.892*x0_t**2 + -1.391*x0*sin(x0) + -1.151*sin(x0)**2 + 2.133*x0*cos(x0) + -1.123*sin(x0)*cos(x0) + -0.819*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6245243549346924\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6237631440162659\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6230128407478333\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6222748160362244\n",
      "expression length:\t 9\n",
      "Result stage 31: 3.784*x0 + -5.53*sin(x0) + 14.403*cos(x0) + 0.888*x0_t**2 + -1.352*x0*sin(x0) + -1.128*sin(x0)**2 + 2.085*x0*cos(x0) + -1.066*sin(x0)*cos(x0) + -0.759*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6215581893920898\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6208544373512268\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6201690435409546\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194937825202942\n",
      "expression length:\t 9\n",
      "Result stage 32: 3.751*x0 + -5.493*sin(x0) + 14.365*cos(x0) + 0.885*x0_t**2 + -1.312*x0*sin(x0) + -1.104*sin(x0)**2 + 2.038*x0*cos(x0) + -1.01*sin(x0)*cos(x0) + -0.7*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6188215613365173\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6181458830833435\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6174971461296082\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6168573498725891\n",
      "expression length:\t 9\n",
      "Result stage 33: 3.717*x0 + -5.457*sin(x0) + 14.327*cos(x0) + 0.882*x0_t**2 + -1.274*x0*sin(x0) + -1.081*sin(x0)**2 + 1.99*x0*cos(x0) + -0.952*sin(x0)*cos(x0) + -0.641*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6162324547767639\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6156243681907654\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.615045964717865\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6144760251045227\n",
      "expression length:\t 9\n",
      "Result stage 34: 3.684*x0 + -5.421*sin(x0) + 14.289*cos(x0) + 0.878*x0_t**2 + -1.236*x0*sin(x0) + -1.057*sin(x0)**2 + 1.943*x0*cos(x0) + -0.896*sin(x0)*cos(x0) + -0.582*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6139199137687683\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.613380491733551\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6128551363945007\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6123452186584473\n",
      "expression length:\t 9\n",
      "Result stage 35: 3.652*x0 + -5.385*sin(x0) + 14.251*cos(x0) + 0.875*x0_t**2 + -1.198*x0*sin(x0) + -1.033*sin(x0)**2 + 1.897*x0*cos(x0) + -0.841*sin(x0)*cos(x0) + -0.523*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6118537187576294\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6113698482513428\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6108999252319336\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6104469895362854\n",
      "expression length:\t 9\n",
      "Result stage 36: 3.62*x0 + -5.349*sin(x0) + 14.214*cos(x0) + 0.872*x0_t**2 + -1.161*x0*sin(x0) + -1.009*sin(x0)**2 + 1.851*x0*cos(x0) + -0.787*sin(x0)*cos(x0) + -0.465*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6100113391876221\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6095952987670898\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6092023849487305\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.608820378780365\n",
      "expression length:\t 9\n",
      "Result stage 37: 3.587*x0 + -5.311*sin(x0) + 14.175*cos(x0) + 0.869*x0_t**2 + -1.124*x0*sin(x0) + -0.983*sin(x0)**2 + 1.806*x0*cos(x0) + -0.734*sin(x0)*cos(x0) + -0.406*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6084561347961426\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6081010103225708\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6077562570571899\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6074216365814209\n",
      "expression length:\t 9\n",
      "Result stage 38: 3.555*x0 + -5.273*sin(x0) + 14.135*cos(x0) + 0.865*x0_t**2 + -1.088*x0*sin(x0) + -0.956*sin(x0)**2 + 1.761*x0*cos(x0) + -0.682*sin(x0)*cos(x0) + -0.348*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6071035265922546\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6068007349967957\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6065077781677246\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6062308549880981\n",
      "expression length:\t 9\n",
      "Result stage 39: 3.521*x0 + -5.235*sin(x0) + 14.095*cos(x0) + 0.862*x0_t**2 + -1.052*x0*sin(x0) + -0.93*sin(x0)**2 + 1.717*x0*cos(x0) + -0.632*sin(x0)*cos(x0) + -0.29*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6059637665748596\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6057097315788269\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6054726839065552\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6052465438842773\n",
      "expression length:\t 9\n",
      "Result stage 40: 3.488*x0 + -5.196*sin(x0) + 14.055*cos(x0) + 0.858*x0_t**2 + -1.017*x0*sin(x0) + -0.903*sin(x0)**2 + 1.673*x0*cos(x0) + -0.582*sin(x0)*cos(x0) + -0.232*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6050345301628113\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6048345565795898\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6046514511108398\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.604482114315033\n",
      "expression length:\t 8\n",
      "Result stage 41: 3.456*x0 + -5.157*sin(x0) + 14.016*cos(x0) + 0.855*x0_t**2 + -0.98*x0*sin(x0) + -0.876*sin(x0)**2 + 1.63*x0*cos(x0) + -0.534*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6032223105430603\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6030030846595764\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6028570532798767\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6027833819389343\n",
      "expression length:\t 8\n",
      "Result stage 42: 3.424*x0 + -5.12*sin(x0) + 13.967*cos(x0) + 0.854*x0_t**2 + -0.936*x0*sin(x0) + -0.837*sin(x0)**2 + 1.582*x0*cos(x0) + -0.487*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6027651429176331\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6027975678443909\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6028838753700256\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6030111908912659\n",
      "expression length:\t 8\n",
      "Result stage 43: 3.393*x0 + -5.08*sin(x0) + 13.925*cos(x0) + 0.848*x0_t**2 + -0.896*x0*sin(x0) + -0.805*sin(x0)**2 + 1.539*x0*cos(x0) + -0.44*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6031748652458191\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6033720374107361\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.603608250617981\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6038637161254883\n",
      "expression length:\t 8\n",
      "Result stage 44: 3.36*x0 + -5.039*sin(x0) + 13.887*cos(x0) + 0.843*x0_t**2 + -0.861*x0*sin(x0) + -0.778*sin(x0)**2 + 1.499*x0*cos(x0) + -0.394*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6041489839553833\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6044577956199646\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6047849655151367\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.605126142501831\n",
      "expression length:\t 8\n",
      "Result stage 45: 3.326*x0 + -4.998*sin(x0) + 13.851*cos(x0) + 0.838*x0_t**2 + -0.83*x0*sin(x0) + -0.756*sin(x0)**2 + 1.461*x0*cos(x0) + -0.349*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6054835915565491\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6058538556098938\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6062321066856384\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6066209077835083\n",
      "expression length:\t 8\n",
      "Result stage 46: 3.291*x0 + -4.954*sin(x0) + 13.81*cos(x0) + 0.834*x0_t**2 + -0.803*x0*sin(x0) + -0.737*sin(x0)**2 + 1.424*x0*cos(x0) + -0.306*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6070220470428467\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6074296832084656\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6078451871871948\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6082548499107361\n",
      "expression length:\t 8\n",
      "Result stage 47: 3.256*x0 + -4.91*sin(x0) + 13.767*cos(x0) + 0.83*x0_t**2 + -0.778*x0*sin(x0) + -0.722*sin(x0)**2 + 1.388*x0*cos(x0) + -0.264*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6086671352386475\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6090784668922424\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6094944477081299\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.609910249710083\n",
      "expression length:\t 8\n",
      "Result stage 48: 3.22*x0 + -4.865*sin(x0) + 13.721*cos(x0) + 0.825*x0_t**2 + -0.756*x0*sin(x0) + -0.71*sin(x0)**2 + 1.352*x0*cos(x0) + -0.224*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6103286147117615\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6107419729232788\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6111481189727783\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6115556359291077\n",
      "expression length:\t 7\n",
      "Result stage 49: 3.184*x0 + -4.818*sin(x0) + 13.677*cos(x0) + 0.821*x0_t**2 + -0.736*x0*sin(x0) + -0.701*sin(x0)**2 + 1.316*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190362572669983\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6183930039405823\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6179836392402649\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6177639365196228\n",
      "expression length:\t 7\n",
      "Result stage 50: 3.141*x0 + -4.787*sin(x0) + 13.632*cos(x0) + 0.818*x0_t**2 + -0.718*x0*sin(x0) + -0.693*sin(x0)**2 + 1.252*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6176618337631226\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6176490187644958\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6176873445510864\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6177797317504883\n",
      "expression length:\t 7\n",
      "Result stage 51: 3.104*x0 + -4.745*sin(x0) + 13.584*cos(x0) + 0.814*x0_t**2 + -0.703*x0*sin(x0) + -0.688*sin(x0)**2 + 1.204*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.617890477180481\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6180295348167419\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6181662678718567\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6183051466941833\n",
      "expression length:\t 7\n",
      "Result stage 52: 3.07*x0 + -4.698*sin(x0) + 13.538*cos(x0) + 0.811*x0_t**2 + -0.69*x0*sin(x0) + -0.686*sin(x0)**2 + 1.167*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6184365749359131\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.618561863899231\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6186707615852356\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6187692880630493\n",
      "expression length:\t 7\n",
      "Result stage 53: 3.036*x0 + -4.647*sin(x0) + 13.495*cos(x0) + 0.808*x0_t**2 + -0.679*x0*sin(x0) + -0.685*sin(x0)**2 + 1.135*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6188695430755615\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6189610958099365\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190389394760132\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191132068634033\n",
      "expression length:\t 7\n",
      "Result stage 54: 3.003*x0 + -4.594*sin(x0) + 13.453*cos(x0) + 0.805*x0_t**2 + -0.669*x0*sin(x0) + -0.685*sin(x0)**2 + 1.108*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191832423210144\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192365288734436\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192854642868042\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193187832832336\n",
      "expression length:\t 7\n",
      "Result stage 55: 2.97*x0 + -4.539*sin(x0) + 13.412*cos(x0) + 0.802*x0_t**2 + -0.66*x0*sin(x0) + -0.686*sin(x0)**2 + 1.083*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193539500236511\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619385838508606\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194084882736206\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194239854812622\n",
      "expression length:\t 7\n",
      "Result stage 56: 2.935*x0 + -4.485*sin(x0) + 13.37*cos(x0) + 0.799*x0_t**2 + -0.651*x0*sin(x0) + -0.688*sin(x0)**2 + 1.059*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194525957107544\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194591522216797\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194658279418945\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619480550289154\n",
      "expression length:\t 7\n",
      "Result stage 57: 2.901*x0 + -4.43*sin(x0) + 13.329*cos(x0) + 0.797*x0_t**2 + -0.643*x0*sin(x0) + -0.689*sin(x0)**2 + 1.037*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194804310798645\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619478166103363\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194778084754944\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194818019866943\n",
      "expression length:\t 7\n",
      "Result stage 58: 2.867*x0 + -4.376*sin(x0) + 13.287*cos(x0) + 0.794*x0_t**2 + -0.634*x0*sin(x0) + -0.691*sin(x0)**2 + 1.015*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194721460342407\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194767355918884\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194673180580139\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194788217544556\n",
      "expression length:\t 7\n",
      "Result stage 59: 2.834*x0 + -4.321*sin(x0) + 13.245*cos(x0) + 0.791*x0_t**2 + -0.626*x0*sin(x0) + -0.693*sin(x0)**2 + 0.993*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619482696056366\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194762587547302\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194786429405212\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194841265678406\n",
      "expression length:\t 7\n",
      "Result stage 60: 2.8*x0 + -4.266*sin(x0) + 13.203*cos(x0) + 0.788*x0_t**2 + -0.618*x0*sin(x0) + -0.695*sin(x0)**2 + 0.971*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.61948561668396\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194815635681152\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619475245475769\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194734573364258\n",
      "expression length:\t 7\n",
      "Result stage 61: 2.766*x0 + -4.212*sin(x0) + 13.161*cos(x0) + 0.785*x0_t**2 + -0.61*x0*sin(x0) + -0.697*sin(x0)**2 + 0.95*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194673776626587\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194499135017395\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194540858268738\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194554567337036\n",
      "expression length:\t 7\n",
      "Result stage 62: 2.732*x0 + -4.157*sin(x0) + 13.118*cos(x0) + 0.783*x0_t**2 + -0.601*x0*sin(x0) + -0.699*sin(x0)**2 + 0.929*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194397807121277\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194353103637695\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194227337837219\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194005608558655\n",
      "expression length:\t 7\n",
      "Result stage 63: 2.698*x0 + -4.103*sin(x0) + 13.076*cos(x0) + 0.78*x0_t**2 + -0.593*x0*sin(x0) + -0.701*sin(x0)**2 + 0.908*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193827390670776\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619368314743042\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193573474884033\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193374991416931\n",
      "expression length:\t 7\n",
      "Result stage 64: 2.664*x0 + -4.049*sin(x0) + 13.034*cos(x0) + 0.777*x0_t**2 + -0.585*x0*sin(x0) + -0.703*sin(x0)**2 + 0.888*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193224191665649\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193044185638428\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192960739135742\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192696690559387\n",
      "expression length:\t 7\n",
      "Result stage 65: 2.63*x0 + -3.995*sin(x0) + 12.991*cos(x0) + 0.774*x0_t**2 + -0.577*x0*sin(x0) + -0.705*sin(x0)**2 + 0.868*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192439198493958\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192277669906616\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192104816436768\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191950440406799\n",
      "expression length:\t 7\n",
      "Result stage 66: 2.596*x0 + -3.938*sin(x0) + 12.948*cos(x0) + 0.771*x0_t**2 + -0.569*x0*sin(x0) + -0.707*sin(x0)**2 + 0.848*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191806197166443\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191584467887878\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191504001617432\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191313862800598\n",
      "expression length:\t 7\n",
      "Result stage 67: 2.561*x0 + -3.882*sin(x0) + 12.906*cos(x0) + 0.768*x0_t**2 + -0.56*x0*sin(x0) + -0.709*sin(x0)**2 + 0.828*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191250085830688\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191127300262451\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190959215164185\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190710663795471\n",
      "expression length:\t 7\n",
      "Result stage 68: 2.526*x0 + -3.825*sin(x0) + 12.863*cos(x0) + 0.765*x0_t**2 + -0.552*x0*sin(x0) + -0.711*sin(x0)**2 + 0.807*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619071364402771\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190593838691711\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190536022186279\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190420389175415\n",
      "expression length:\t 7\n",
      "Result stage 69: 2.492*x0 + -3.769*sin(x0) + 12.82*cos(x0) + 0.763*x0_t**2 + -0.543*x0*sin(x0) + -0.713*sin(x0)**2 + 0.787*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190353035926819\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190415024757385\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190338134765625\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190384030342102\n",
      "expression length:\t 7\n",
      "Result stage 70: 2.457*x0 + -3.714*sin(x0) + 12.777*cos(x0) + 0.76*x0_t**2 + -0.535*x0*sin(x0) + -0.715*sin(x0)**2 + 0.767*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190274953842163\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190233826637268\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190141439437866\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190075278282166\n",
      "expression length:\t 7\n",
      "Result stage 71: 2.422*x0 + -3.658*sin(x0) + 12.735*cos(x0) + 0.757*x0_t**2 + -0.526*x0*sin(x0) + -0.717*sin(x0)**2 + 0.747*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190173029899597\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190149188041687\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190180778503418\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190120577812195\n",
      "expression length:\t 7\n",
      "Result stage 72: 2.387*x0 + -3.603*sin(x0) + 12.692*cos(x0) + 0.754*x0_t**2 + -0.517*x0*sin(x0) + -0.719*sin(x0)**2 + 0.727*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190090179443359\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190042495727539\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190028786659241\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190118193626404\n",
      "expression length:\t 7\n",
      "Result stage 73: 2.353*x0 + -3.548*sin(x0) + 12.647*cos(x0) + 0.751*x0_t**2 + -0.509*x0*sin(x0) + -0.721*sin(x0)**2 + 0.707*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190124750137329\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190134882926941\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190239787101746\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190285086631775\n",
      "expression length:\t 7\n",
      "Result stage 74: 2.318*x0 + -3.493*sin(x0) + 12.603*cos(x0) + 0.748*x0_t**2 + -0.5*x0*sin(x0) + -0.722*sin(x0)**2 + 0.686*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619041919708252\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190425157546997\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190468072891235\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190511584281921\n",
      "expression length:\t 7\n",
      "Result stage 75: 2.283*x0 + -3.437*sin(x0) + 12.558*cos(x0) + 0.745*x0_t**2 + -0.491*x0*sin(x0) + -0.724*sin(x0)**2 + 0.667*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190590858459473\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190581917762756\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190654039382935\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190634965896606\n",
      "expression length:\t 7\n",
      "Result stage 76: 2.249*x0 + -3.382*sin(x0) + 12.514*cos(x0) + 0.742*x0_t**2 + -0.482*x0*sin(x0) + -0.726*sin(x0)**2 + 0.647*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190665364265442\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190647482872009\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190710663795471\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190731525421143\n",
      "expression length:\t 7\n",
      "Result stage 77: 2.215*x0 + -3.327*sin(x0) + 12.469*cos(x0) + 0.739*x0_t**2 + -0.474*x0*sin(x0) + -0.728*sin(x0)**2 + 0.627*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190771460533142\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190809011459351\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190838813781738\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190856695175171\n",
      "expression length:\t 7\n",
      "Result stage 78: 2.181*x0 + -3.271*sin(x0) + 12.424*cos(x0) + 0.736*x0_t**2 + -0.465*x0*sin(x0) + -0.73*sin(x0)**2 + 0.607*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190912127494812\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190950870513916\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6190974116325378\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191065907478333\n",
      "expression length:\t 7\n",
      "Result stage 79: 2.146*x0 + -3.216*sin(x0) + 12.38*cos(x0) + 0.733*x0_t**2 + -0.456*x0*sin(x0) + -0.732*sin(x0)**2 + 0.587*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191118955612183\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191146969795227\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191251277923584\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191380023956299\n",
      "expression length:\t 7\n",
      "Result stage 80: 2.111*x0 + -3.161*sin(x0) + 12.338*cos(x0) + 0.73*x0_t**2 + -0.447*x0*sin(x0) + -0.734*sin(x0)**2 + 0.567*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191502809524536\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619166374206543\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191851496696472\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6191975474357605\n",
      "expression length:\t 7\n",
      "Result stage 81: 2.077*x0 + -3.106*sin(x0) + 12.299*cos(x0) + 0.727*x0_t**2 + -0.438*x0*sin(x0) + -0.735*sin(x0)**2 + 0.547*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192156076431274\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192312240600586\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192502975463867\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192639470100403\n",
      "expression length:\t 7\n",
      "Result stage 82: 2.042*x0 + -3.051*sin(x0) + 12.26*cos(x0) + 0.725*x0_t**2 + -0.429*x0*sin(x0) + -0.737*sin(x0)**2 + 0.527*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192877292633057\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6192977428436279\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193209290504456\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193366050720215\n",
      "expression length:\t 7\n",
      "Result stage 83: 2.008*x0 + -2.996*sin(x0) + 12.219*cos(x0) + 0.722*x0_t**2 + -0.42*x0*sin(x0) + -0.739*sin(x0)**2 + 0.507*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193577647209167\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6193720698356628\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619387686252594\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194033026695251\n",
      "expression length:\t 7\n",
      "Result stage 84: 1.974*x0 + -2.941*sin(x0) + 12.176*cos(x0) + 0.719*x0_t**2 + -0.411*x0*sin(x0) + -0.741*sin(x0)**2 + 0.488*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619414746761322\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619428277015686\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194519996643066\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194732785224915\n",
      "expression length:\t 7\n",
      "Result stage 85: 1.939*x0 + -2.885*sin(x0) + 12.137*cos(x0) + 0.716*x0_t**2 + -0.402*x0*sin(x0) + -0.743*sin(x0)**2 + 0.468*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6194887161254883\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619519829750061\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6195428967475891\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6195772886276245\n",
      "expression length:\t 7\n",
      "Result stage 86: 1.905*x0 + -2.831*sin(x0) + 12.099*cos(x0) + 0.713*x0_t**2 + -0.393*x0*sin(x0) + -0.745*sin(x0)**2 + 0.448*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6195933222770691\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6196275949478149\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6196439862251282\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.619674563407898\n",
      "expression length:\t 7\n",
      "Result stage 87: 1.871*x0 + -2.776*sin(x0) + 12.06*cos(x0) + 0.711*x0_t**2 + -0.384*x0*sin(x0) + -0.747*sin(x0)**2 + 0.428*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6196982264518738\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6197314262390137\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6197572946548462\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6197845935821533\n",
      "expression length:\t 7\n",
      "Result stage 88: 1.837*x0 + -2.722*sin(x0) + 12.021*cos(x0) + 0.708*x0_t**2 + -0.374*x0*sin(x0) + -0.749*sin(x0)**2 + 0.408*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6198208928108215\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6198453903198242\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6198724508285522\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6199015378952026\n",
      "expression length:\t 7\n",
      "Result stage 89: 1.803*x0 + -2.668*sin(x0) + 11.982*cos(x0) + 0.705*x0_t**2 + -0.365*x0*sin(x0) + -0.752*sin(x0)**2 + 0.389*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6199288368225098\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6199568510055542\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6199873685836792\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6200085878372192\n",
      "expression length:\t 7\n",
      "Result stage 90: 1.769*x0 + -2.614*sin(x0) + 11.943*cos(x0) + 0.703*x0_t**2 + -0.356*x0*sin(x0) + -0.754*sin(x0)**2 + 0.369*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6200428009033203\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6200677156448364\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6200951933860779\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6201295256614685\n",
      "expression length:\t 7\n",
      "Result stage 91: 1.736*x0 + -2.56*sin(x0) + 11.904*cos(x0) + 0.7*x0_t**2 + -0.346*x0*sin(x0) + -0.756*sin(x0)**2 + 0.349*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6201527118682861\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6201887726783752\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6202183961868286\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6202580332756042\n",
      "expression length:\t 7\n",
      "Result stage 92: 1.702*x0 + -2.507*sin(x0) + 11.865*cos(x0) + 0.697*x0_t**2 + -0.337*x0*sin(x0) + -0.758*sin(x0)**2 + 0.33*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6202875971794128\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6203233599662781\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6203504800796509\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.620386004447937\n",
      "expression length:\t 7\n",
      "Result stage 93: 1.669*x0 + -2.453*sin(x0) + 11.826*cos(x0) + 0.694*x0_t**2 + -0.328*x0*sin(x0) + -0.761*sin(x0)**2 + 0.311*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6204141974449158\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6204479932785034\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6204697489738464\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6205014586448669\n",
      "expression length:\t 7\n",
      "Result stage 94: 1.635*x0 + -2.4*sin(x0) + 11.788*cos(x0) + 0.692*x0_t**2 + -0.318*x0*sin(x0) + -0.763*sin(x0)**2 + 0.291*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6205286979675293\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6205636262893677\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6205882430076599\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6206202507019043\n",
      "expression length:\t 7\n",
      "Result stage 95: 1.602*x0 + -2.346*sin(x0) + 11.749*cos(x0) + 0.689*x0_t**2 + -0.309*x0*sin(x0) + -0.766*sin(x0)**2 + 0.272*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.620657742023468\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.620689868927002\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6207236051559448\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.620760977268219\n",
      "expression length:\t 7\n",
      "Result stage 96: 1.568*x0 + -2.292*sin(x0) + 11.71*cos(x0) + 0.686*x0_t**2 + -0.299*x0*sin(x0) + -0.769*sin(x0)**2 + 0.252*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.620799720287323\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6208360195159912\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6208655834197998\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6208966374397278\n",
      "expression length:\t 7\n",
      "Result stage 97: 1.535*x0 + -2.239*sin(x0) + 11.668*cos(x0) + 0.683*x0_t**2 + -0.29*x0*sin(x0) + -0.771*sin(x0)**2 + 0.234*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6209208965301514\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6209511160850525\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6209854483604431\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6210190057754517\n",
      "expression length:\t 7\n",
      "Result stage 98: 1.501*x0 + -2.185*sin(x0) + 11.625*cos(x0) + 0.68*x0_t**2 + -0.28*x0*sin(x0) + -0.774*sin(x0)**2 + 0.215*x0*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6210556030273438\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6210876107215881\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6211262345314026\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6211667060852051\n",
      "expression length:\t 6\n",
      "Result stage 99: 1.468*x0 + -2.131*sin(x0) + 11.581*cos(x0) + 0.677*x0_t**2 + -0.27*x0*sin(x0) + -0.776*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6482652425765991\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6453034281730652\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6426824927330017\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6402981281280518\n",
      "expression length:\t 6\n",
      "Result stage 100: 1.432*x0 + -2.041*sin(x0) + 11.543*cos(x0) + 0.679*x0_t**2 + -0.278*x0*sin(x0) + -0.791*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6381474733352661\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6362598538398743\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6345487833023071\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6329918503761292\n",
      "expression length:\t 6\n",
      "Result stage 101: 1.391*x0 + -1.963*sin(x0) + 11.506*cos(x0) + 0.677*x0_t**2 + -0.281*x0*sin(x0) + -0.8*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6315838694572449\n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6302943825721741\n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.629141628742218\n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6280919313430786\n",
      "expression length:\t 6\n",
      "Result stage 102: 1.347*x0 + -1.892*sin(x0) + 11.469*cos(x0) + 0.674*x0_t**2 + -0.278*x0*sin(x0) + -0.805*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6271281838417053\n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6262523531913757\n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6254642009735107\n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.624755322933197\n",
      "expression length:\t 6\n",
      "Result stage 103: 1.304*x0 + -1.828*sin(x0) + 11.43*cos(x0) + 0.671*x0_t**2 + -0.272*x0*sin(x0) + -0.808*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6241065859794617\n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6235175728797913\n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6229929327964783\n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6225175857543945\n",
      "expression length:\t 6\n",
      "Result stage 104: 1.262*x0 + -1.767*sin(x0) + 11.393*cos(x0) + 0.668*x0_t**2 + -0.262*x0*sin(x0) + -0.808*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6220985651016235\n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6217300891876221\n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6214065551757812\n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6211353540420532\n",
      "expression length:\t 6\n",
      "Result stage 105: 1.222*x0 + -1.71*sin(x0) + 11.356*cos(x0) + 0.665*x0_t**2 + -0.251*x0*sin(x0) + -0.807*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.620902419090271\n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6207015514373779\n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6205462217330933\n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6204220652580261\n",
      "expression length:\t 6\n",
      "Result stage 106: 1.184*x0 + -1.657*sin(x0) + 11.315*cos(x0) + 0.662*x0_t**2 + -0.238*x0*sin(x0) + -0.805*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6203224658966064\n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.620257556438446\n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6202215552330017\n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6202071905136108\n",
      "expression length:\t 6\n",
      "Result stage 107: 1.147*x0 + -1.605*sin(x0) + 11.273*cos(x0) + 0.658*x0_t**2 + -0.225*x0*sin(x0) + -0.802*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.620215892791748\n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6202613711357117\n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6203339695930481\n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6204244494438171\n",
      "expression length:\t 6\n",
      "Result stage 108: 1.112*x0 + -1.557*sin(x0) + 11.229*cos(x0) + 0.654*x0_t**2 + -0.21*x0*sin(x0) + -0.799*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6205383539199829\n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6206706166267395\n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6208178400993347\n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.620984673500061\n",
      "expression length:\t 5\n",
      "Result stage 109: 1.079*x0 + -1.511*sin(x0) + 11.184*cos(x0) + 0.65*x0_t**2 + -0.797*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6355240941047668\n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6346823573112488\n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6339212656021118\n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6332430243492126\n",
      "expression length:\t 5\n",
      "Result stage 110: 1.042*x0 + -1.46*sin(x0) + 11.139*cos(x0) + 0.636*x0_t**2 + -0.815*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6326401829719543\n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6321096420288086\n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6316550970077515\n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6312652230262756\n",
      "expression length:\t 5\n",
      "Result stage 111: 1.007*x0 + -1.413*sin(x0) + 11.097*cos(x0) + 0.633*x0_t**2 + -0.828*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6309300065040588\n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6306470036506653\n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6304158568382263\n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6302292943000793\n",
      "expression length:\t 5\n",
      "Result stage 112: 0.974*x0 + -1.369*sin(x0) + 11.056*cos(x0) + 0.631*x0_t**2 + -0.838*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6300885081291199\n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6299886703491211\n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6299336552619934\n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6299018859863281\n",
      "expression length:\t 5\n",
      "Result stage 113: 0.943*x0 + -1.328*sin(x0) + 11.015*cos(x0) + 0.629*x0_t**2 + -0.843*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6299094557762146\n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6299442648887634\n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6300088167190552\n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6301038861274719\n",
      "expression length:\t 5\n",
      "Result stage 114: 0.915*x0 + -1.29*sin(x0) + 10.972*cos(x0) + 0.626*x0_t**2 + -0.846*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.630213737487793\n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6303497552871704\n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6305052638053894\n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6306865215301514\n",
      "expression length:\t 5\n",
      "Result stage 115: 0.888*x0 + -1.254*sin(x0) + 10.93*cos(x0) + 0.624*x0_t**2 + -0.847*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6308814883232117\n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6310874819755554\n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6313203573226929\n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6315682530403137\n",
      "expression length:\t 5\n",
      "Result stage 116: 0.864*x0 + -1.22*sin(x0) + 10.889*cos(x0) + 0.622*x0_t**2 + -0.847*sin(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6318268179893494\n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6320938467979431\n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6323646903038025\n"
     ]
    }
   ],
   "source": [
    "for stage in range(200):\n",
    "    if stage >= 140:\n",
    "        lr = 1e-7\n",
    "    #Redefine computation after thresholding\n",
    "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=False)\n",
    "    Eta = Eta.to(device)\n",
    "    Zeta = Zeta.to(device)\n",
    "    Delta = Delta.to(device)\n",
    "\n",
    "    #Training\n",
    "    Epoch = 200\n",
    "    i = 1\n",
    "    lr = 1e-5\n",
    "    if(stage==1):\n",
    "        lam = 0\n",
    "    else:\n",
    "        lam = 0.1\n",
    "    temp = 1000\n",
    "    while(i<=Epoch):   \n",
    "        xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "        if i %50 == 0:\n",
    "            print(\"\\n\")\n",
    "            print(\"Stage \",stage)\n",
    "            print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "            print(\"Learning rate : \", lr)\n",
    "            print(\"Average loss : \" , lossitem)\n",
    "        temp = lossitem\n",
    "        if(temp <=1e-6):\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    ## Thresholding small indices ##\n",
    "    threshold = 2e-1\n",
    "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "    expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "    prevxi_L = xi_L.clone().detach()\n",
    "    mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "    ## obtaining analytical model\n",
    "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
    "    L = HL.generateExpression(xi_Lcpu,expr)\n",
    "    print(\"expression length:\\t\",len(xi_L))\n",
    "    print(\"Result stage \" + str(stage+2) + \":\" , L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda7f63-7451-4f1f-8bb8-6d994abdedee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8813673-d9ae-4769-9aa8-dc6b89ba4f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
