{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21abee7a-e86d-4999-a297-36f121b1dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "\n",
    "from sympy import symbols, simplify, derive_by_array\n",
    "from scipy.integrate import solve_ivp\n",
    "from xLSINDy_sp import *\n",
    "from sympy.physics.mechanics import *\n",
    "from sympy import *\n",
    "from Data_generator_py import image_process\n",
    "import sympy\n",
    "import torch\n",
    "import sys\n",
    "import HLsearch as HL\n",
    "import example_pendulum\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3931f0-8bba-44e2-8d56-b263f77bac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "environment = \"server\"\n",
    "sample_size = 10\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e864928-be31-4c73-b320-bfe0a2143ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4998, 2)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(r'../../../HLsearch/')\n",
    "#Saving Directory\n",
    "params = {}\n",
    "params['adding_noise'] = True\n",
    "params['noise_type'] = 'angle_noise'\n",
    "params['noiselevel'] = 1e-3\n",
    "if environment == 'laptop':\n",
    "    root_dir =R'C:\\Users\\87106\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'desktop':\n",
    "    root_dir = R'E:\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'server':\n",
    "    root_dir = R'/mnt/ssd1/stilrmy/Autoencoder-conservtive_expression'\n",
    "x,dx,ddx = image_process(sample_size,params)\n",
    "X = []\n",
    "Xdot = []\n",
    "for i in range(len(x)):\n",
    "    temp_list = [float(x[i]),float(dx[i])]\n",
    "    X.append(temp_list)\n",
    "    temp_list = [float(dx[i]),float(ddx[i])]\n",
    "    Xdot.append(temp_list)\n",
    "X = np.vstack(X)\n",
    "print(X.shape)\n",
    "Xdot = np.vstack(Xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8250a64-b76e-48f3-810e-28b181740486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states are: (x0, x0_t)\n",
      "states derivatives are:  (x0_t, x0_tt)\n"
     ]
    }
   ],
   "source": [
    "states_dim = 2\n",
    "states = ()\n",
    "states_dot = ()\n",
    "for i in range(states_dim):\n",
    "    if(i<states_dim//2):\n",
    "        states = states + (symbols('x{}'.format(i)),)\n",
    "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
    "    else:\n",
    "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
    "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
    "print('states are:',states)\n",
    "print('states derivatives are: ', states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c6dddc-ac3c-4e37-91dc-ac6bab075c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn from sympy to str\n",
    "states_sym = states\n",
    "states_dot_sym = states_dot\n",
    "states = list(str(descr) for descr in states)\n",
    "states_dot = list(str(descr) for descr in states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f71e4d-17ce-4af8-9d22-dc28cfc93f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'x0*x0_t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build function expression for the library in str\n",
    "expr= HL.buildFunctionExpressions(2,states_dim,states,use_sine=True)\n",
    "#expr=['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n",
    "\"a list of candidate function\"\n",
    "print(expr)\n",
    "expr.pop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ad60fa-1b07-423b-b96d-be7e84e9f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=True)\n",
    "Eta = Eta.to(device)\n",
    "Zeta = Zeta.to(device)\n",
    "Delta = Delta.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e559dd-c8b0-4041-b1f8-64e6124095be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(len(expr),device=device)\n",
    "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
    "prevxi_L = xi_L.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8134ecd-8d51-42b3-bd18-0a0f50901312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, targ):\n",
    "    loss = torch.mean((pred - targ)**2) \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c008f0b2-21b3-4433-afbf-8353931150dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(w, alpha):\n",
    "    clipped = torch.minimum(w,alpha)\n",
    "    clipped = torch.maximum(clipped,-alpha)\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16edbe1-038d-45e5-b403-07e08b441d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxL1norm(w_hat, alpha):\n",
    "    if(torch.is_tensor(alpha)==False):\n",
    "        alpha = torch.tensor(alpha)\n",
    "    w = w_hat - clip(w_hat,alpha)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8dffb9-95ba-4ef5-8987-6cc6a2e20a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(coef, prevcoef, Zeta, Eta, Delta,xdot, bs, lr, lam):\n",
    "    loss_list = []\n",
    "    tl = xdot.shape[0]\n",
    "    n = xdot.shape[1]\n",
    "    momentum = True\n",
    "    #if(torch.is_tensor(xdot)==False):\n",
    "        #xdot = torch.from_numpy(xdot).to(device).float()\n",
    "    v = coef.clone().detach().to(device).requires_grad_(True)\n",
    "    prev = prevcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    for i in range(tl//bs):\n",
    "        #computing acceleration with momentum\n",
    "        #if (momentum == True):\n",
    "            #vhat = (v + ((i - 1) / (i + 2)) * (v - prev)).clone().detach().requires_grad_(True)\n",
    "       # else:\n",
    "        vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "        prev = v\n",
    "        #Computing loss\n",
    "        zeta = Zeta[:,i*bs:(i+1)*bs]\n",
    "        eta = Eta[:,i*bs:(i+1)*bs]\n",
    "        delta = Delta[:,i*bs:(i+1)*bs]\n",
    "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
    "        #forward\n",
    "        q_tt_pred = lagrangianforward(vhat,zeta,eta,delta,x_t,device)\n",
    "        q_tt_true = xdot[i*bs:(i+1)*bs,n//2:].T\n",
    "        q_tt_true = torch.from_numpy(q_tt_true).to(device).float()\n",
    "        lossval = loss(q_tt_pred, q_tt_true)\n",
    "        lossval.requires_grad_(True)\n",
    "        #Backpropagation\n",
    "        lossval.backward()\n",
    "        with torch.no_grad():\n",
    "            v = vhat - lr * vhat.grad\n",
    "            v = proxL1norm(v, lr * lam)\n",
    "            # Manually zero the gradients after updating weights\n",
    "            vhat.grad = None\n",
    "        loss_list.append(lossval.item())\n",
    "    return v, prev, torch.tensor(loss_list).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9ace20-122c-4072-9ccf-9a3bd554abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 20/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  31.429067611694336\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 40/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  31.285449981689453\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 60/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  31.135942459106445\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 80/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  30.977832794189453\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 100/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  30.80778694152832\n"
     ]
    }
   ],
   "source": [
    "Epoch = 100\n",
    "i = 1\n",
    "lr = 1e-3\n",
    "lam = 0.1\n",
    "temp = 1000\n",
    "while(i<=Epoch):\n",
    "    xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "    if i %20 == 0:\n",
    "        print(\"\\n\")\n",
    "        print(\"Stage 1\")\n",
    "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "        print(\"Learning rate : \", lr)\n",
    "        print(\"Average loss : \" , lossitem)\n",
    "    if(temp <=5e-3):\n",
    "        break\n",
    "    if(temp <=1e-1):\n",
    "        lr = 1e-5\n",
    "    temp = lossitem\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04e39cf-6dac-45ba-8c38-8bfeaf4626b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "-1.35*x0 + -8.94*sin(x0) + 9.5*cos(x0) + 7.79*x0**2 + 5.44*x0_t**2 + -2.11*x0*sin(x0) + 5.73*x0_t*sin(x0) + -5.4*sin(x0)**2 + 7.89*x0*cos(x0) + -6.98*x0_t*cos(x0) + -7.97*sin(x0)*cos(x0) + -1.06*cos(x0)**2 + \n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-2\n",
    "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "## obtaining analytical model\n",
    "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
    "L = HL.generateExpression(xi_Lcpu,expr)\n",
    "print(len(xi_L))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbf84e-74af-4e42-a56c-c701b029dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  38.747371673583984\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  37.54408264160156\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  36.52942657470703\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  35.64136505126953\n",
      "expression length:\t 12\n",
      "Result stage 2: -1.185*x0 + -8.798*sin(x0) + 9.885*cos(x0) + 6.2*x0**2 + 6.255*x0_t**2 + -2.482*x0*sin(x0) + 5.65*x0_t*sin(x0) + -5.441*sin(x0)**2 + 7.852*x0*cos(x0) + -6.905*x0_t*cos(x0) + -7.838*sin(x0)*cos(x0) + -0.857*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  34.84897232055664\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  34.126304626464844\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  33.45498275756836\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  32.82178497314453\n",
      "expression length:\t 12\n",
      "Result stage 3: -1.12*x0 + -8.747*sin(x0) + 10.288*cos(x0) + 4.892*x0**2 + 6.6*x0_t**2 + -2.876*x0*sin(x0) + 5.65*x0_t*sin(x0) + -5.553*sin(x0)**2 + 7.883*x0*cos(x0) + -6.905*x0_t*cos(x0) + -7.797*sin(x0)*cos(x0) + -0.745*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  32.20276641845703\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  31.598642349243164\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  31.002607345581055\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  30.408418655395508\n",
      "expression length:\t 12\n",
      "Result stage 4: -0.98*x0 + -8.624*sin(x0) + 10.593*cos(x0) + 3.583*x0**2 + 6.544*x0_t**2 + -3.17*x0*sin(x0) + 5.572*x0_t*sin(x0) + -5.582*sin(x0)**2 + 7.833*x0*cos(x0) + -6.827*x0_t*cos(x0) + -7.681*sin(x0)*cos(x0) + -0.56*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  29.809951782226562\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  29.200986862182617\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  28.574800491333008\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  27.923667907714844\n",
      "expression length:\t 12\n",
      "Result stage 5: -0.838*x0 + -8.501*sin(x0) + 10.895*cos(x0) + 2.287*x0**2 + 6.202*x0_t**2 + -3.462*x0*sin(x0) + 5.494*x0_t*sin(x0) + -5.61*sin(x0)**2 + 7.783*x0*cos(x0) + -6.749*x0_t*cos(x0) + -7.564*sin(x0)*cos(x0) + -0.375*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  27.238500595092773\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  26.507740020751953\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  25.716501235961914\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  24.844423294067383\n",
      "expression length:\t 12\n",
      "Result stage 6: -0.685*x0 + -8.371*sin(x0) + 11.209*cos(x0) + 0.95*x0**2 + 5.511*x0_t**2 + -3.764*x0*sin(x0) + 5.416*x0_t*sin(x0) + -5.641*sin(x0)**2 + 7.734*x0*cos(x0) + -6.671*x0_t*cos(x0) + -7.442*sin(x0)*cos(x0) + -0.189*cos(x0)**2 + \n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  23.86252212524414\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  22.72733497619629\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  21.39357566833496\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  19.774076461791992\n",
      "expression length:\t 11\n",
      "Result stage 7: -0.5*x0 + -8.22*sin(x0) + 11.561*cos(x0) + -0.464*x0**2 + 4.213*x0_t**2 + -4.099*x0*sin(x0) + 5.338*x0_t*sin(x0) + -5.675*sin(x0)**2 + 7.69*x0*cos(x0) + -6.593*x0_t*cos(x0) + -7.307*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  17.617650985717773\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  14.503332138061523\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  10.027405738830566\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  7.140846252441406\n",
      "expression length:\t 10\n",
      "Result stage 8: -7.905*sin(x0) + 11.975*cos(x0) + -2.057*x0**2 + 1.712*x0_t**2 + -4.433*x0*sin(x0) + 5.26*x0_t*sin(x0) + -5.648*sin(x0)**2 + 7.673*x0*cos(x0) + -6.515*x0_t*cos(x0) + -7.065*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  6.340117931365967\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  5.76151180267334\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  5.279363632202148\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  4.872138977050781\n",
      "expression length:\t 10\n",
      "Result stage 9: -7.321*sin(x0) + 12.171*cos(x0) + -3.095*x0**2 + 1.718*x0_t**2 + -4.436*x0*sin(x0) + 5.182*x0_t*sin(x0) + -5.406*sin(x0)**2 + 7.633*x0*cos(x0) + -6.436*x0_t*cos(x0) + -6.67*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  4.524020195007324\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  4.223230838775635\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  3.9608192443847656\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  3.72989821434021\n",
      "expression length:\t 10\n",
      "Result stage 10: -6.835*sin(x0) + 12.283*cos(x0) + -3.799*x0**2 + 1.777*x0_t**2 + -4.402*x0*sin(x0) + 5.103*x0_t*sin(x0) + -5.196*sin(x0)**2 + 7.519*x0*cos(x0) + -6.358*x0_t*cos(x0) + -6.355*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  3.5251054763793945\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 100/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  3.34222412109375\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 150/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  3.1778695583343506\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 200/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  3.029285430908203\n",
      "expression length:\t 10\n",
      "Result stage 11: -6.416*sin(x0) + 12.35*cos(x0) + -4.315*x0**2 + 1.823*x0_t**2 + -4.357*x0*sin(x0) + 5.025*x0_t*sin(x0) + -5.012*sin(x0)**2 + 7.363*x0*cos(x0) + -6.28*x0_t*cos(x0) + -6.092*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 50/200\n",
      "Learning rate :  0.0001\n",
      "Average loss :  2.894285202026367\n"
     ]
    }
   ],
   "source": [
    "for stage in range(100):\n",
    "    \n",
    "    #Redefine computation after thresholding\n",
    "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=False)\n",
    "    Eta = Eta.to(device)\n",
    "    Zeta = Zeta.to(device)\n",
    "    Delta = Delta.to(device)\n",
    "\n",
    "    #Training\n",
    "    Epoch = 200\n",
    "    i = 1\n",
    "    lr = 1e-4\n",
    "    if(stage==1):\n",
    "        lam = 0\n",
    "    else:\n",
    "        lam = 0.1\n",
    "    temp = 1000\n",
    "    while(i<=Epoch):   \n",
    "        xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "        if i %50 == 0:\n",
    "            print(\"\\n\")\n",
    "            print(\"Stage \",stage)\n",
    "            print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "            print(\"Learning rate : \", lr)\n",
    "            print(\"Average loss : \" , lossitem)\n",
    "        temp = lossitem\n",
    "        if(temp <=1e-6):\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    ## Thresholding small indices ##\n",
    "    threshold = 1e-1\n",
    "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "    expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "    prevxi_L = xi_L.clone().detach()\n",
    "    mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "    ## obtaining analytical model\n",
    "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
    "    L = HL.generateExpression(xi_Lcpu,expr)\n",
    "    print(\"expression length:\\t\",len(xi_L))\n",
    "    print(\"Result stage \" + str(stage+2) + \":\" , L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda7f63-7451-4f1f-8bb8-6d994abdedee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8813673-d9ae-4769-9aa8-dc6b89ba4f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
