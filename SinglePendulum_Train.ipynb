{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21abee7a-e86d-4999-a297-36f121b1dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 500, 51, 51)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "\n",
    "from sympy import symbols, simplify, derive_by_array\n",
    "from scipy.integrate import solve_ivp\n",
    "from xLSINDy_sp import *\n",
    "from sympy.physics.mechanics import *\n",
    "from sympy import *\n",
    "from Data_generator_py import image_process\n",
    "import sympy\n",
    "import torch\n",
    "import sys\n",
    "import HLsearch as HL\n",
    "import example_pendulum\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3931f0-8bba-44e2-8d56-b263f77bac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "environment = \"server\"\n",
    "sample_size = 10\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e864928-be31-4c73-b320-bfe0a2143ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4998, 2)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(r'../../../HLsearch/')\n",
    "#Saving Directory\n",
    "params = {}\n",
    "params['adding_noise'] = False\n",
    "params['noise_type'] = 'image_noise'\n",
    "params['noiselevel'] = 1e-1\n",
    "params['changing_length'] = True\n",
    "params['c'] = 1.4e-4\n",
    "params['g'] = 9.81\n",
    "params['l'] = 1\n",
    "if environment == 'laptop':\n",
    "    root_dir =R'C:\\Users\\87106\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'desktop':\n",
    "    root_dir = R'E:\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'server':\n",
    "    root_dir = R'/mnt/ssd1/stilrmy/Autoencoder-conservtive_expression'\n",
    "x,dx,ddx = image_process(sample_size,params)\n",
    "X = []\n",
    "Xdot = []\n",
    "for i in range(len(x)):\n",
    "    temp_list = [float(x[i]),float(dx[i])]\n",
    "    X.append(temp_list)\n",
    "    temp_list = [float(dx[i]),float(ddx[i])]\n",
    "    Xdot.append(temp_list)\n",
    "X = np.vstack(X)\n",
    "print(X.shape)\n",
    "Xdot = np.vstack(Xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8250a64-b76e-48f3-810e-28b181740486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states are: (x0, x0_t)\n",
      "states derivatives are:  (x0_t, x0_tt)\n"
     ]
    }
   ],
   "source": [
    "states_dim = 2\n",
    "states = ()\n",
    "states_dot = ()\n",
    "for i in range(states_dim):\n",
    "    if(i<states_dim//2):\n",
    "        states = states + (symbols('x{}'.format(i)),)\n",
    "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
    "    else:\n",
    "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
    "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
    "print('states are:',states)\n",
    "print('states derivatives are: ', states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c6dddc-ac3c-4e37-91dc-ac6bab075c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn from sympy to str\n",
    "states_sym = states\n",
    "states_dot_sym = states_dot\n",
    "states = list(str(descr) for descr in states)\n",
    "states_dot = list(str(descr) for descr in states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f71e4d-17ce-4af8-9d22-dc28cfc93f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n"
     ]
    }
   ],
   "source": [
    "#build function expression for the library in str\n",
    "expr= HL.buildFunctionExpressions(2,states_dim,states,use_sine=True)\n",
    "#expr=['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n",
    "\"a list of candidate function\"\n",
    "print(expr)\n",
    "expr.pop(5)\n",
    "\n",
    "#library function expression for the dissipation function in str\n",
    "d_expr = [ 'x0_t','x0_t**2','x0_t**3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ad60fa-1b07-423b-b96d-be7e84e9f662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_qdot [1, 2*x0_t, 3*x0_t**2]\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0') cuda:0\n",
      "tensor([ 0.6201,  0.4699,  0.3027,  ..., -1.5455, -1.7719, -1.9971],\n",
      "       device='cuda:0', dtype=torch.float64) cuda:0\n",
      "tensor([0.2884, 0.1656, 0.0687,  ..., 1.7914, 2.3547, 2.9914], device='cuda:0',\n",
      "       dtype=torch.float64) cuda:0\n"
     ]
    }
   ],
   "source": [
    "Zeta, Eta, Delta, Dissip = LagrangianLibraryTensor(X,Xdot,expr,d_expr,states,states_dot,device,scaling=True)\n",
    "Eta = Eta.to(device)\n",
    "Zeta = Zeta.to(device)\n",
    "Delta = Delta.to(device)\n",
    "Dissip = Dissip.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e559dd-c8b0-4041-b1f8-64e6124095be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coefficients for the Lagrangian\n",
    "mask = torch.ones(len(expr),device=device)\n",
    "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "# coefficients for the dissipation function\n",
    "d_mask = torch.ones(len(d_expr),device=device)\n",
    "xi_d = torch.ones(len(d_expr),device=device).data.uniform_(-10,10)\n",
    "prevxi_d = xi_d.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8134ecd-8d51-42b3-bd18-0a0f50901312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, targ):\n",
    "    loss = torch.mean((pred - targ)**2) \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c008f0b2-21b3-4433-afbf-8353931150dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(w, alpha):\n",
    "    clipped = torch.minimum(w,alpha)\n",
    "    clipped = torch.maximum(clipped,-alpha)\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16edbe1-038d-45e5-b403-07e08b441d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxL1norm(w_hat, alpha):\n",
    "    if(torch.is_tensor(alpha)==False):\n",
    "        alpha = torch.tensor(alpha)\n",
    "    w = w_hat - clip(w_hat,alpha)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8dffb9-95ba-4ef5-8987-6cc6a2e20a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(coef,d_coef, prevcoef, predcoef, Zeta, Eta, Delta,Dissip, xdot, bs, lr, lam):\n",
    "    loss_list = []\n",
    "    tl = xdot.shape[0]\n",
    "    n = xdot.shape[1]\n",
    "    momentum = True\n",
    "    #if(torch.is_tensor(xdot)==False):\n",
    "        #xdot = torch.from_numpy(xdot).to(device).float()\n",
    "    v = coef.clone().detach().to(device).requires_grad_(True)\n",
    "    v_d = d_coef.clone().detach().to(device).requires_grad_(True)\n",
    "    prev = prevcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    prev_d = prevdcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    for i in range(tl//bs):\n",
    "        #computing acceleration with momentum\n",
    "        #if (momentum == True):\n",
    "        vhat = (v + ((i - 1) / (i + 2)) * (v - prev)).clone().detach().requires_grad_(True)\n",
    "        vdhat = (v_d + ((i - 1) / (i + 2)) * (v_d - prev_d)).clone().detach().requires_grad_(True)\n",
    "        #vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "        prev = v\n",
    "        prev_d = v_d\n",
    "        #Computing loss\n",
    "        zeta = Zeta[:,i*bs:(i+1)*bs]\n",
    "        eta = Eta[:,i*bs:(i+1)*bs]\n",
    "        delta = Delta[:,i*bs:(i+1)*bs]\n",
    "        dissip = Dissip[:,i*bs:(i+1)*bs]\n",
    "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
    "        #forward\n",
    "        q_tt_pred = lagrangianforward(vhat,vdhat,zeta,eta,delta,dissip,x_t,device)\n",
    "        q_tt_true = xdot[i*bs:(i+1)*bs,n//2:].T\n",
    "        q_tt_true = torch.from_numpy(q_tt_true).to(device).float()\n",
    "        lossval = loss(q_tt_pred, q_tt_true)\n",
    "        lossval.requires_grad_(True)\n",
    "        #Backpropagation\n",
    "        lossval.backward()\n",
    "        with torch.no_grad():\n",
    "            v = vhat - lr * vhat.grad\n",
    "            v = proxL1norm(v, lr * lam)\n",
    "            v_d = vdhat - lr * vdhat.grad\n",
    "            v_d = proxL1norm(v_d, lr * lam)\n",
    "            # Manually zero the gradients after updating weights\n",
    "            vhat.grad = None\n",
    "            vdhat.grad = None\n",
    "        loss_list.append(lossval.item())\n",
    "    return v, v_d, prev, prev_d, torch.tensor(loss_list).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9ace20-122c-4072-9ccf-9a3bd554abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 20/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  29.1403751373291\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 40/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  4.471088409423828\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 60/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  2.585818290710449\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 80/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  2.0875608921051025\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 100/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  1.724971890449524\n"
     ]
    }
   ],
   "source": [
    "Epoch = 100\n",
    "i = 1\n",
    "lr = 1e-3\n",
    "lam = 0.1\n",
    "temp = 1000\n",
    "while(i<=Epoch):\n",
    "    xi_L , xi_d, prevxi_L, prevxi_d, lossitem= training_loop(xi_L,xi_d,prevxi_L,prevxi_d,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "    if i %20 == 0:\n",
    "        print(\"\\n\")\n",
    "        print(\"Stage 1\")\n",
    "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "        print(\"Learning rate : \", lr)\n",
    "        print(\"Average loss : \" , lossitem)\n",
    "    if(temp <=5e-3):\n",
    "        break\n",
    "    if(temp <=1e-1):\n",
    "        lr = 1e-5\n",
    "    temp = lossitem\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04e39cf-6dac-45ba-8c38-8bfeaf4626b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "1.66*x0 + -2.72*sin(x0) + 12.14*cos(x0) + 1.84*x0**2 + 0.68*x0_t**2 + -4.82*x0*sin(x0) + 0.04*x0_t*sin(x0) + -5.96*sin(x0)**2 + 7.85*x0*cos(x0) + -5.8*x0_t*cos(x0) + -4.21*sin(x0)*cos(x0) + \n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-2\n",
    "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "expr = np.array(expr)[surv_index].tolist()\n",
    "surv_index_d = ((torch.abs(xi_d) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "d_expr = np.array(d_expr)[surv_index_d].tolist()\n",
    "\n",
    "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "xi_d =xi_d[surv_index_d].clone().detach().requires_grad_(True)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "prevxi_d = xi_d.clone().detach()\n",
    "mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "## obtaining analytical model\n",
    "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
    "xi_dcpu = np.around(xi_d.detach().cpu().numpy(),decimals=2)\n",
    "L = HL.generateExpression(xi_Lcpu,expr)\n",
    "D = HL.generateExpression(xi_dcpu,d_expr)\n",
    "print(len(xi_L))\n",
    "print(L)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fbbf84e-74af-4e42-a56c-c701b029dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  3.0039851665496826\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.8566794395446777\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.7247416973114014\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.606511354446411\n",
      "expression length:\t 10\n",
      "Result stage 2: 1.527*x0 + -2.826*sin(x0) + 12.162*cos(x0) + 1.577*x0**2 + 1.229*x0_t**2 + -4.818*x0*sin(x0) + -5.844*sin(x0)**2 + 7.572*x0*cos(x0) + -5.76*x0_t*cos(x0) + -4.371*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.50380539894104\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.410598039627075\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.3259778022766113\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.2487308979034424\n",
      "expression length:\t 10\n",
      "Result stage 3: 1.483*x0 + -2.922*sin(x0) + 12.233*cos(x0) + 1.343*x0**2 + 1.255*x0_t**2 + -4.868*x0*sin(x0) + -5.789*sin(x0)**2 + 7.393*x0*cos(x0) + -5.76*x0_t*cos(x0) + -4.526*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.1739649772644043\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.105896472930908\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  2.0425968170166016\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.9841293096542358\n",
      "expression length:\t 10\n",
      "Result stage 4: 1.425*x0 + -2.949*sin(x0) + 12.249*cos(x0) + 1.101*x0**2 + 1.271*x0_t**2 + -4.861*x0*sin(x0) + -5.687*sin(x0)**2 + 7.206*x0*cos(x0) + -5.721*x0_t*cos(x0) + -4.61*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.9295578002929688\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.8780865669250488\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.8293838500976562\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.7836393117904663\n",
      "expression length:\t 10\n",
      "Result stage 5: 1.385*x0 + -2.955*sin(x0) + 12.257*cos(x0) + 0.878*x0**2 + 1.286*x0_t**2 + -4.845*x0*sin(x0) + -5.584*sin(x0)**2 + 7.044*x0*cos(x0) + -5.682*x0_t*cos(x0) + -4.672*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.7405279874801636\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.6994268894195557\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.6602742671966553\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.6229442358016968\n",
      "expression length:\t 10\n",
      "Result stage 6: 1.356*x0 + -2.947*sin(x0) + 12.263*cos(x0) + 0.672*x0**2 + 1.3*x0_t**2 + -4.824*x0*sin(x0) + -5.478*sin(x0)**2 + 6.904*x0*cos(x0) + -5.643*x0_t*cos(x0) + -4.717*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.587275743484497\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.5533124208450317\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.5208039283752441\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.4895943403244019\n",
      "expression length:\t 10\n",
      "Result stage 7: 1.335*x0 + -2.928*sin(x0) + 12.264*cos(x0) + 0.478*x0**2 + 1.313*x0_t**2 + -4.798*x0*sin(x0) + -5.374*sin(x0)**2 + 6.775*x0*cos(x0) + -5.604*x0_t*cos(x0) + -4.749*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.4596434831619263\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.4307281970977783\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.4027832746505737\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.3759795427322388\n",
      "expression length:\t 10\n",
      "Result stage 8: 1.32*x0 + -2.902*sin(x0) + 12.26*cos(x0) + 0.297*x0**2 + 1.324*x0_t**2 + -4.77*x0*sin(x0) + -5.271*sin(x0)**2 + 6.658*x0*cos(x0) + -5.565*x0_t*cos(x0) + -4.77*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.3502755165100098\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.3254878520965576\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.3015328645706177\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.2782875299453735\n",
      "expression length:\t 9\n",
      "Result stage 9: 1.307*x0 + -2.87*sin(x0) + 12.251*cos(x0) + 1.333*x0_t**2 + -4.736*x0*sin(x0) + -5.168*sin(x0)**2 + 6.552*x0*cos(x0) + -5.526*x0_t*cos(x0) + -4.782*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.2270880937576294\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.2196823358535767\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.2122186422348022\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.2047579288482666\n",
      "expression length:\t 9\n",
      "Result stage 10: 1.296*x0 + -2.834*sin(x0) + 12.239*cos(x0) + 1.338*x0_t**2 + -4.698*x0*sin(x0) + -5.065*sin(x0)**2 + 6.454*x0*cos(x0) + -5.486*x0_t*cos(x0) + -4.789*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1974246501922607\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1901613473892212\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1830062866210938\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1759673357009888\n",
      "expression length:\t 9\n",
      "Result stage 11: 1.287*x0 + -2.795*sin(x0) + 12.226*cos(x0) + 1.328*x0_t**2 + -4.659*x0*sin(x0) + -4.961*sin(x0)**2 + 6.36*x0*cos(x0) + -5.447*x0_t*cos(x0) + -4.792*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1690212488174438\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.162122130393982\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1553183794021606\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1485613584518433\n",
      "expression length:\t 9\n",
      "Result stage 12: 1.28*x0 + -2.753*sin(x0) + 12.213*cos(x0) + 1.317*x0_t**2 + -4.621*x0*sin(x0) + -4.857*sin(x0)**2 + 6.269*x0*cos(x0) + -5.408*x0_t*cos(x0) + -4.791*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1418629884719849\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.135227084159851\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.128675937652588\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.122145652770996\n",
      "expression length:\t 9\n",
      "Result stage 13: 1.272*x0 + -2.71*sin(x0) + 12.2*cos(x0) + 1.306*x0_t**2 + -4.582*x0*sin(x0) + -4.753*sin(x0)**2 + 6.179*x0*cos(x0) + -5.369*x0_t*cos(x0) + -4.786*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1156461238861084\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1092208623886108\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.1028227806091309\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0964981317520142\n",
      "expression length:\t 9\n",
      "Result stage 14: 1.266*x0 + -2.666*sin(x0) + 12.187*cos(x0) + 1.296*x0_t**2 + -4.543*x0*sin(x0) + -4.649*sin(x0)**2 + 6.093*x0*cos(x0) + -5.33*x0_t*cos(x0) + -4.778*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0902864933013916\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.084110975265503\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0780212879180908\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0719581842422485\n",
      "expression length:\t 9\n",
      "Result stage 15: 1.26*x0 + -2.62*sin(x0) + 12.174*cos(x0) + 1.285*x0_t**2 + -4.504*x0*sin(x0) + -4.545*sin(x0)**2 + 6.012*x0*cos(x0) + -5.291*x0_t*cos(x0) + -4.767*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0659217834472656\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0599478483200073\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0539913177490234\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.048137903213501\n",
      "expression length:\t 9\n",
      "Result stage 16: 1.253*x0 + -2.574*sin(x0) + 12.161*cos(x0) + 1.274*x0_t**2 + -4.464*x0*sin(x0) + -4.441*sin(x0)**2 + 5.934*x0*cos(x0) + -5.252*x0_t*cos(x0) + -4.753*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.042295217514038\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.036549687385559\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.030812382698059\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0251284837722778\n",
      "expression length:\t 9\n",
      "Result stage 17: 1.245*x0 + -2.528*sin(x0) + 12.145*cos(x0) + 1.263*x0_t**2 + -4.424*x0*sin(x0) + -4.336*sin(x0)**2 + 5.858*x0*cos(x0) + -5.213*x0_t*cos(x0) + -4.737*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.019454002380371\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0138354301452637\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.0082217454910278\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.00261390209198\n",
      "expression length:\t 9\n",
      "Result stage 18: 1.237*x0 + -2.481*sin(x0) + 12.129*cos(x0) + 1.252*x0_t**2 + -4.383*x0*sin(x0) + -4.232*sin(x0)**2 + 5.785*x0*cos(x0) + -5.174*x0_t*cos(x0) + -4.72*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9970223903656006\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9914848208427429\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9859562516212463\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9804272651672363\n",
      "expression length:\t 9\n",
      "Result stage 19: 1.227*x0 + -2.433*sin(x0) + 12.112*cos(x0) + 1.241*x0_t**2 + -4.342*x0*sin(x0) + -4.127*sin(x0)**2 + 5.714*x0*cos(x0) + -5.134*x0_t*cos(x0) + -4.702*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9749249219894409\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9694104194641113\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9639279246330261\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9584679007530212\n",
      "expression length:\t 9\n",
      "Result stage 20: 1.217*x0 + -2.386*sin(x0) + 12.095*cos(x0) + 1.23*x0_t**2 + -4.3*x0*sin(x0) + -4.022*sin(x0)**2 + 5.643*x0*cos(x0) + -5.095*x0_t*cos(x0) + -4.682*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9530096054077148\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9475796222686768\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9421858191490173\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9367716908454895\n",
      "expression length:\t 9\n",
      "Result stage 21: 1.206*x0 + -2.338*sin(x0) + 12.077*cos(x0) + 1.219*x0_t**2 + -4.258*x0*sin(x0) + -3.917*sin(x0)**2 + 5.573*x0*cos(x0) + -5.056*x0_t*cos(x0) + -4.662*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9313868284225464\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9260342121124268\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9206988215446472\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9153810143470764\n",
      "expression length:\t 9\n",
      "Result stage 22: 1.194*x0 + -2.291*sin(x0) + 12.06*cos(x0) + 1.208*x0_t**2 + -4.216*x0*sin(x0) + -3.812*sin(x0)**2 + 5.504*x0*cos(x0) + -5.017*x0_t*cos(x0) + -4.638*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.910085916519165\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.9048151969909668\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8995439410209656\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8942651152610779\n",
      "expression length:\t 9\n",
      "Result stage 23: 1.181*x0 + -2.245*sin(x0) + 12.042*cos(x0) + 1.197*x0_t**2 + -4.172*x0*sin(x0) + -3.707*sin(x0)**2 + 5.436*x0*cos(x0) + -4.978*x0_t*cos(x0) + -4.612*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.889000415802002\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8837172389030457\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.878423810005188\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8731613159179688\n",
      "expression length:\t 9\n",
      "Result stage 24: 1.167*x0 + -2.198*sin(x0) + 12.024*cos(x0) + 1.186*x0_t**2 + -4.128*x0*sin(x0) + -3.602*sin(x0)**2 + 5.368*x0*cos(x0) + -4.939*x0_t*cos(x0) + -4.586*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8679162859916687\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.862686038017273\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.857477605342865\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8522494435310364\n",
      "expression length:\t 9\n",
      "Result stage 25: 1.152*x0 + -2.152*sin(x0) + 12.007*cos(x0) + 1.174*x0_t**2 + -4.083*x0*sin(x0) + -3.497*sin(x0)**2 + 5.302*x0*cos(x0) + -4.9*x0_t*cos(x0) + -4.559*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8470416069030762\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8418620228767395\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8366777300834656\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8314962983131409\n",
      "expression length:\t 9\n",
      "Result stage 26: 1.137*x0 + -2.106*sin(x0) + 11.989*cos(x0) + 1.163*x0_t**2 + -4.037*x0*sin(x0) + -3.392*sin(x0)**2 + 5.237*x0*cos(x0) + -4.861*x0_t*cos(x0) + -4.531*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8263170123100281\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8211683630943298\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8159956932067871\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8108355402946472\n",
      "expression length:\t 9\n",
      "Result stage 27: 1.12*x0 + -2.061*sin(x0) + 11.971*cos(x0) + 1.152*x0_t**2 + -3.991*x0*sin(x0) + -3.286*sin(x0)**2 + 5.173*x0*cos(x0) + -4.822*x0_t*cos(x0) + -4.504*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8056936860084534\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8005584478378296\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7954207062721252\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7902916073799133\n",
      "expression length:\t 9\n",
      "Result stage 28: 1.103*x0 + -2.016*sin(x0) + 11.952*cos(x0) + 1.14*x0_t**2 + -3.943*x0*sin(x0) + -3.18*sin(x0)**2 + 5.11*x0*cos(x0) + -4.783*x0_t*cos(x0) + -4.474*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7851701378822327\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7800666093826294\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7749558687210083\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7698447704315186\n",
      "expression length:\t 9\n",
      "Result stage 29: 1.085*x0 + -1.971*sin(x0) + 11.932*cos(x0) + 1.128*x0_t**2 + -3.894*x0*sin(x0) + -3.074*sin(x0)**2 + 5.049*x0*cos(x0) + -4.743*x0_t*cos(x0) + -4.444*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7647375464439392\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7596144676208496\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7544988393783569\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.74935382604599\n",
      "expression length:\t 9\n",
      "Result stage 30: 1.067*x0 + -1.925*sin(x0) + 11.913*cos(x0) + 1.116*x0_t**2 + -3.845*x0*sin(x0) + -2.968*sin(x0)**2 + 4.989*x0*cos(x0) + -4.704*x0_t*cos(x0) + -4.414*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7442180514335632\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7390516996383667\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7339023947715759\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7287577986717224\n",
      "expression length:\t 9\n",
      "Result stage 31: 1.048*x0 + -1.88*sin(x0) + 11.893*cos(x0) + 1.104*x0_t**2 + -3.796*x0*sin(x0) + -2.861*sin(x0)**2 + 4.928*x0*cos(x0) + -4.665*x0_t*cos(x0) + -4.383*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.723645806312561\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7185127139091492\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7133716344833374\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7082327604293823\n",
      "expression length:\t 9\n",
      "Result stage 32: 1.028*x0 + -1.836*sin(x0) + 11.873*cos(x0) + 1.093*x0_t**2 + -3.745*x0*sin(x0) + -2.754*sin(x0)**2 + 4.869*x0*cos(x0) + -4.626*x0_t*cos(x0) + -4.351*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.7030600309371948\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6978949308395386\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6927408576011658\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6875879168510437\n",
      "expression length:\t 9\n",
      "Result stage 33: 1.007*x0 + -1.791*sin(x0) + 11.853*cos(x0) + 1.081*x0_t**2 + -3.696*x0*sin(x0) + -2.647*sin(x0)**2 + 4.81*x0*cos(x0) + -4.587*x0_t*cos(x0) + -4.319*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6824281215667725\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6773436665534973\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6722791790962219\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6672146916389465\n",
      "expression length:\t 9\n",
      "Result stage 34: 0.985*x0 + -1.746*sin(x0) + 11.833*cos(x0) + 1.069*x0_t**2 + -3.645*x0*sin(x0) + -2.54*sin(x0)**2 + 4.752*x0*cos(x0) + -4.548*x0_t*cos(x0) + -4.287*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6621580123901367\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6570870876312256\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6520029902458191\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6469284892082214\n",
      "expression length:\t 9\n",
      "Result stage 35: 0.961*x0 + -1.702*sin(x0) + 11.812*cos(x0) + 1.057*x0_t**2 + -3.595*x0*sin(x0) + -2.434*sin(x0)**2 + 4.695*x0*cos(x0) + -4.509*x0_t*cos(x0) + -4.253*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6418843865394592\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6368484497070312\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6318220496177673\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6268085837364197\n",
      "expression length:\t 9\n",
      "Result stage 36: 0.937*x0 + -1.657*sin(x0) + 11.789*cos(x0) + 1.045*x0_t**2 + -3.544*x0*sin(x0) + -2.327*sin(x0)**2 + 4.638*x0*cos(x0) + -4.47*x0_t*cos(x0) + -4.217*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6217772364616394\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6167478561401367\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6116793155670166\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6066192388534546\n",
      "expression length:\t 9\n",
      "Result stage 37: 0.912*x0 + -1.613*sin(x0) + 11.765*cos(x0) + 1.033*x0_t**2 + -3.493*x0*sin(x0) + -2.22*sin(x0)**2 + 4.582*x0*cos(x0) + -4.431*x0_t*cos(x0) + -4.182*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.6015145778656006\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5964207649230957\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5913235545158386\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5862149000167847\n",
      "expression length:\t 9\n",
      "Result stage 38: 0.888*x0 + -1.568*sin(x0) + 11.74*cos(x0) + 1.02*x0_t**2 + -3.441*x0*sin(x0) + -2.112*sin(x0)**2 + 4.526*x0*cos(x0) + -4.391*x0_t*cos(x0) + -4.145*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5811244249343872\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5760480761528015\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5710029602050781\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5659224390983582\n",
      "expression length:\t 9\n",
      "Result stage 39: 0.862*x0 + -1.524*sin(x0) + 11.713*cos(x0) + 1.008*x0_t**2 + -3.389*x0*sin(x0) + -2.004*sin(x0)**2 + 4.472*x0*cos(x0) + -4.352*x0_t*cos(x0) + -4.108*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5608358979225159\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5557305216789246\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5505481362342834\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.545381486415863\n",
      "expression length:\t 9\n",
      "Result stage 40: 0.837*x0 + -1.48*sin(x0) + 11.688*cos(x0) + 0.995*x0_t**2 + -3.335*x0*sin(x0) + -1.895*sin(x0)**2 + 4.419*x0*cos(x0) + -4.313*x0_t*cos(x0) + -4.069*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5402325987815857\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5350781083106995\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5299212336540222\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5247674584388733\n",
      "expression length:\t 9\n",
      "Result stage 41: 0.811*x0 + -1.435*sin(x0) + 11.665*cos(x0) + 0.983*x0_t**2 + -3.281*x0*sin(x0) + -1.787*sin(x0)**2 + 4.365*x0*cos(x0) + -4.274*x0_t*cos(x0) + -4.031*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5196210145950317\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5144784450531006\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5093271136283875\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5041741132736206\n",
      "expression length:\t 9\n",
      "Result stage 42: 0.785*x0 + -1.391*sin(x0) + 11.642*cos(x0) + 0.97*x0_t**2 + -3.226*x0*sin(x0) + -1.679*sin(x0)**2 + 4.313*x0*cos(x0) + -4.235*x0_t*cos(x0) + -3.993*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4990089535713196\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.493878036737442\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.48874399065971375\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.48360389471054077\n",
      "expression length:\t 9\n",
      "Result stage 43: 0.758*x0 + -1.347*sin(x0) + 11.619*cos(x0) + 0.958*x0_t**2 + -3.171*x0*sin(x0) + -1.571*sin(x0)**2 + 4.26*x0*cos(x0) + -4.196*x0_t*cos(x0) + -3.956*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4784534275531769\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4733309745788574\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.46823355555534363\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4631389081478119\n",
      "expression length:\t 9\n",
      "Result stage 44: 0.732*x0 + -1.303*sin(x0) + 11.597*cos(x0) + 0.945*x0_t**2 + -3.115*x0*sin(x0) + -1.464*sin(x0)**2 + 4.207*x0*cos(x0) + -4.157*x0_t*cos(x0) + -3.918*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4580322206020355\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.45293378829956055\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.447826623916626\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4427461326122284\n",
      "expression length:\t 9\n",
      "Result stage 45: 0.704*x0 + -1.26*sin(x0) + 11.574*cos(x0) + 0.933*x0_t**2 + -3.059*x0*sin(x0) + -1.357*sin(x0)**2 + 4.153*x0*cos(x0) + -4.118*x0_t*cos(x0) + -3.879*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.43766266107559204\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.43259263038635254\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4275270700454712\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4224647581577301\n",
      "expression length:\t 9\n",
      "Result stage 46: 0.677*x0 + -1.216*sin(x0) + 11.547*cos(x0) + 0.92*x0_t**2 + -3.003*x0*sin(x0) + -1.25*sin(x0)**2 + 4.098*x0*cos(x0) + -4.079*x0_t*cos(x0) + -3.84*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4174115061759949\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.41236749291419983\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4073356091976166\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.4023086726665497\n",
      "expression length:\t 9\n",
      "Result stage 47: 0.65*x0 + -1.172*sin(x0) + 11.52*cos(x0) + 0.908*x0_t**2 + -2.946*x0*sin(x0) + -1.144*sin(x0)**2 + 4.043*x0*cos(x0) + -4.04*x0_t*cos(x0) + -3.801*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.39728468656539917\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.3922736346721649\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.3872710168361664\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.38231128454208374\n",
      "expression length:\t 9\n",
      "Result stage 48: 0.623*x0 + -1.129*sin(x0) + 11.492*cos(x0) + 0.895*x0_t**2 + -2.889*x0*sin(x0) + -1.038*sin(x0)**2 + 3.988*x0*cos(x0) + -4.0*x0_t*cos(x0) + -3.761*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.37739646434783936\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.37243422865867615\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.3674193024635315\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.36243772506713867\n",
      "expression length:\t 9\n",
      "Result stage 49: 0.596*x0 + -1.085*sin(x0) + 11.463*cos(x0) + 0.882*x0_t**2 + -2.832*x0*sin(x0) + -0.932*sin(x0)**2 + 3.936*x0*cos(x0) + -3.96*x0_t*cos(x0) + -3.721*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.35750219225883484\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.3525725305080414\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.347651869058609\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.34272336959838867\n",
      "expression length:\t 9\n",
      "Result stage 50: 0.568*x0 + -1.042*sin(x0) + 11.432*cos(x0) + 0.869*x0_t**2 + -2.774*x0*sin(x0) + -0.825*sin(x0)**2 + 3.883*x0*cos(x0) + -3.921*x0_t*cos(x0) + -3.681*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.3378114402294159\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.33291763067245483\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.32802706956863403\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.32316282391548157\n",
      "expression length:\t 9\n",
      "Result stage 51: 0.541*x0 + -0.999*sin(x0) + 11.403*cos(x0) + 0.857*x0_t**2 + -2.717*x0*sin(x0) + -0.72*sin(x0)**2 + 3.83*x0*cos(x0) + -3.881*x0_t*cos(x0) + -3.64*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.31831032037734985\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.31348657608032227\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.3086787462234497\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.30390268564224243\n",
      "expression length:\t 9\n",
      "Result stage 52: 0.513*x0 + -0.953*sin(x0) + 11.373*cos(x0) + 0.844*x0_t**2 + -2.658*x0*sin(x0) + -0.615*sin(x0)**2 + 3.777*x0*cos(x0) + -3.841*x0_t*cos(x0) + -3.6*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.29914531111717224\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.2944299876689911\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.2897528111934662\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.2851060628890991\n",
      "expression length:\t 9\n",
      "Result stage 53: 0.484*x0 + -0.908*sin(x0) + 11.343*cos(x0) + 0.831*x0_t**2 + -2.6*x0*sin(x0) + -0.511*sin(x0)**2 + 3.725*x0*cos(x0) + -3.801*x0_t*cos(x0) + -3.559*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.28047478199005127\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.27587971091270447\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.2713061273097992\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.26676109433174133\n",
      "expression length:\t 9\n",
      "Result stage 54: 0.455*x0 + -0.863*sin(x0) + 11.312*cos(x0) + 0.818*x0_t**2 + -2.542*x0*sin(x0) + -0.408*sin(x0)**2 + 3.672*x0*cos(x0) + -3.761*x0_t*cos(x0) + -3.519*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.26224127411842346\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.25775349140167236\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.253297358751297\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.2488728016614914\n",
      "expression length:\t 9\n",
      "Result stage 55: 0.426*x0 + -0.817*sin(x0) + 11.28*cos(x0) + 0.806*x0_t**2 + -2.483*x0*sin(x0) + -0.307*sin(x0)**2 + 3.619*x0*cos(x0) + -3.721*x0_t*cos(x0) + -3.478*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.24447377026081085\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.24012291431427002\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.2358412891626358\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.23160359263420105\n",
      "expression length:\t 9\n",
      "Result stage 56: 0.397*x0 + -0.772*sin(x0) + 11.247*cos(x0) + 0.793*x0_t**2 + -2.423*x0*sin(x0) + -0.207*sin(x0)**2 + 3.565*x0*cos(x0) + -3.681*x0_t*cos(x0) + -3.437*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.22740547358989716\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.2232494354248047\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.2191402167081833\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.21507380902767181\n",
      "expression length:\t 8\n",
      "Result stage 57: 0.367*x0 + -0.727*sin(x0) + 11.211*cos(x0) + 0.781*x0_t**2 + -2.364*x0*sin(x0) + 3.512*x0*cos(x0) + -3.641*x0_t*cos(x0) + -3.396*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19959931075572968\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19890591502189636\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19821277260780334\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19751526415348053\n",
      "expression length:\t 8\n",
      "Result stage 58: 0.337*x0 + -0.682*sin(x0) + 11.174*cos(x0) + 0.767*x0_t**2 + -2.305*x0*sin(x0) + 3.46*x0*cos(x0) + -3.601*x0_t*cos(x0) + -3.354*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19681015610694885\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19610175490379333\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19538860023021698\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19467684626579285\n",
      "expression length:\t 8\n",
      "Result stage 59: 0.308*x0 + -0.638*sin(x0) + 11.137*cos(x0) + 0.76*x0_t**2 + -2.245*x0*sin(x0) + 3.407*x0*cos(x0) + -3.561*x0_t*cos(x0) + -3.313*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19395700097084045\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19324082136154175\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1925162971019745\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1917942613363266\n",
      "expression length:\t 8\n",
      "Result stage 60: 0.279*x0 + -0.593*sin(x0) + 11.099*cos(x0) + 0.753*x0_t**2 + -2.185*x0*sin(x0) + 3.354*x0*cos(x0) + -3.521*x0_t*cos(x0) + -3.272*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19105792045593262\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1903182864189148\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18958015739917755\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18883530795574188\n",
      "expression length:\t 8\n",
      "Result stage 61: 0.25*x0 + -0.548*sin(x0) + 11.062*cos(x0) + 0.746*x0_t**2 + -2.125*x0*sin(x0) + 3.301*x0*cos(x0) + -3.481*x0_t*cos(x0) + -3.231*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.188070148229599\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18730343878269196\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18653614819049835\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18575960397720337\n",
      "expression length:\t 8\n",
      "Result stage 62: 0.222*x0 + -0.503*sin(x0) + 11.024*cos(x0) + 0.739*x0_t**2 + -2.065*x0*sin(x0) + 3.247*x0*cos(x0) + -3.441*x0_t*cos(x0) + -3.19*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18498094379901886\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18420010805130005\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18341724574565887\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18263618648052216\n",
      "expression length:\t 7\n",
      "Result stage 63: -0.458*sin(x0) + 10.987*cos(x0) + 0.732*x0_t**2 + -2.004*x0*sin(x0) + 3.193*x0*cos(x0) + -3.401*x0_t*cos(x0) + -3.15*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.20856793224811554\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.20457524061203003\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.20113123953342438\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1981021761894226\n",
      "expression length:\t 7\n",
      "Result stage 64: -0.368*sin(x0) + 10.95*cos(x0) + 0.725*x0_t**2 + -1.946*x0*sin(x0) + 3.156*x0*cos(x0) + -3.361*x0_t*cos(x0) + -3.097*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19540193676948547\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1929408609867096\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.19067640602588654\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18858617544174194\n",
      "expression length:\t 7\n",
      "Result stage 65: -0.297*sin(x0) + 10.912*cos(x0) + 0.718*x0_t**2 + -1.886*x0*sin(x0) + 3.105*x0*cos(x0) + -3.321*x0_t*cos(x0) + -3.054*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1866186559200287\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18476048111915588\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1829896867275238\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.18129727244377136\n",
      "expression length:\t 7\n",
      "Result stage 66: -0.239*sin(x0) + 10.872*cos(x0) + 0.711*x0_t**2 + -1.827*x0*sin(x0) + 3.049*x0*cos(x0) + -3.281*x0_t*cos(x0) + -3.019*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1796608418226242\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.17805710434913635\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.17649345099925995\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.17497341334819794\n",
      "expression length:\t 6\n",
      "Result stage 67: 10.832*cos(x0) + 0.704*x0_t**2 + -1.768*x0*sin(x0) + 2.987*x0*cos(x0) + -3.241*x0_t*cos(x0) + -2.985*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.17911851406097412\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.17601867020130157\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1733153760433197\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.170968160033226\n",
      "expression length:\t 6\n",
      "Result stage 68: 10.792*cos(x0) + 0.697*x0_t**2 + -1.709*x0*sin(x0) + 2.901*x0*cos(x0) + -3.201*x0_t*cos(x0) + -2.974*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1688816100358963\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1669597327709198\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.16527512669563293\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1637711524963379\n",
      "expression length:\t 6\n",
      "Result stage 69: 10.756*cos(x0) + 0.691*x0_t**2 + -1.65*x0*sin(x0) + 2.83*x0*cos(x0) + -3.161*x0_t*cos(x0) + -2.949*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1623709797859192\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.16106323897838593\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1598045974969864\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.15860123932361603\n",
      "expression length:\t 6\n",
      "Result stage 70: 10.72*cos(x0) + 0.684*x0_t**2 + -1.591*x0*sin(x0) + 2.767*x0*cos(x0) + -3.121*x0_t*cos(x0) + -2.916*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.15745513141155243\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1563609093427658\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.15530262887477875\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1542811095714569\n",
      "expression length:\t 6\n",
      "Result stage 71: 10.683*cos(x0) + 0.677*x0_t**2 + -1.532*x0*sin(x0) + 2.711*x0*cos(x0) + -3.081*x0_t*cos(x0) + -2.878*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.15328384935855865\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.15230503678321838\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.15133129060268402\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.15037350356578827\n",
      "expression length:\t 6\n",
      "Result stage 72: 10.645*cos(x0) + 0.67*x0_t**2 + -1.473*x0*sin(x0) + 2.66*x0*cos(x0) + -3.041*x0_t*cos(x0) + -2.835*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.14942118525505066\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.14847315847873688\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.14753209054470062\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.14659009873867035\n",
      "expression length:\t 6\n",
      "Result stage 73: 10.605*cos(x0) + 0.663*x0_t**2 + -1.413*x0*sin(x0) + 2.61*x0*cos(x0) + -3.001*x0_t*cos(x0) + -2.789*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.14565040171146393\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1447119414806366\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.14377741515636444\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.14284396171569824\n",
      "expression length:\t 6\n",
      "Result stage 74: 10.566*cos(x0) + 0.656*x0_t**2 + -1.353*x0*sin(x0) + 2.562*x0*cos(x0) + -2.961*x0_t*cos(x0) + -2.742*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1419103890657425\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.14097753167152405\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.14004778861999512\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1391129046678543\n",
      "expression length:\t 6\n",
      "Result stage 75: 10.527*cos(x0) + 0.649*x0_t**2 + -1.294*x0*sin(x0) + 2.515*x0*cos(x0) + -2.921*x0_t*cos(x0) + -2.695*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.13817568123340607\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.13722790777683258\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.13627254962921143\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.13532337546348572\n",
      "expression length:\t 6\n",
      "Result stage 76: 10.491*cos(x0) + 0.642*x0_t**2 + -1.234*x0*sin(x0) + 2.469*x0*cos(x0) + -2.881*x0_t*cos(x0) + -2.646*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.13437098264694214\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.13341960310935974\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1324653923511505\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.13151176273822784\n",
      "expression length:\t 6\n",
      "Result stage 77: 10.457*cos(x0) + 0.635*x0_t**2 + -1.174*x0*sin(x0) + 2.423*x0*cos(x0) + -2.841*x0_t*cos(x0) + -2.596*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1305648684501648\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1296147257089615\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.12866836786270142\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1277197152376175\n",
      "expression length:\t 6\n",
      "Result stage 78: 10.423*cos(x0) + 0.629*x0_t**2 + -1.114*x0*sin(x0) + 2.377*x0*cos(x0) + -2.802*x0_t*cos(x0) + -2.547*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.12676994502544403\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.12582090497016907\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1248684674501419\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.12391738593578339\n",
      "expression length:\t 6\n",
      "Result stage 79: 10.389*cos(x0) + 0.622*x0_t**2 + -1.055*x0*sin(x0) + 2.331*x0*cos(x0) + -2.762*x0_t*cos(x0) + -2.498*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.12296649813652039\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.12201614677906036\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.12106595933437347\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.12010598927736282\n",
      "expression length:\t 6\n",
      "Result stage 80: 10.355*cos(x0) + 0.615*x0_t**2 + -0.995*x0*sin(x0) + 2.286*x0*cos(x0) + -2.722*x0_t*cos(x0) + -2.448*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.11913178861141205\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.11815334856510162\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.11718109250068665\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.11620788276195526\n",
      "expression length:\t 6\n",
      "Result stage 81: 10.321*cos(x0) + 0.608*x0_t**2 + -0.933*x0*sin(x0) + 2.24*x0*cos(x0) + -2.682*x0_t*cos(x0) + -2.398*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1152331531047821\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.11426280438899994\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.11329314857721329\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.11232646554708481\n",
      "expression length:\t 6\n",
      "Result stage 82: 10.286*cos(x0) + 0.601*x0_t**2 + -0.871*x0*sin(x0) + 2.194*x0*cos(x0) + -2.642*x0_t*cos(x0) + -2.348*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.11135733127593994\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1103944256901741\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.10943496227264404\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.10847648978233337\n",
      "expression length:\t 6\n",
      "Result stage 83: 10.25*cos(x0) + 0.594*x0_t**2 + -0.81*x0*sin(x0) + 2.148*x0*cos(x0) + -2.602*x0_t*cos(x0) + -2.298*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.10752120614051819\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.10656782239675522\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.10561338067054749\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.10466266423463821\n",
      "expression length:\t 6\n",
      "Result stage 84: 10.212*cos(x0) + 0.587*x0_t**2 + -0.748*x0*sin(x0) + 2.103*x0*cos(x0) + -2.562*x0_t*cos(x0) + -2.248*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.1037130355834961\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.10276626795530319\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.10182594507932663\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.10088762640953064\n",
      "expression length:\t 6\n",
      "Result stage 85: 10.173*cos(x0) + 0.58*x0_t**2 + -0.687*x0*sin(x0) + 2.057*x0*cos(x0) + -2.522*x0_t*cos(x0) + -2.198*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09995200484991074\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09901858121156693\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09808763116598129\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09715770184993744\n",
      "expression length:\t 6\n",
      "Result stage 86: 10.134*cos(x0) + 0.573*x0_t**2 + -0.626*x0*sin(x0) + 2.011*x0*cos(x0) + -2.482*x0_t*cos(x0) + -2.149*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09622702747583389\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09530246257781982\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09438492357730865\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09347031265497208\n",
      "expression length:\t 6\n",
      "Result stage 87: 10.093*cos(x0) + 0.566*x0_t**2 + -0.565*x0*sin(x0) + 1.966*x0*cos(x0) + -2.442*x0_t*cos(x0) + -2.099*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09255706518888474\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09165041148662567\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.09073935449123383\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08983394503593445\n",
      "expression length:\t 6\n",
      "Result stage 88: 10.053*cos(x0) + 0.558*x0_t**2 + -0.505*x0*sin(x0) + 1.921*x0*cos(x0) + -2.402*x0_t*cos(x0) + -2.05*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08892735093832016\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08802275359630585\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08711808919906616\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08621721714735031\n",
      "expression length:\t 6\n",
      "Result stage 89: 10.014*cos(x0) + 0.551*x0_t**2 + -0.444*x0*sin(x0) + 1.876*x0*cos(x0) + -2.362*x0_t*cos(x0) + -2.0*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08532212674617767\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0844285637140274\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08354304730892181\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08266164362430573\n",
      "expression length:\t 6\n",
      "Result stage 90: 9.975*cos(x0) + 0.544*x0_t**2 + -0.384*x0*sin(x0) + 1.831*x0*cos(x0) + -2.322*x0_t*cos(x0) + -1.951*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08178415149450302\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08091101050376892\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.08004038035869598\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.07917295396327972\n",
      "expression length:\t 6\n",
      "Result stage 91: 9.935*cos(x0) + 0.537*x0_t**2 + -0.324*x0*sin(x0) + 1.786*x0*cos(x0) + -2.282*x0_t*cos(x0) + -1.902*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.07830999791622162\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.07745033502578735\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.07659687101840973\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.07574672996997833\n",
      "expression length:\t 6\n",
      "Result stage 92: 9.896*cos(x0) + 0.53*x0_t**2 + -0.265*x0*sin(x0) + 1.741*x0*cos(x0) + -2.242*x0_t*cos(x0) + -1.852*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.07489992678165436\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.07406686246395111\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.07323669642210007\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.07241097837686539\n",
      "expression length:\t 6\n",
      "Result stage 93: 9.855*cos(x0) + 0.523*x0_t**2 + -0.206*x0*sin(x0) + 1.696*x0*cos(x0) + -2.202*x0_t*cos(x0) + -1.803*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0715906098484993\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.07077745348215103\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0699673518538475\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.06916342675685883\n",
      "expression length:\t 5\n",
      "Result stage 94: 9.813*cos(x0) + 0.516*x0_t**2 + 1.65*x0*cos(x0) + -2.162*x0_t*cos(x0) + -1.753*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.06438424438238144\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.06404483318328857\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.06370757520198822\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.063372403383255\n",
      "expression length:\t 5\n",
      "Result stage 95: 9.769*cos(x0) + 0.502*x0_t**2 + 1.605*x0*cos(x0) + -2.122*x0_t*cos(x0) + -1.704*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.06303726136684418\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.06270401179790497\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.06237158551812172\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.062039412558078766\n",
      "expression length:\t 5\n",
      "Result stage 96: 9.724*cos(x0) + 0.499*x0_t**2 + 1.56*x0*cos(x0) + -2.082*x0_t*cos(x0) + -1.655*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.061708707362413406\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.06137961149215698\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.06105181574821472\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0607256144285202\n",
      "expression length:\t 5\n",
      "Result stage 97: 9.679*cos(x0) + 0.497*x0_t**2 + 1.515*x0*cos(x0) + -2.042*x0_t*cos(x0) + -1.606*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.060400303453207016\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.06007567420601845\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.059752900153398514\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.059432294219732285\n",
      "expression length:\t 5\n",
      "Result stage 98: 9.634*cos(x0) + 0.495*x0_t**2 + 1.47*x0*cos(x0) + -2.002*x0_t*cos(x0) + -1.557*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0591142438352108\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.058798208832740784\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05848289281129837\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.058170001953840256\n",
      "expression length:\t 5\n",
      "Result stage 99: 9.589*cos(x0) + 0.492*x0_t**2 + 1.425*x0*cos(x0) + -1.962*x0_t*cos(x0) + -1.509*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05785846710205078\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0575479120016098\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.057238541543483734\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05692942440509796\n",
      "expression length:\t 5\n",
      "Result stage 100: 9.544*cos(x0) + 0.49*x0_t**2 + 1.38*x0*cos(x0) + -1.922*x0_t*cos(x0) + -1.461*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05662112310528755\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.056314822286367416\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05601134151220322\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05570848658680916\n",
      "expression length:\t 5\n",
      "Result stage 101: 9.499*cos(x0) + 0.488*x0_t**2 + 1.336*x0*cos(x0) + -1.882*x0_t*cos(x0) + -1.413*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05540589615702629\n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.055105917155742645\n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05480595678091049\n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05450809746980667\n",
      "expression length:\t 5\n",
      "Result stage 102: 9.454*cos(x0) + 0.486*x0_t**2 + 1.291*x0*cos(x0) + -1.842*x0_t*cos(x0) + -1.365*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05421175807714462\n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05391770601272583\n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.053625933825969696\n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05333549529314041\n",
      "expression length:\t 5\n",
      "Result stage 103: 9.408*cos(x0) + 0.483*x0_t**2 + 1.247*x0*cos(x0) + -1.802*x0_t*cos(x0) + -1.317*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05304693058133125\n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.052758362144231796\n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.052470751106739044\n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.052184607833623886\n",
      "expression length:\t 5\n",
      "Result stage 104: 9.362*cos(x0) + 0.481*x0_t**2 + 1.203*x0*cos(x0) + -1.762*x0_t*cos(x0) + -1.269*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.051901042461395264\n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.051620617508888245\n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05134158954024315\n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05106509476900101\n",
      "expression length:\t 5\n",
      "Result stage 105: 9.315*cos(x0) + 0.478*x0_t**2 + 1.159*x0*cos(x0) + -1.722*x0_t*cos(x0) + -1.221*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05078880488872528\n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.050513170659542084\n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.050237175077199936\n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.049962904304265976\n",
      "expression length:\t 5\n",
      "Result stage 106: 9.27*cos(x0) + 0.476*x0_t**2 + 1.115*x0*cos(x0) + -1.682*x0_t*cos(x0) + -1.173*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.049690086394548416\n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04941777512431145\n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.049145713448524475\n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04887734353542328\n",
      "expression length:\t 5\n",
      "Result stage 107: 9.228*cos(x0) + 0.474*x0_t**2 + 1.071*x0*cos(x0) + -1.642*x0_t*cos(x0) + -1.126*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04861029237508774\n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.048345182090997696\n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04808339849114418\n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.047822970896959305\n",
      "expression length:\t 5\n",
      "Result stage 108: 9.187*cos(x0) + 0.472*x0_t**2 + 1.027*x0*cos(x0) + -1.602*x0_t*cos(x0) + -1.079*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04756466671824455\n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04730753228068352\n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0470479279756546\n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04678624868392944\n",
      "expression length:\t 5\n",
      "Result stage 109: 9.146*cos(x0) + 0.47*x0_t**2 + 0.983*x0*cos(x0) + -1.562*x0_t*cos(x0) + -1.032*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04652624949812889\n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04626864194869995\n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04601087421178818\n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04574836045503616\n",
      "expression length:\t 5\n",
      "Result stage 110: 9.104*cos(x0) + 0.468*x0_t**2 + 0.938*x0*cos(x0) + -1.522*x0_t*cos(x0) + -0.983*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04548973590135574\n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04523357376456261\n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.044978611171245575\n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04472609609365463\n",
      "expression length:\t 5\n",
      "Result stage 111: 9.063*cos(x0) + 0.466*x0_t**2 + 0.892*x0*cos(x0) + -1.482*x0_t*cos(x0) + -0.934*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04447628930211067\n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04422935098409653\n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04398326948285103\n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.043739475309848785\n",
      "expression length:\t 5\n",
      "Result stage 112: 9.021*cos(x0) + 0.464*x0_t**2 + 0.847*x0*cos(x0) + -1.442*x0_t*cos(x0) + -0.885*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04349827021360397\n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04325900599360466\n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04302211478352547\n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04278772324323654\n",
      "expression length:\t 5\n",
      "Result stage 113: 8.979*cos(x0) + 0.461*x0_t**2 + 0.802*x0*cos(x0) + -1.402*x0_t*cos(x0) + -0.836*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04255486652255058\n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04232418164610863\n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04209660366177559\n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04187115281820297\n",
      "expression length:\t 5\n",
      "Result stage 114: 8.937*cos(x0) + 0.459*x0_t**2 + 0.757*x0*cos(x0) + -1.362*x0_t*cos(x0) + -0.787*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04164848104119301\n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.041427917778491974\n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04120994731783867\n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04099433124065399\n",
      "expression length:\t 5\n",
      "Result stage 115: 8.895*cos(x0) + 0.457*x0_t**2 + 0.712*x0*cos(x0) + -1.322*x0_t*cos(x0) + -0.739*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04078048840165138\n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.040569137781858444\n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04036002978682518\n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.04015371575951576\n",
      "expression length:\t 5\n",
      "Result stage 116: 8.852*cos(x0) + 0.455*x0_t**2 + 0.667*x0*cos(x0) + -1.282*x0_t*cos(x0) + -0.69*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0399496853351593\n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03974781930446625\n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.039548084139823914\n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03935087099671364\n",
      "expression length:\t 5\n",
      "Result stage 117: 8.811*cos(x0) + 0.453*x0_t**2 + 0.623*x0*cos(x0) + -1.242*x0_t*cos(x0) + -0.642*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  116\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.039155974984169006\n",
      "\n",
      "\n",
      "Stage  116\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03896207734942436\n",
      "\n",
      "\n",
      "Stage  116\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03877091780304909\n",
      "\n",
      "\n",
      "Stage  116\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.038581594824790955\n",
      "expression length:\t 5\n",
      "Result stage 118: 8.772*cos(x0) + 0.451*x0_t**2 + 0.578*x0*cos(x0) + -1.202*x0_t*cos(x0) + -0.594*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  117\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.038394514471292496\n",
      "\n",
      "\n",
      "Stage  117\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03820956125855446\n",
      "\n",
      "\n",
      "Stage  117\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03802705556154251\n",
      "\n",
      "\n",
      "Stage  117\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0378468818962574\n",
      "expression length:\t 5\n",
      "Result stage 119: 8.734*cos(x0) + 0.449*x0_t**2 + 0.534*x0*cos(x0) + -1.162*x0_t*cos(x0) + -0.546*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  118\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03766899183392525\n",
      "\n",
      "\n",
      "Stage  118\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03749406710267067\n",
      "\n",
      "\n",
      "Stage  118\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03732294216752052\n",
      "\n",
      "\n",
      "Stage  118\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03715477138757706\n",
      "expression length:\t 5\n",
      "Result stage 120: 8.696*cos(x0) + 0.447*x0_t**2 + 0.49*x0*cos(x0) + -1.123*x0_t*cos(x0) + -0.499*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  119\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.036988623440265656\n",
      "\n",
      "\n",
      "Stage  119\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03682471439242363\n",
      "\n",
      "\n",
      "Stage  119\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03666367754340172\n",
      "\n",
      "\n",
      "Stage  119\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.036505453288555145\n",
      "expression length:\t 5\n",
      "Result stage 121: 8.659*cos(x0) + 0.445*x0_t**2 + 0.446*x0*cos(x0) + -1.083*x0_t*cos(x0) + -0.451*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  120\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03634950891137123\n",
      "\n",
      "\n",
      "Stage  120\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.036196496337652206\n",
      "\n",
      "\n",
      "Stage  120\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.036045946180820465\n",
      "\n",
      "\n",
      "Stage  120\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.035898126661777496\n",
      "expression length:\t 5\n",
      "Result stage 122: 8.621*cos(x0) + 0.443*x0_t**2 + 0.402*x0*cos(x0) + -1.043*x0_t*cos(x0) + -0.404*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  121\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03575264662504196\n",
      "\n",
      "\n",
      "Stage  121\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03561007231473923\n",
      "\n",
      "\n",
      "Stage  121\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03547029569745064\n",
      "\n",
      "\n",
      "Stage  121\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03533333167433739\n",
      "expression length:\t 5\n",
      "Result stage 123: 8.584*cos(x0) + 0.441*x0_t**2 + 0.359*x0*cos(x0) + -1.003*x0_t*cos(x0) + -0.357*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  122\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03519923985004425\n",
      "\n",
      "\n",
      "Stage  122\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03506755083799362\n",
      "\n",
      "\n",
      "Stage  122\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03493861109018326\n",
      "\n",
      "\n",
      "Stage  122\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03481203690171242\n",
      "expression length:\t 5\n",
      "Result stage 124: 8.547*cos(x0) + 0.439*x0_t**2 + 0.316*x0*cos(x0) + -0.961*x0_t*cos(x0) + -0.311*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  123\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03468864783644676\n",
      "\n",
      "\n",
      "Stage  123\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03456784039735794\n",
      "\n",
      "\n",
      "Stage  123\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0344497412443161\n",
      "\n",
      "\n",
      "Stage  123\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03433465212583542\n",
      "expression length:\t 5\n",
      "Result stage 125: 8.509*cos(x0) + 0.437*x0_t**2 + 0.272*x0*cos(x0) + -0.919*x0_t*cos(x0) + -0.264*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  124\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.034222617745399475\n",
      "\n",
      "\n",
      "Stage  124\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.034114450216293335\n",
      "\n",
      "\n",
      "Stage  124\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0340084470808506\n",
      "\n",
      "\n",
      "Stage  124\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03390539437532425\n",
      "expression length:\t 5\n",
      "Result stage 126: 8.471*cos(x0) + 0.436*x0_t**2 + 0.23*x0*cos(x0) + -0.877*x0_t*cos(x0) + -0.218*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  125\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033805184066295624\n",
      "\n",
      "\n",
      "Stage  125\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033707745373249054\n",
      "\n",
      "\n",
      "Stage  125\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033612728118896484\n",
      "\n",
      "\n",
      "Stage  125\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03352006524801254\n",
      "expression length:\t 3\n",
      "Result stage 127: 8.435*cos(x0) + 0.434*x0_t**2 + -0.836*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  126\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03326934948563576\n",
      "\n",
      "\n",
      "Stage  126\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033270351588726044\n",
      "\n",
      "\n",
      "Stage  126\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03327159583568573\n",
      "\n",
      "\n",
      "Stage  126\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033272795379161835\n",
      "expression length:\t 3\n",
      "Result stage 128: 8.4*cos(x0) + 0.432*x0_t**2 + -0.794*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  127\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033273834735155106\n",
      "\n",
      "\n",
      "Stage  127\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033275097608566284\n",
      "\n",
      "\n",
      "Stage  127\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03327581286430359\n",
      "\n",
      "\n",
      "Stage  127\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033276960253715515\n",
      "expression length:\t 3\n",
      "Result stage 129: 8.364*cos(x0) + 0.43*x0_t**2 + -0.752*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  128\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03327818214893341\n",
      "\n",
      "\n",
      "Stage  128\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033279240131378174\n",
      "\n",
      "\n",
      "Stage  128\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033280279487371445\n",
      "\n",
      "\n",
      "Stage  128\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03328139707446098\n",
      "expression length:\t 3\n",
      "Result stage 130: 8.328*cos(x0) + 0.428*x0_t**2 + -0.71*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  129\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03328240290284157\n",
      "\n",
      "\n",
      "Stage  129\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03328345715999603\n",
      "\n",
      "\n",
      "Stage  129\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033284466713666916\n",
      "\n",
      "\n",
      "Stage  129\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033285897225141525\n",
      "expression length:\t 3\n",
      "Result stage 131: 8.292*cos(x0) + 0.427*x0_t**2 + -0.668*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  130\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033286888152360916\n",
      "\n",
      "\n",
      "Stage  130\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033287789672613144\n",
      "\n",
      "\n",
      "Stage  130\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03328923508524895\n",
      "\n",
      "\n",
      "Stage  130\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03329041972756386\n",
      "expression length:\t 3\n",
      "Result stage 132: 8.256*cos(x0) + 0.425*x0_t**2 + -0.627*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  131\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0332915298640728\n",
      "\n",
      "\n",
      "Stage  131\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033292677253484726\n",
      "\n",
      "\n",
      "Stage  131\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033294010907411575\n",
      "\n",
      "\n",
      "Stage  131\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03329529985785484\n",
      "expression length:\t 3\n",
      "Result stage 133: 8.22*cos(x0) + 0.423*x0_t**2 + -0.585*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  132\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03329610079526901\n",
      "\n",
      "\n",
      "Stage  132\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03329746425151825\n",
      "\n",
      "\n",
      "Stage  132\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03329877555370331\n",
      "\n",
      "\n",
      "Stage  132\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033299971371889114\n",
      "expression length:\t 3\n",
      "Result stage 134: 8.182*cos(x0) + 0.421*x0_t**2 + -0.543*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  133\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0333012156188488\n",
      "\n",
      "\n",
      "Stage  133\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03330255299806595\n",
      "\n",
      "\n",
      "Stage  133\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03330357372760773\n",
      "\n",
      "\n",
      "Stage  133\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03330474719405174\n",
      "expression length:\t 3\n",
      "Result stage 135: 8.145*cos(x0) + 0.419*x0_t**2 + -0.501*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  134\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03330620378255844\n",
      "\n",
      "\n",
      "Stage  134\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03330744802951813\n",
      "\n",
      "\n",
      "Stage  134\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033308688551187515\n",
      "\n",
      "\n",
      "Stage  134\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0333097018301487\n",
      "expression length:\t 3\n",
      "Result stage 136: 8.108*cos(x0) + 0.417*x0_t**2 + -0.459*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  135\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03331107646226883\n",
      "\n",
      "\n",
      "Stage  135\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033312395215034485\n",
      "\n",
      "\n",
      "Stage  135\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03331359475851059\n",
      "\n",
      "\n",
      "Stage  135\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03331480547785759\n",
      "expression length:\t 3\n",
      "Result stage 137: 8.07*cos(x0) + 0.415*x0_t**2 + -0.417*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  136\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033316221088171005\n",
      "\n",
      "\n",
      "Stage  136\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03331717476248741\n",
      "\n",
      "\n",
      "Stage  136\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0333186499774456\n",
      "\n",
      "\n",
      "Stage  136\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03331978991627693\n",
      "expression length:\t 3\n",
      "Result stage 138: 8.033*cos(x0) + 0.413*x0_t**2 + -0.374*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  137\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03332124650478363\n",
      "\n",
      "\n",
      "Stage  137\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033322524279356\n",
      "\n",
      "\n",
      "Stage  137\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033323828130960464\n",
      "\n",
      "\n",
      "Stage  137\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03332514688372612\n",
      "expression length:\t 3\n",
      "Result stage 139: 7.995*cos(x0) + 0.411*x0_t**2 + -0.332*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  138\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03332676738500595\n",
      "\n",
      "\n",
      "Stage  138\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03332803025841713\n",
      "\n",
      "\n",
      "Stage  138\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033329375088214874\n",
      "\n",
      "\n",
      "Stage  138\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03333117812871933\n",
      "expression length:\t 3\n",
      "Result stage 140: 7.955*cos(x0) + 0.409*x0_t**2 + -0.289*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  139\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03333204984664917\n",
      "\n",
      "\n",
      "Stage  139\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03333357721567154\n",
      "\n",
      "\n",
      "Stage  139\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033335212618112564\n",
      "\n",
      "\n",
      "Stage  139\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03333654627203941\n",
      "expression length:\t 3\n",
      "Result stage 141: 7.915*cos(x0) + 0.407*x0_t**2 + -0.247*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  140\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033338144421577454\n",
      "\n",
      "\n",
      "Stage  140\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033339474350214005\n",
      "\n",
      "\n",
      "Stage  140\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03334060683846474\n",
      "\n",
      "\n",
      "Stage  140\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03334234282374382\n",
      "expression length:\t 3\n",
      "Result stage 142: 7.876*cos(x0) + 0.405*x0_t**2 + -0.205*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  141\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033343665301799774\n",
      "\n",
      "\n",
      "Stage  141\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033345337957143784\n",
      "\n",
      "\n",
      "Stage  141\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033346474170684814\n",
      "\n",
      "\n",
      "Stage  141\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03334822878241539\n",
      "expression length:\t 2\n",
      "Result stage 143: 7.836*cos(x0) + 0.403*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  142\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03334967419505119\n",
      "\n",
      "\n",
      "Stage  142\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03335105627775192\n",
      "\n",
      "\n",
      "Stage  142\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0333528034389019\n",
      "\n",
      "\n",
      "Stage  142\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033354390412569046\n",
      "expression length:\t 2\n",
      "Result stage 144: 7.795*cos(x0) + 0.401*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  143\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03335600718855858\n",
      "\n",
      "\n",
      "Stage  143\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03335751220583916\n",
      "\n",
      "\n",
      "Stage  143\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03335908055305481\n",
      "\n",
      "\n",
      "Stage  143\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03336038812994957\n",
      "expression length:\t 2\n",
      "Result stage 145: 7.753*cos(x0) + 0.399*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  144\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03336230665445328\n",
      "\n",
      "\n",
      "Stage  144\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03336358442902565\n",
      "\n",
      "\n",
      "Stage  144\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03336535394191742\n",
      "\n",
      "\n",
      "Stage  144\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03336680680513382\n",
      "expression length:\t 2\n",
      "Result stage 146: 7.713*cos(x0) + 0.397*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  145\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033368416130542755\n",
      "\n",
      "\n",
      "Stage  145\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033369701355695724\n",
      "\n",
      "\n",
      "Stage  145\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033371150493621826\n",
      "\n",
      "\n",
      "Stage  145\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03337300196290016\n",
      "expression length:\t 2\n",
      "Result stage 147: 7.672*cos(x0) + 0.395*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  146\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03337462991476059\n",
      "\n",
      "\n",
      "Stage  146\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033376533538103104\n",
      "\n",
      "\n",
      "Stage  146\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03337796777486801\n",
      "\n",
      "\n",
      "Stage  146\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033379532396793365\n",
      "expression length:\t 2\n",
      "Result stage 148: 7.631*cos(x0) + 0.393*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  147\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033381301909685135\n",
      "\n",
      "\n",
      "Stage  147\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03338299319148064\n",
      "\n",
      "\n",
      "Stage  147\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0333847776055336\n",
      "\n",
      "\n",
      "Stage  147\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03338629752397537\n",
      "expression length:\t 2\n",
      "Result stage 149: 7.59*cos(x0) + 0.391*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  148\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033387813717126846\n",
      "\n",
      "\n",
      "Stage  148\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03338969126343727\n",
      "\n",
      "\n",
      "Stage  148\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03339115157723427\n",
      "\n",
      "\n",
      "Stage  148\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03339310362935066\n",
      "expression length:\t 2\n",
      "Result stage 150: 7.549*cos(x0) + 0.388*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  149\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033394671976566315\n",
      "\n",
      "\n",
      "Stage  149\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03339660167694092\n",
      "\n",
      "\n",
      "Stage  149\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03339798003435135\n",
      "\n",
      "\n",
      "Stage  149\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03339970484375954\n",
      "expression length:\t 2\n",
      "Result stage 151: 7.508*cos(x0) + 0.386*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  150\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03340153396129608\n",
      "\n",
      "\n",
      "Stage  150\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03340370953083038\n",
      "\n",
      "\n",
      "Stage  150\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0334051214158535\n",
      "\n",
      "\n",
      "Stage  150\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03340691328048706\n",
      "expression length:\t 2\n",
      "Result stage 152: 7.467*cos(x0) + 0.384*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  151\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033408503979444504\n",
      "\n",
      "\n",
      "Stage  151\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03341002017259598\n",
      "\n",
      "\n",
      "Stage  151\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033412009477615356\n",
      "\n",
      "\n",
      "Stage  151\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033413905650377274\n",
      "expression length:\t 2\n",
      "Result stage 153: 7.426*cos(x0) + 0.382*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  152\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03341572731733322\n",
      "\n",
      "\n",
      "Stage  152\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03341749310493469\n",
      "\n",
      "\n",
      "Stage  152\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03341924771666527\n",
      "\n",
      "\n",
      "Stage  152\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03342089429497719\n",
      "expression length:\t 2\n",
      "Result stage 154: 7.386*cos(x0) + 0.38*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  153\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03342283144593239\n",
      "\n",
      "\n",
      "Stage  153\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03342430293560028\n",
      "\n",
      "\n",
      "Stage  153\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03342602029442787\n",
      "\n",
      "\n",
      "Stage  153\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03342806547880173\n",
      "expression length:\t 2\n",
      "Result stage 155: 7.345*cos(x0) + 0.378*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  154\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03343024104833603\n",
      "\n",
      "\n",
      "Stage  154\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03343188017606735\n",
      "\n",
      "\n",
      "Stage  154\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03343360871076584\n",
      "\n",
      "\n",
      "Stage  154\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03343559056520462\n",
      "expression length:\t 2\n",
      "Result stage 156: 7.304*cos(x0) + 0.376*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  155\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03343738242983818\n",
      "\n",
      "\n",
      "Stage  155\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033439069986343384\n",
      "\n",
      "\n",
      "Stage  155\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033440954983234406\n",
      "\n",
      "\n",
      "Stage  155\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03344302251935005\n",
      "expression length:\t 2\n",
      "Result stage 157: 7.264*cos(x0) + 0.374*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  156\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03344501554965973\n",
      "\n",
      "\n",
      "Stage  156\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03344693034887314\n",
      "\n",
      "\n",
      "Stage  156\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03344881534576416\n",
      "\n",
      "\n",
      "Stage  156\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03345053270459175\n",
      "expression length:\t 2\n",
      "Result stage 158: 7.223*cos(x0) + 0.372*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  157\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033452704548835754\n",
      "\n",
      "\n",
      "Stage  157\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03345463052392006\n",
      "\n",
      "\n",
      "Stage  157\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0334564708173275\n",
      "\n",
      "\n",
      "Stage  157\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03345858305692673\n",
      "expression length:\t 2\n",
      "Result stage 159: 7.183*cos(x0) + 0.37*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  158\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03346019983291626\n",
      "\n",
      "\n",
      "Stage  158\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03346237167716026\n",
      "\n",
      "\n",
      "Stage  158\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03346455842256546\n",
      "\n",
      "\n",
      "Stage  158\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03346622735261917\n",
      "expression length:\t 2\n",
      "Result stage 160: 7.142*cos(x0) + 0.367*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  159\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03346813842654228\n",
      "\n",
      "\n",
      "Stage  159\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033470261842012405\n",
      "\n",
      "\n",
      "Stage  159\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033472690731287\n",
      "\n",
      "\n",
      "Stage  159\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033474523574113846\n",
      "expression length:\t 2\n",
      "Result stage 161: 7.1*cos(x0) + 0.365*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  160\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033476926386356354\n",
      "\n",
      "\n",
      "Stage  160\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033478762954473495\n",
      "\n",
      "\n",
      "Stage  160\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033481232821941376\n",
      "\n",
      "\n",
      "Stage  160\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03348337858915329\n",
      "expression length:\t 2\n",
      "Result stage 162: 7.057*cos(x0) + 0.363*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  161\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033485449850559235\n",
      "\n",
      "\n",
      "Stage  161\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033487819135189056\n",
      "\n",
      "\n",
      "Stage  161\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03349008783698082\n",
      "\n",
      "\n",
      "Stage  161\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03349216654896736\n",
      "expression length:\t 2\n",
      "Result stage 163: 7.014*cos(x0) + 0.361*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  162\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033494431525468826\n",
      "\n",
      "\n",
      "Stage  162\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033496491611003876\n",
      "\n",
      "\n",
      "Stage  162\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03349856287240982\n",
      "\n",
      "\n",
      "Stage  162\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033501069992780685\n",
      "expression length:\t 2\n",
      "Result stage 164: 6.971*cos(x0) + 0.359*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  163\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03350323438644409\n",
      "\n",
      "\n",
      "Stage  163\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033505771309137344\n",
      "\n",
      "\n",
      "Stage  163\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033508189022541046\n",
      "\n",
      "\n",
      "Stage  163\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03351018205285072\n",
      "expression length:\t 2\n",
      "Result stage 165: 6.928*cos(x0) + 0.357*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  164\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03351232409477234\n",
      "\n",
      "\n",
      "Stage  164\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03351479768753052\n",
      "\n",
      "\n",
      "Stage  164\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03351718932390213\n",
      "\n",
      "\n",
      "Stage  164\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03351945802569389\n",
      "expression length:\t 2\n",
      "Result stage 166: 6.885*cos(x0) + 0.354*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  165\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03352203220129013\n",
      "\n",
      "\n",
      "Stage  165\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033524490892887115\n",
      "\n",
      "\n",
      "Stage  165\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03352689743041992\n",
      "\n",
      "\n",
      "Stage  165\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03352968394756317\n",
      "expression length:\t 2\n",
      "Result stage 167: 6.842*cos(x0) + 0.352*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  166\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03353184461593628\n",
      "\n",
      "\n",
      "Stage  166\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033534202724695206\n",
      "\n",
      "\n",
      "Stage  166\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03353691101074219\n",
      "\n",
      "\n",
      "Stage  166\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0335395447909832\n",
      "expression length:\t 2\n",
      "Result stage 168: 6.797*cos(x0) + 0.35*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  167\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03354167938232422\n",
      "\n",
      "\n",
      "Stage  167\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03354432061314583\n",
      "\n",
      "\n",
      "Stage  167\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033546753227710724\n",
      "\n",
      "\n",
      "Stage  167\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033549416810274124\n",
      "expression length:\t 2\n",
      "Result stage 169: 6.754*cos(x0) + 0.348*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  168\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033551886677742004\n",
      "\n",
      "\n",
      "Stage  168\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033554431051015854\n",
      "\n",
      "\n",
      "Stage  168\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03355681151151657\n",
      "\n",
      "\n",
      "Stage  168\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03355963155627251\n",
      "expression length:\t 2\n",
      "Result stage 170: 6.711*cos(x0) + 0.345*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  169\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03356194496154785\n",
      "\n",
      "\n",
      "Stage  169\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.03356464207172394\n",
      "\n",
      "\n",
      "Stage  169\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033567458391189575\n",
      "\n",
      "\n",
      "Stage  169\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.033569853752851486\n",
      "expression length:\t 2\n",
      "Result stage 171: 6.668*cos(x0) + 0.343*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  170\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289813548326492\n",
      "\n",
      "\n",
      "Stage  170\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289807587862015\n",
      "\n",
      "\n",
      "Stage  170\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328981950879097\n",
      "\n",
      "\n",
      "Stage  170\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898254692554474\n",
      "expression length:\t 2\n",
      "Result stage 172: 6.666*cos(x0) + 0.343*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  171\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289796784520149\n",
      "\n",
      "\n",
      "Stage  171\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898105680942535\n",
      "\n",
      "\n",
      "Stage  171\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289806470274925\n",
      "\n",
      "\n",
      "Stage  171\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289782255887985\n",
      "expression length:\t 2\n",
      "Result stage 173: 6.665*cos(x0) + 0.343*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  172\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289802744984627\n",
      "\n",
      "\n",
      "Stage  172\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289776295423508\n",
      "\n",
      "\n",
      "Stage  172\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289789333939552\n",
      "\n",
      "\n",
      "Stage  172\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897818833589554\n",
      "expression length:\t 2\n",
      "Result stage 174: 6.664*cos(x0) + 0.343*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  173\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289774805307388\n",
      "\n",
      "\n",
      "Stage  173\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328977033495903\n",
      "\n",
      "\n",
      "Stage  173\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289765864610672\n",
      "\n",
      "\n",
      "Stage  173\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289820998907089\n",
      "expression length:\t 2\n",
      "Result stage 175: 6.662*cos(x0) + 0.343*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  174\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289845213294029\n",
      "\n",
      "\n",
      "Stage  174\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289860486984253\n",
      "\n",
      "\n",
      "Stage  174\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289870545268059\n",
      "\n",
      "\n",
      "Stage  174\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289858624339104\n",
      "expression length:\t 2\n",
      "Result stage 176: 6.661*cos(x0) + 0.343*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  175\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898467034101486\n",
      "\n",
      "\n",
      "Stage  175\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289832919836044\n",
      "\n",
      "\n",
      "Stage  175\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289823234081268\n",
      "\n",
      "\n",
      "Stage  175\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898131757974625\n",
      "expression length:\t 2\n",
      "Result stage 177: 6.66*cos(x0) + 0.343*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  176\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289801999926567\n",
      "\n",
      "\n",
      "Stage  176\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289792314171791\n",
      "\n",
      "\n",
      "Stage  176\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289788216352463\n",
      "\n",
      "\n",
      "Stage  176\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328981950879097\n",
      "expression length:\t 2\n",
      "Result stage 178: 6.658*cos(x0) + 0.343*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  177\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289836645126343\n",
      "\n",
      "\n",
      "Stage  177\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289865329861641\n",
      "\n",
      "\n",
      "Stage  177\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289898857474327\n",
      "\n",
      "\n",
      "Stage  177\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328989177942276\n",
      "expression length:\t 2\n",
      "Result stage 179: 6.657*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  178\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899048179388046\n",
      "\n",
      "\n",
      "Stage  178\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289906308054924\n",
      "\n",
      "\n",
      "Stage  178\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898858189582825\n",
      "\n",
      "\n",
      "Stage  178\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898709177970886\n",
      "expression length:\t 2\n",
      "Result stage 180: 6.655*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  179\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289863094687462\n",
      "\n",
      "\n",
      "Stage  179\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289872407913208\n",
      "\n",
      "\n",
      "Stage  179\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289887681603432\n",
      "\n",
      "\n",
      "Stage  179\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289857134222984\n",
      "expression length:\t 2\n",
      "Result stage 181: 6.654*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  180\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289858624339104\n",
      "\n",
      "\n",
      "Stage  180\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289874270558357\n",
      "\n",
      "\n",
      "Stage  180\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289845958352089\n",
      "\n",
      "\n",
      "Stage  180\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289845213294029\n",
      "expression length:\t 2\n",
      "Result stage 182: 6.653*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  181\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898563891649246\n",
      "\n",
      "\n",
      "Stage  181\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289848938584328\n",
      "\n",
      "\n",
      "Stage  181\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289874270558357\n",
      "\n",
      "\n",
      "Stage  181\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898347824811935\n",
      "expression length:\t 2\n",
      "Result stage 183: 6.651*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  182\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898008823394775\n",
      "\n",
      "\n",
      "Stage  182\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328979454934597\n",
      "\n",
      "\n",
      "Stage  182\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289809077978134\n",
      "\n",
      "\n",
      "Stage  182\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289787098765373\n",
      "expression length:\t 2\n",
      "Result stage 184: 6.65*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  183\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289781138300896\n",
      "\n",
      "\n",
      "Stage  183\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289797529578209\n",
      "\n",
      "\n",
      "Stage  183\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898180186748505\n",
      "\n",
      "\n",
      "Stage  183\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289807215332985\n",
      "expression length:\t 2\n",
      "Result stage 185: 6.649*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  184\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289809450507164\n",
      "\n",
      "\n",
      "Stage  184\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289801627397537\n",
      "\n",
      "\n",
      "Stage  184\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289802744984627\n",
      "\n",
      "\n",
      "Stage  184\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898034900426865\n",
      "expression length:\t 2\n",
      "Result stage 186: 6.647*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  185\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898273319005966\n",
      "\n",
      "\n",
      "Stage  185\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289806842803955\n",
      "\n",
      "\n",
      "Stage  185\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289807215332985\n",
      "\n",
      "\n",
      "Stage  185\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289814665913582\n",
      "expression length:\t 2\n",
      "Result stage 187: 6.646*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  186\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289914131164551\n",
      "\n",
      "\n",
      "Stage  186\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289918228983879\n",
      "\n",
      "\n",
      "Stage  186\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289905562996864\n",
      "\n",
      "\n",
      "Stage  186\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289894759654999\n",
      "expression length:\t 2\n",
      "Result stage 188: 6.645*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  187\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289889544248581\n",
      "\n",
      "\n",
      "Stage  187\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289876505732536\n",
      "\n",
      "\n",
      "Stage  187\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898638397455215\n",
      "\n",
      "\n",
      "Stage  187\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328986831009388\n",
      "expression length:\t 2\n",
      "Result stage 189: 6.643*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  188\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289853036403656\n",
      "\n",
      "\n",
      "Stage  188\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898399978876114\n",
      "\n",
      "\n",
      "Stage  188\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289828076958656\n",
      "\n",
      "\n",
      "Stage  188\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289828449487686\n",
      "expression length:\t 2\n",
      "Result stage 190: 6.642*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  189\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289824724197388\n",
      "\n",
      "\n",
      "Stage  189\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289821371436119\n",
      "\n",
      "\n",
      "Stage  189\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898083329200745\n",
      "\n",
      "\n",
      "Stage  189\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897669821977615\n",
      "expression length:\t 2\n",
      "Result stage 191: 6.641*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  190\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289793059229851\n",
      "\n",
      "\n",
      "Stage  190\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898202538490295\n",
      "\n",
      "\n",
      "Stage  190\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898180186748505\n",
      "\n",
      "\n",
      "Stage  190\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289797902107239\n",
      "expression length:\t 2\n",
      "Result stage 192: 6.639*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  191\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289790078997612\n",
      "\n",
      "\n",
      "Stage  191\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328977108001709\n",
      "\n",
      "\n",
      "Stage  191\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897625118494034\n",
      "\n",
      "\n",
      "Stage  191\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289749100804329\n",
      "expression length:\t 2\n",
      "Result stage 193: 6.638*cos(x0) + 0.342*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  192\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289754316210747\n",
      "\n",
      "\n",
      "Stage  192\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897211611270905\n",
      "\n",
      "\n",
      "Stage  192\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289709985256195\n",
      "\n",
      "\n",
      "Stage  192\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289739415049553\n",
      "expression length:\t 2\n",
      "Result stage 194: 6.637*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  193\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289768472313881\n",
      "\n",
      "\n",
      "Stage  193\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289799019694328\n",
      "\n",
      "\n",
      "Stage  193\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898254692554474\n",
      "\n",
      "\n",
      "Stage  193\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289838507771492\n",
      "expression length:\t 2\n",
      "Result stage 195: 6.635*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  194\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289831429719925\n",
      "\n",
      "\n",
      "Stage  194\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289824351668358\n",
      "\n",
      "\n",
      "Stage  194\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289823606610298\n",
      "\n",
      "\n",
      "Stage  194\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289812430739403\n",
      "expression length:\t 2\n",
      "Result stage 196: 6.634*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  195\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289806842803955\n",
      "\n",
      "\n",
      "Stage  195\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289801627397537\n",
      "\n",
      "\n",
      "Stage  195\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289812058210373\n",
      "\n",
      "\n",
      "Stage  195\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898254692554474\n",
      "expression length:\t 2\n",
      "Result stage 197: 6.633*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  196\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898761332035065\n",
      "\n",
      "\n",
      "Stage  196\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289881348609924\n",
      "\n",
      "\n",
      "Stage  196\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289908543229103\n",
      "\n",
      "\n",
      "Stage  196\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289905935525894\n",
      "expression length:\t 2\n",
      "Result stage 198: 6.631*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  197\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899029552936554\n",
      "\n",
      "\n",
      "Stage  197\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898835837841034\n",
      "\n",
      "\n",
      "Stage  197\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289899602532387\n",
      "\n",
      "\n",
      "Stage  197\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289860486984253\n",
      "expression length:\t 2\n",
      "Result stage 199: 6.63*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  198\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289875015616417\n",
      "\n",
      "\n",
      "Stage  198\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289862722158432\n",
      "\n",
      "\n",
      "Stage  198\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289846330881119\n",
      "\n",
      "\n",
      "Stage  198\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898567616939545\n",
      "expression length:\t 2\n",
      "Result stage 200: 6.629*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  199\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289856016635895\n",
      "\n",
      "\n",
      "Stage  199\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898422330617905\n",
      "\n",
      "\n",
      "Stage  199\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898273319005966\n",
      "\n",
      "\n",
      "Stage  199\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289814665913582\n",
      "expression length:\t 2\n",
      "Result stage 201: 6.627*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  200\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898109406232834\n",
      "\n",
      "\n",
      "Stage  200\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898128032684326\n",
      "\n",
      "\n",
      "Stage  200\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897982746362686\n",
      "\n",
      "\n",
      "Stage  200\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289783373475075\n",
      "expression length:\t 2\n",
      "Result stage 202: 6.626*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  201\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289779648184776\n",
      "\n",
      "\n",
      "Stage  201\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289758786559105\n",
      "\n",
      "\n",
      "Stage  201\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897450029850006\n",
      "\n",
      "\n",
      "Stage  201\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289731219410896\n",
      "expression length:\t 2\n",
      "Result stage 203: 6.625*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  202\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289744257926941\n",
      "\n",
      "\n",
      "Stage  202\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289702162146568\n",
      "\n",
      "\n",
      "Stage  202\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289692476391792\n",
      "\n",
      "\n",
      "Stage  202\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289693966507912\n",
      "expression length:\t 2\n",
      "Result stage 204: 6.623*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  203\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289719298481941\n",
      "\n",
      "\n",
      "Stage  203\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328972265124321\n",
      "\n",
      "\n",
      "Stage  203\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289753943681717\n",
      "\n",
      "\n",
      "Stage  203\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289768099784851\n",
      "expression length:\t 2\n",
      "Result stage 205: 6.622*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  204\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328977033495903\n",
      "\n",
      "\n",
      "Stage  204\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289748355746269\n",
      "\n",
      "\n",
      "Stage  204\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897260040044785\n",
      "\n",
      "\n",
      "Stage  204\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897088676691055\n",
      "expression length:\t 2\n",
      "Result stage 206: 6.621*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  205\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898224890232086\n",
      "\n",
      "\n",
      "Stage  205\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289801627397537\n",
      "\n",
      "\n",
      "Stage  205\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897789031267166\n",
      "\n",
      "\n",
      "Stage  205\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289758786559105\n",
      "expression length:\t 2\n",
      "Result stage 207: 6.619*cos(x0) + 0.341*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  206\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289879485964775\n",
      "\n",
      "\n",
      "Stage  206\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289858624339104\n",
      "\n",
      "\n",
      "Stage  206\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289837762713432\n",
      "\n",
      "\n",
      "Stage  206\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289816528558731\n",
      "expression length:\t 2\n",
      "Result stage 208: 6.618*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  207\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289913013577461\n",
      "\n",
      "\n",
      "Stage  207\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328989140689373\n",
      "\n",
      "\n",
      "Stage  207\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289869800209999\n",
      "\n",
      "\n",
      "Stage  207\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289848566055298\n",
      "expression length:\t 2\n",
      "Result stage 209: 6.617*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  208\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289896994829178\n",
      "\n",
      "\n",
      "Stage  208\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289876505732536\n",
      "\n",
      "\n",
      "Stage  208\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289853036403656\n",
      "\n",
      "\n",
      "Stage  208\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289834037423134\n",
      "expression length:\t 2\n",
      "Result stage 210: 6.615*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  209\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289872407913208\n",
      "\n",
      "\n",
      "Stage  209\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289848938584328\n",
      "\n",
      "\n",
      "Stage  209\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289835900068283\n",
      "\n",
      "\n",
      "Stage  209\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289882466197014\n",
      "expression length:\t 2\n",
      "Result stage 211: 6.614*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  210\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328986831009388\n",
      "\n",
      "\n",
      "Stage  210\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289885073900223\n",
      "\n",
      "\n",
      "Stage  210\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289913758635521\n",
      "\n",
      "\n",
      "Stage  210\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289913758635521\n",
      "expression length:\t 2\n",
      "Result stage 212: 6.613*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  211\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289930149912834\n",
      "\n",
      "\n",
      "Stage  211\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899029552936554\n",
      "\n",
      "\n",
      "Stage  211\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289900720119476\n",
      "\n",
      "\n",
      "Stage  211\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899100333452225\n",
      "expression length:\t 2\n",
      "Result stage 213: 6.611*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  212\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289907053112984\n",
      "\n",
      "\n",
      "Stage  212\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899029552936554\n",
      "\n",
      "\n",
      "Stage  212\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289900720119476\n",
      "\n",
      "\n",
      "Stage  212\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328986831009388\n",
      "expression length:\t 2\n",
      "Result stage 214: 6.61*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  213\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898664474487305\n",
      "\n",
      "\n",
      "Stage  213\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289886564016342\n",
      "\n",
      "\n",
      "Stage  213\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289898484945297\n",
      "\n",
      "\n",
      "Stage  213\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289942815899849\n",
      "expression length:\t 2\n",
      "Result stage 215: 6.609*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  214\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289974108338356\n",
      "\n",
      "\n",
      "Stage  214\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289990872144699\n",
      "\n",
      "\n",
      "Stage  214\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289983794093132\n",
      "\n",
      "\n",
      "Stage  214\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289970010519028\n",
      "expression length:\t 2\n",
      "Result stage 216: 6.607*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  215\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289966657757759\n",
      "\n",
      "\n",
      "Stage  215\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289951756596565\n",
      "\n",
      "\n",
      "Stage  215\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289971128106117\n",
      "\n",
      "\n",
      "Stage  215\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289996087551117\n",
      "expression length:\t 2\n",
      "Result stage 217: 6.606*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  216\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032900191843509674\n",
      "\n",
      "\n",
      "Stage  216\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032900210469961166\n",
      "\n",
      "\n",
      "Stage  216\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03290039300918579\n",
      "\n",
      "\n",
      "Stage  216\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289986774325371\n",
      "expression length:\t 2\n",
      "Result stage 218: 6.605*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  217\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03290029615163803\n",
      "\n",
      "\n",
      "Stage  217\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032900094985961914\n",
      "\n",
      "\n",
      "Stage  217\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032900456339120865\n",
      "\n",
      "\n",
      "Stage  217\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03290041163563728\n",
      "expression length:\t 2\n",
      "Result stage 219: 6.604*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  218\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032900162041187286\n",
      "\n",
      "\n",
      "Stage  218\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032900113612413406\n",
      "\n",
      "\n",
      "Stage  218\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03290005773305893\n",
      "\n",
      "\n",
      "Stage  218\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03290000557899475\n",
      "expression length:\t 2\n",
      "Result stage 220: 6.603*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  219\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899949699640274\n",
      "\n",
      "\n",
      "Stage  219\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899901270866394\n",
      "\n",
      "\n",
      "Stage  219\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289986401796341\n",
      "\n",
      "\n",
      "Stage  219\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899800688028336\n",
      "expression length:\t 2\n",
      "Result stage 221: 6.603*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  220\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289957717061043\n",
      "\n",
      "\n",
      "Stage  220\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899558544158936\n",
      "\n",
      "\n",
      "Stage  220\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899945974349976\n",
      "\n",
      "\n",
      "Stage  220\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328998938202858\n",
      "expression length:\t 2\n",
      "Result stage 222: 6.602*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  221\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899409532547\n",
      "\n",
      "\n",
      "Stage  221\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289938345551491\n",
      "\n",
      "\n",
      "Stage  221\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899558544158936\n",
      "\n",
      "\n",
      "Stage  221\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289952874183655\n",
      "expression length:\t 2\n",
      "Result stage 223: 6.602*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  222\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289949893951416\n",
      "\n",
      "\n",
      "Stage  222\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328993946313858\n",
      "\n",
      "\n",
      "Stage  222\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289962559938431\n",
      "\n",
      "\n",
      "Stage  222\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899368554353714\n",
      "expression length:\t 2\n",
      "Result stage 224: 6.601*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  223\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289932757616043\n",
      "\n",
      "\n",
      "Stage  223\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289923071861267\n",
      "\n",
      "\n",
      "Stage  223\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899465411901474\n",
      "\n",
      "\n",
      "Stage  223\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289921209216118\n",
      "expression length:\t 2\n",
      "Result stage 225: 6.6*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  224\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289936110377312\n",
      "\n",
      "\n",
      "Stage  224\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899096608161926\n",
      "\n",
      "\n",
      "Stage  224\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289901837706566\n",
      "\n",
      "\n",
      "Stage  224\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899241894483566\n",
      "expression length:\t 2\n",
      "Result stage 226: 6.6*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  225\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898951321840286\n",
      "\n",
      "\n",
      "Stage  225\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289918228983879\n",
      "\n",
      "\n",
      "Stage  225\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289894014596939\n",
      "\n",
      "\n",
      "Stage  225\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289909288287163\n",
      "expression length:\t 2\n",
      "Result stage 227: 6.599*cos(x0) + 0.34*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  226\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289901092648506\n",
      "\n",
      "\n",
      "Stage  226\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289872407913208\n",
      "\n",
      "\n",
      "Stage  226\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032899048179388046\n",
      "\n",
      "\n",
      "Stage  226\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289903327822685\n",
      "expression length:\t 2\n",
      "Result stage 228: 6.599*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  227\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289896622300148\n",
      "\n",
      "\n",
      "Stage  227\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289856016635895\n",
      "\n",
      "\n",
      "Stage  227\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289887681603432\n",
      "\n",
      "\n",
      "Stage  227\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898806035518646\n",
      "expression length:\t 2\n",
      "Result stage 229: 6.598*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  228\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898347824811935\n",
      "\n",
      "\n",
      "Stage  228\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289869800209999\n",
      "\n",
      "\n",
      "Stage  228\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328984335064888\n",
      "\n",
      "\n",
      "Stage  228\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898176461458206\n",
      "expression length:\t 2\n",
      "Result stage 230: 6.598*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  229\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898131757974625\n",
      "\n",
      "\n",
      "Stage  229\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289829567074776\n",
      "\n",
      "\n",
      "Stage  229\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289833292365074\n",
      "\n",
      "\n",
      "Stage  229\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289828076958656\n",
      "expression length:\t 2\n",
      "Result stage 231: 6.597*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  230\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898228615522385\n",
      "\n",
      "\n",
      "Stage  230\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898224890232086\n",
      "\n",
      "\n",
      "Stage  230\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289813920855522\n",
      "\n",
      "\n",
      "Stage  230\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898131757974625\n",
      "expression length:\t 2\n",
      "Result stage 232: 6.596*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  231\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289806470274925\n",
      "\n",
      "\n",
      "Stage  231\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289806470274925\n",
      "\n",
      "\n",
      "Stage  231\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289797902107239\n",
      "\n",
      "\n",
      "Stage  231\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289797157049179\n",
      "expression length:\t 2\n",
      "Result stage 233: 6.596*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  232\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898031175136566\n",
      "\n",
      "\n",
      "Stage  232\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897885888814926\n",
      "\n",
      "\n",
      "Stage  232\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328979566693306\n",
      "\n",
      "\n",
      "Stage  232\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289779648184776\n",
      "expression length:\t 2\n",
      "Result stage 234: 6.595*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  233\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897740602493286\n",
      "\n",
      "\n",
      "Stage  233\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289773687720299\n",
      "\n",
      "\n",
      "Stage  233\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289785236120224\n",
      "\n",
      "\n",
      "Stage  233\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289756551384926\n",
      "expression length:\t 2\n",
      "Result stage 235: 6.595*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  234\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289774805307388\n",
      "\n",
      "\n",
      "Stage  234\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289744257926941\n",
      "\n",
      "\n",
      "Stage  234\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897692173719406\n",
      "\n",
      "\n",
      "Stage  234\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289739415049553\n",
      "expression length:\t 2\n",
      "Result stage 236: 6.594*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  235\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289761766791344\n",
      "\n",
      "\n",
      "Stage  235\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897502183914185\n",
      "\n",
      "\n",
      "Stage  235\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897572964429855\n",
      "\n",
      "\n",
      "Stage  235\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328974612057209\n",
      "expression length:\t 2\n",
      "Result stage 237: 6.594*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  236\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289743885397911\n",
      "\n",
      "\n",
      "Stage  236\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289716690778732\n",
      "\n",
      "\n",
      "Stage  236\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289730101823807\n",
      "\n",
      "\n",
      "Stage  236\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897282391786575\n",
      "expression length:\t 2\n",
      "Result stage 238: 6.593*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  237\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897304743528366\n",
      "\n",
      "\n",
      "Stage  237\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289731964468956\n",
      "\n",
      "\n",
      "Stage  237\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289743512868881\n",
      "\n",
      "\n",
      "Stage  237\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897356897592545\n",
      "expression length:\t 2\n",
      "Result stage 239: 6.592*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  238\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289736062288284\n",
      "\n",
      "\n",
      "Stage  238\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289748355746269\n",
      "\n",
      "\n",
      "Stage  238\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289740905165672\n",
      "\n",
      "\n",
      "Stage  238\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289740905165672\n",
      "expression length:\t 2\n",
      "Result stage 240: 6.592*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  239\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289743512868881\n",
      "\n",
      "\n",
      "Stage  239\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289741650223732\n",
      "\n",
      "\n",
      "Stage  239\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897479832172394\n",
      "\n",
      "\n",
      "Stage  239\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897479832172394\n",
      "expression length:\t 2\n",
      "Result stage 241: 6.591*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  240\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289751335978508\n",
      "\n",
      "\n",
      "Stage  240\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289749100804329\n",
      "\n",
      "\n",
      "Stage  240\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289756178855896\n",
      "\n",
      "\n",
      "Stage  240\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289756551384926\n",
      "expression length:\t 2\n",
      "Result stage 242: 6.591*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  241\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289756551384926\n",
      "\n",
      "\n",
      "Stage  241\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897550612688065\n",
      "\n",
      "\n",
      "Stage  241\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289761766791344\n",
      "\n",
      "\n",
      "Stage  241\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289761394262314\n",
      "expression length:\t 2\n",
      "Result stage 243: 6.59*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  242\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897718250751495\n",
      "\n",
      "\n",
      "Stage  242\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897647470235825\n",
      "\n",
      "\n",
      "Stage  242\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289764001965523\n",
      "\n",
      "\n",
      "Stage  242\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328977070748806\n",
      "expression length:\t 2\n",
      "Result stage 244: 6.59*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  243\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328977033495903\n",
      "\n",
      "\n",
      "Stage  243\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328977033495903\n",
      "\n",
      "\n",
      "Stage  243\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897770404815674\n",
      "\n",
      "\n",
      "Stage  243\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289775177836418\n",
      "expression length:\t 2\n",
      "Result stage 245: 6.589*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  244\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897789031267166\n",
      "\n",
      "\n",
      "Stage  244\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289777785539627\n",
      "\n",
      "\n",
      "Stage  244\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897911965847015\n",
      "\n",
      "\n",
      "Stage  244\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897837460041046\n",
      "expression length:\t 2\n",
      "Result stage 246: 6.588*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  245\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897837460041046\n",
      "\n",
      "\n",
      "Stage  245\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289781138300896\n",
      "\n",
      "\n",
      "Stage  245\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897837460041046\n",
      "\n",
      "\n",
      "Stage  245\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289783373475075\n",
      "expression length:\t 2\n",
      "Result stage 247: 6.588*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  246\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289787098765373\n",
      "\n",
      "\n",
      "Stage  246\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289788216352463\n",
      "\n",
      "\n",
      "Stage  246\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897889614105225\n",
      "\n",
      "\n",
      "Stage  246\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897964119911194\n",
      "expression length:\t 2\n",
      "Result stage 248: 6.587*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  247\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289792686700821\n",
      "\n",
      "\n",
      "Stage  247\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289790451526642\n",
      "\n",
      "\n",
      "Stage  247\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032897863537073135\n",
      "\n",
      "\n",
      "Stage  247\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289785236120224\n",
      "expression length:\t 2\n",
      "Result stage 249: 6.587*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  248\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289797529578209\n",
      "\n",
      "\n",
      "Stage  248\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.0328979417681694\n",
      "\n",
      "\n",
      "Stage  248\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289790824055672\n",
      "\n",
      "\n",
      "Stage  248\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289802744984627\n",
      "expression length:\t 2\n",
      "Result stage 250: 6.586*cos(x0) + 0.339*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  249\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289809450507164\n",
      "\n",
      "\n",
      "Stage  249\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.032898083329200745\n",
      "\n",
      "\n",
      "Stage  249\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289814665913582\n",
      "\n",
      "\n",
      "Stage  249\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-06\n",
      "Average loss :  0.03289813920855522\n",
      "expression length:\t 2\n",
      "Result stage 251: 6.586*cos(x0) + 0.339*x0_t**2 + \n"
     ]
    }
   ],
   "source": [
    "for stage in range(250):\n",
    "\n",
    "    #Redefine computation after thresholding\n",
    "    Zeta, Eta, Delta, Dissip = LagrangianLibraryTensor(X,Xdot,expr,d_expr,states,states_dot,device,scaling=True)\n",
    "    Eta = Eta.to(device)\n",
    "    Zeta = Zeta.to(device)\n",
    "    Delta = Delta.to(device)\n",
    "    Dissip = Dissip.to(device)\n",
    "\n",
    "    #Training\n",
    "    Epoch = 200\n",
    "    i = 1\n",
    "    lr = 1e-5\n",
    "    if(stage==1):\n",
    "        lam = 0\n",
    "    else:\n",
    "        lam = 0.1\n",
    "    temp = 1000\n",
    "    if stage >= 170:\n",
    "        lr = 1e-6\n",
    "    while(i<=Epoch):   \n",
    "        xi_L , xi_d, prevxi_L, prevxi_d, lossitem= training_loop(xi_L,xi_d,prevxi_L,prevxi_d,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "        if i %50 == 0:\n",
    "            print(\"\\n\")\n",
    "            print(\"Stage \",stage)\n",
    "            print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "            print(\"Learning rate : \", lr)\n",
    "            print(\"Average loss : \" , lossitem)\n",
    "        temp = lossitem\n",
    "        if(temp <=1e-6):\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    ## Thresholding small indices ##\n",
    "    threshold = 2e-1\n",
    "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "    expr = np.array(expr)[surv_index].tolist()\n",
    "    surv_index_d = ((torch.abs(xi_d) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "    d_expr = np.array(d_expr)[surv_index_d].tolist()\n",
    "\n",
    "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "    xi_d =xi_d[surv_index_d].clone().detach().requires_grad_(True)\n",
    "    prevxi_L = xi_L.clone().detach()\n",
    "    prevxi_d = xi_d.clone().detach()\n",
    "    mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "    ## obtaining analytical model\n",
    "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
    "    xi_dcpu = np.around(xi_d.detach().cpu().numpy(),decimals=2)\n",
    "    L = HL.generateExpression(xi_Lcpu,expr)\n",
    "    D = HL.generateExpression(xi_dcpu,d_expr)\n",
    "    print(\"expression length:\\t\",len(xi_L))\n",
    "    print(\"Result stage \" + str(stage+2) + \":\" , L)\n",
    "    print(\"Dissipation function\": D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda7f63-7451-4f1f-8bb8-6d994abdedee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8813673-d9ae-4769-9aa8-dc6b89ba4f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
