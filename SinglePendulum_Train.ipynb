{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21abee7a-e86d-4999-a297-36f121b1dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Data_generator.ipynb\n",
      "2.846567707766985e-79\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "import import_ipynb\n",
    "\n",
    "from sympy import symbols, simplify, derive_by_array\n",
    "from scipy.integrate import solve_ivp\n",
    "from xLSINDy_sp import *\n",
    "from sympy.physics.mechanics import *\n",
    "from sympy import *\n",
    "from Data_generator import image_process\n",
    "import sympy\n",
    "import torch\n",
    "import sys\n",
    "import HLsearch as HL\n",
    "import example_pendulum\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3931f0-8bba-44e2-8d56-b263f77bac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "environment = \"laptop\"\n",
    "sample_size = 10\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e864928-be31-4c73-b320-bfe0a2143ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4998, 2)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(r'../../../HLsearch/')\n",
    "#Saving Directory\n",
    "if environment == 'laptop':\n",
    "    root_dir =R'C:\\Users\\87106\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'desktop':\n",
    "    root_dir = R'E:\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'server':\n",
    "    root_dir = R'/home/stilrmy/Angle_extractor'\n",
    "x,dx,ddx = image_process(sample_size)\n",
    "X = []\n",
    "Xdot = []\n",
    "for i in range(len(x)):\n",
    "    temp_list = [float(x[i]),float(dx[i])]\n",
    "    X.append(temp_list)\n",
    "    temp_list = [float(dx[i]),float(ddx[i])]\n",
    "    Xdot.append(temp_list)\n",
    "X = np.vstack(X)\n",
    "print(X.shape)\n",
    "Xdot = np.vstack(Xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8250a64-b76e-48f3-810e-28b181740486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states are: (x0, x0_t)\n",
      "states derivatives are:  (x0_t, x0_tt)\n"
     ]
    }
   ],
   "source": [
    "states_dim = 2\n",
    "states = ()\n",
    "states_dot = ()\n",
    "for i in range(states_dim):\n",
    "    if(i<states_dim//2):\n",
    "        states = states + (symbols('x{}'.format(i)),)\n",
    "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
    "    else:\n",
    "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
    "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
    "print('states are:',states)\n",
    "print('states derivatives are: ', states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c6dddc-ac3c-4e37-91dc-ac6bab075c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn from sympy to str\n",
    "states_sym = states\n",
    "states_dot_sym = states_dot\n",
    "states = list(str(descr) for descr in states)\n",
    "states_dot = list(str(descr) for descr in states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f71e4d-17ce-4af8-9d22-dc28cfc93f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'x0*x0_t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build function expression for the library in str\n",
    "expr= HL.buildFunctionExpressions(2,states_dim,states,use_sine=True)\n",
    "#expr=['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n",
    "\"a list of candidate function\"\n",
    "print(expr)\n",
    "expr.pop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ad60fa-1b07-423b-b96d-be7e84e9f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=True)\n",
    "Eta = Eta.to(device)\n",
    "Zeta = Zeta.to(device)\n",
    "Delta = Delta.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e559dd-c8b0-4041-b1f8-64e6124095be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(len(expr),device=device)\n",
    "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
    "prevxi_L = xi_L.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8134ecd-8d51-42b3-bd18-0a0f50901312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, targ):\n",
    "    loss = torch.mean((pred - targ)**2) \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c008f0b2-21b3-4433-afbf-8353931150dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(w, alpha):\n",
    "    clipped = torch.minimum(w,alpha)\n",
    "    clipped = torch.maximum(clipped,-alpha)\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16edbe1-038d-45e5-b403-07e08b441d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxL1norm(w_hat, alpha):\n",
    "    if(torch.is_tensor(alpha)==False):\n",
    "        alpha = torch.tensor(alpha)\n",
    "    w = w_hat - clip(w_hat,alpha)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8dffb9-95ba-4ef5-8987-6cc6a2e20a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(coef, prevcoef, Zeta, Eta, Delta,xdot, bs, lr, lam):\n",
    "    loss_list = []\n",
    "    tl = xdot.shape[0]\n",
    "    n = xdot.shape[1]\n",
    "    momentum = True\n",
    "    #if(torch.is_tensor(xdot)==False):\n",
    "        #xdot = torch.from_numpy(xdot).to(device).float()\n",
    "    v = coef.clone().detach().to(device).requires_grad_(True)\n",
    "    prev = prevcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    for i in range(tl//bs):\n",
    "        #computing acceleration with momentum\n",
    "        #if (momentum == True):\n",
    "            #vhat = (v + ((i - 1) / (i + 2)) * (v - prev)).clone().detach().requires_grad_(True)\n",
    "       # else:\n",
    "        vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "        prev = v\n",
    "        #Computing loss\n",
    "        zeta = Zeta[:,i*bs:(i+1)*bs]\n",
    "        eta = Eta[:,i*bs:(i+1)*bs]\n",
    "        delta = Delta[:,i*bs:(i+1)*bs]\n",
    "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
    "        #forward\n",
    "        q_tt_pred = lagrangianforward(vhat,zeta,eta,delta,x_t,device)\n",
    "        q_tt_true = xdot[i*bs:(i+1)*bs,n//2:].T\n",
    "        q_tt_true = torch.from_numpy(q_tt_true).to(device).float()\n",
    "        lossval = loss(q_tt_pred, q_tt_true)\n",
    "        lossval.requires_grad_(True)\n",
    "        #Backpropagation\n",
    "        lossval.backward()\n",
    "        with torch.no_grad():\n",
    "            v = vhat - lr * vhat.grad\n",
    "            v = proxL1norm(v, lr * lam)\n",
    "            # Manually zero the gradients after updating weights\n",
    "            vhat.grad = None\n",
    "        loss_list.append(lossval.item())\n",
    "    return v, prev, torch.tensor(loss_list).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9ace20-122c-4072-9ccf-9a3bd554abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 20/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  35.04500198364258\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 40/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  34.69642639160156\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 60/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  34.292083740234375\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 80/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  33.801658630371094\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 100/100\n",
      "Learning rate :  0.001\n",
      "Average loss :  33.168701171875\n"
     ]
    }
   ],
   "source": [
    "Epoch = 100\n",
    "i = 1\n",
    "lr = 1e-3\n",
    "lam = 0.1\n",
    "temp = 1000\n",
    "while(i<=Epoch):\n",
    "    xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "    if i %20 == 0:\n",
    "        print(\"\\n\")\n",
    "        print(\"Stage 1\")\n",
    "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "        print(\"Learning rate : \", lr)\n",
    "        print(\"Average loss : \" , lossitem)\n",
    "    if(temp <=5e-3):\n",
    "        break\n",
    "    if(temp <=1e-1):\n",
    "        lr = 1e-5\n",
    "    temp = lossitem\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04e39cf-6dac-45ba-8c38-8bfeaf4626b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "-1.24*x0 + -8.86*sin(x0) + 9.74*cos(x0) + 7.64*x0**2 + 4.46*x0_t**2 + -2.65*x0*sin(x0) + 5.52*x0_t*sin(x0) + -6.05*sin(x0)**2 + 7.89*x0*cos(x0) + -7.09*x0_t*cos(x0) + -8.0*sin(x0)*cos(x0) + -0.41*cos(x0)**2 + \n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-2\n",
    "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "## obtaining analytical model\n",
    "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
    "L = HL.generateExpression(xi_Lcpu,expr)\n",
    "print(len(xi_L))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fbbf84e-74af-4e42-a56c-c701b029dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08848682045936584\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08847572654485703\n",
      "expression length:\t 2\n",
      "Result stage 2: 5.945*cos(x0) + 0.305*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08842732012271881\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08842746168375015\n",
      "expression length:\t 2\n",
      "Result stage 3: 5.946*cos(x0) + 0.305*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08846458047628403\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08845273405313492\n",
      "expression length:\t 2\n",
      "Result stage 4: 5.905*cos(x0) + 0.303*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08844084292650223\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08842861652374268\n",
      "expression length:\t 2\n",
      "Result stage 5: 5.864*cos(x0) + 0.301*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08841604739427567\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0884031280875206\n",
      "expression length:\t 2\n",
      "Result stage 6: 5.823*cos(x0) + 0.299*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.088390052318573\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0883764773607254\n",
      "expression length:\t 2\n",
      "Result stage 7: 5.783*cos(x0) + 0.297*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0883626937866211\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08834847062826157\n",
      "expression length:\t 2\n",
      "Result stage 8: 5.742*cos(x0) + 0.294*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0883341059088707\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08831927180290222\n",
      "expression length:\t 2\n",
      "Result stage 9: 5.701*cos(x0) + 0.292*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08830413222312927\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08828864991664886\n",
      "expression length:\t 2\n",
      "Result stage 10: 5.661*cos(x0) + 0.29*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08827277272939682\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08825655281543732\n",
      "expression length:\t 2\n",
      "Result stage 11: 5.62*cos(x0) + 0.288*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08824002742767334\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08822313696146011\n",
      "expression length:\t 2\n",
      "Result stage 12: 5.579*cos(x0) + 0.286*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08820591866970062\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0881882905960083\n",
      "expression length:\t 2\n",
      "Result stage 13: 5.539*cos(x0) + 0.284*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.088170126080513\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08815181255340576\n",
      "expression length:\t 2\n",
      "Result stage 14: 5.498*cos(x0) + 0.282*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08813294023275375\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08811374753713608\n",
      "expression length:\t 2\n",
      "Result stage 15: 5.458*cos(x0) + 0.28*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08809404820203781\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08807394653558731\n",
      "expression length:\t 2\n",
      "Result stage 16: 5.417*cos(x0) + 0.278*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08805353194475174\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.088032566010952\n",
      "expression length:\t 2\n",
      "Result stage 17: 5.377*cos(x0) + 0.275*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0880112498998642\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08798948675394058\n",
      "expression length:\t 2\n",
      "Result stage 18: 5.336*cos(x0) + 0.273*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08796714246273041\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08794448524713516\n",
      "expression length:\t 2\n",
      "Result stage 19: 5.296*cos(x0) + 0.271*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0879213809967041\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08789759874343872\n",
      "expression length:\t 2\n",
      "Result stage 20: 5.255*cos(x0) + 0.269*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08787354081869125\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08784893155097961\n",
      "expression length:\t 2\n",
      "Result stage 21: 5.214*cos(x0) + 0.267*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08782391995191574\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0877983421087265\n",
      "expression length:\t 2\n",
      "Result stage 22: 5.174*cos(x0) + 0.265*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08777238428592682\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08774590492248535\n",
      "expression length:\t 2\n",
      "Result stage 23: 5.133*cos(x0) + 0.263*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08771893382072449\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08769122511148453\n",
      "expression length:\t 2\n",
      "Result stage 24: 5.093*cos(x0) + 0.261*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08766314387321472\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08763445168733597\n",
      "expression length:\t 2\n",
      "Result stage 25: 5.052*cos(x0) + 0.259*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0876053050160408\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08757542818784714\n",
      "expression length:\t 2\n",
      "Result stage 26: 5.011*cos(x0) + 0.256*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08754515647888184\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08751428872346878\n",
      "expression length:\t 2\n",
      "Result stage 27: 4.971*cos(x0) + 0.254*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08748304843902588\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08745113760232925\n",
      "expression length:\t 2\n",
      "Result stage 28: 4.93*cos(x0) + 0.252*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08741860091686249\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08738549053668976\n",
      "expression length:\t 2\n",
      "Result stage 29: 4.89*cos(x0) + 0.25*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08735193312168121\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0873178169131279\n",
      "expression length:\t 2\n",
      "Result stage 30: 4.85*cos(x0) + 0.248*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08728302270174026\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08724763244390488\n",
      "expression length:\t 2\n",
      "Result stage 31: 4.809*cos(x0) + 0.246*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0872114971280098\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08717496693134308\n",
      "expression length:\t 2\n",
      "Result stage 32: 4.769*cos(x0) + 0.244*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08713767677545547\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08709976077079773\n",
      "expression length:\t 2\n",
      "Result stage 33: 4.728*cos(x0) + 0.242*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08706137537956238\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0870223194360733\n",
      "expression length:\t 2\n",
      "Result stage 34: 4.688*cos(x0) + 0.24*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08698248118162155\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08694208413362503\n",
      "expression length:\t 2\n",
      "Result stage 35: 4.647*cos(x0) + 0.238*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08690091967582703\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08685917407274246\n",
      "expression length:\t 2\n",
      "Result stage 36: 4.607*cos(x0) + 0.235*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08681680262088776\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08677371591329575\n",
      "expression length:\t 2\n",
      "Result stage 37: 4.567*cos(x0) + 0.233*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08672979474067688\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08668521791696548\n",
      "expression length:\t 2\n",
      "Result stage 38: 4.526*cos(x0) + 0.231*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08663994073867798\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08659403026103973\n",
      "expression length:\t 2\n",
      "Result stage 39: 4.486*cos(x0) + 0.229*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08654740452766418\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08650016784667969\n",
      "expression length:\t 2\n",
      "Result stage 40: 4.445*cos(x0) + 0.227*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0864521935582161\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08640348166227341\n",
      "expression length:\t 2\n",
      "Result stage 41: 4.405*cos(x0) + 0.225*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08635400235652924\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08630385249853134\n",
      "expression length:\t 2\n",
      "Result stage 42: 4.365*cos(x0) + 0.223*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0862528458237648\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08620110899209976\n",
      "expression length:\t 2\n",
      "Result stage 43: 4.324*cos(x0) + 0.221*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08614858239889145\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08609548956155777\n",
      "expression length:\t 2\n",
      "Result stage 44: 4.284*cos(x0) + 0.219*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0860416367650032\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08598703145980835\n",
      "expression length:\t 2\n",
      "Result stage 45: 4.244*cos(x0) + 0.217*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08593156933784485\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08587541431188583\n",
      "expression length:\t 2\n",
      "Result stage 46: 4.203*cos(x0) + 0.215*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08581847697496414\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08576060086488724\n",
      "expression length:\t 2\n",
      "Result stage 47: 4.163*cos(x0) + 0.212*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08570186793804169\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08564228564500809\n",
      "expression length:\t 2\n",
      "Result stage 48: 4.123*cos(x0) + 0.21*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08558198064565659\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08552081137895584\n",
      "expression length:\t 2\n",
      "Result stage 49: 4.082*cos(x0) + 0.208*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08545897901058197\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08539630472660065\n",
      "expression length:\t 2\n",
      "Result stage 50: 4.042*cos(x0) + 0.206*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08533278107643127\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08526845276355743\n",
      "expression length:\t 2\n",
      "Result stage 51: 4.002*cos(x0) + 0.204*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0852033942937851\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08513747900724411\n",
      "expression length:\t 2\n",
      "Result stage 52: 3.962*cos(x0) + 0.202*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08507084846496582\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08500321954488754\n",
      "expression length:\t 2\n",
      "Result stage 53: 3.922*cos(x0) + 0.2*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08493481576442719\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08486554026603699\n",
      "expression length:\t 2\n",
      "Result stage 54: 3.881*cos(x0) + 0.198*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08479537814855576\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08472434431314468\n",
      "expression length:\t 2\n",
      "Result stage 55: 3.841*cos(x0) + 0.196*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08465240150690079\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0845794603228569\n",
      "expression length:\t 2\n",
      "Result stage 56: 3.801*cos(x0) + 0.194*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08450567722320557\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08443106710910797\n",
      "expression length:\t 2\n",
      "Result stage 57: 3.761*cos(x0) + 0.192*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08435546606779099\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08427898585796356\n",
      "expression length:\t 2\n",
      "Result stage 58: 3.721*cos(x0) + 0.19*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08420167863368988\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08412335813045502\n",
      "expression length:\t 2\n",
      "Result stage 59: 3.681*cos(x0) + 0.188*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08404409885406494\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08396391570568085\n",
      "expression length:\t 2\n",
      "Result stage 60: 3.641*cos(x0) + 0.186*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08388272672891617\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08380067348480225\n",
      "expression length:\t 2\n",
      "Result stage 61: 3.601*cos(x0) + 0.184*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08371756970882416\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08363345265388489\n",
      "expression length:\t 2\n",
      "Result stage 62: 3.561*cos(x0) + 0.181*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08354842662811279\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08346232026815414\n",
      "expression length:\t 2\n",
      "Result stage 63: 3.521*cos(x0) + 0.179*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08337517827749252\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08328702300786972\n",
      "expression length:\t 2\n",
      "Result stage 64: 3.481*cos(x0) + 0.177*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08319801837205887\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08310780674219131\n",
      "expression length:\t 2\n",
      "Result stage 65: 3.441*cos(x0) + 0.175*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08301672339439392\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08292463421821594\n",
      "expression length:\t 2\n",
      "Result stage 66: 3.401*cos(x0) + 0.173*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08283144235610962\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08273734897375107\n",
      "expression length:\t 2\n",
      "Result stage 67: 3.361*cos(x0) + 0.171*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08264218270778656\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08254590630531311\n",
      "expression length:\t 2\n",
      "Result stage 68: 3.322*cos(x0) + 0.169*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0824485644698143\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08235029131174088\n",
      "expression length:\t 2\n",
      "Result stage 69: 3.282*cos(x0) + 0.167*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08225082606077194\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08215034008026123\n",
      "expression length:\t 2\n",
      "Result stage 70: 3.242*cos(x0) + 0.165*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08204871416091919\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08194591104984283\n",
      "expression length:\t 2\n",
      "Result stage 71: 3.203*cos(x0) + 0.163*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0818418636918068\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08173654973506927\n",
      "expression length:\t 2\n",
      "Result stage 72: 3.163*cos(x0) + 0.161*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08163004368543625\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08152228593826294\n",
      "expression length:\t 2\n",
      "Result stage 73: 3.123*cos(x0) + 0.159*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08141352236270905\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0813034400343895\n",
      "expression length:\t 2\n",
      "Result stage 74: 3.083*cos(x0) + 0.157*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08119244873523712\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08108028024435043\n",
      "expression length:\t 2\n",
      "Result stage 75: 3.044*cos(x0) + 0.155*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08096707612276077\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08085264265537262\n",
      "expression length:\t 2\n",
      "Result stage 76: 3.004*cos(x0) + 0.153*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08073706924915314\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08062037825584412\n",
      "expression length:\t 2\n",
      "Result stage 77: 2.965*cos(x0) + 0.151*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08050256222486496\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08038369566202164\n",
      "expression length:\t 2\n",
      "Result stage 78: 2.925*cos(x0) + 0.149*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08026362210512161\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.08014245331287384\n",
      "expression length:\t 2\n",
      "Result stage 79: 2.886*cos(x0) + 0.147*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0800202414393425\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07989688217639923\n",
      "expression length:\t 2\n",
      "Result stage 80: 2.846*cos(x0) + 0.145*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07977256923913956\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.079646997153759\n",
      "expression length:\t 2\n",
      "Result stage 81: 2.807*cos(x0) + 0.143*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07952035963535309\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07939275354146957\n",
      "expression length:\t 2\n",
      "Result stage 82: 2.768*cos(x0) + 0.141*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07926424592733383\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07913465052843094\n",
      "expression length:\t 2\n",
      "Result stage 83: 2.728*cos(x0) + 0.139*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07900417596101761\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07887273281812668\n",
      "expression length:\t 2\n",
      "Result stage 84: 2.689*cos(x0) + 0.137*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07874049991369247\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07860738039016724\n",
      "expression length:\t 2\n",
      "Result stage 85: 2.65*cos(x0) + 0.135*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07847339659929276\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0783386081457138\n",
      "expression length:\t 2\n",
      "Result stage 86: 2.611*cos(x0) + 0.133*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07820311933755875\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07806697487831116\n",
      "expression length:\t 2\n",
      "Result stage 87: 2.572*cos(x0) + 0.131*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07793017476797104\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07779289782047272\n",
      "expression length:\t 2\n",
      "Result stage 88: 2.533*cos(x0) + 0.129*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0776551365852356\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0775170549750328\n",
      "expression length:\t 2\n",
      "Result stage 89: 2.494*cos(x0) + 0.127*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07737861573696136\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07724025845527649\n",
      "expression length:\t 2\n",
      "Result stage 90: 2.455*cos(x0) + 0.125*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07710182666778564\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07696343213319778\n",
      "expression length:\t 2\n",
      "Result stage 91: 2.416*cos(x0) + 0.123*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07682523876428604\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07668723165988922\n",
      "expression length:\t 2\n",
      "Result stage 92: 2.378*cos(x0) + 0.121*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07654968649148941\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07641270756721497\n",
      "expression length:\t 2\n",
      "Result stage 93: 2.339*cos(x0) + 0.119*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07627657800912857\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07614143192768097\n",
      "expression length:\t 2\n",
      "Result stage 94: 2.3*cos(x0) + 0.117*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07600739598274231\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07587461173534393\n",
      "expression length:\t 2\n",
      "Result stage 95: 2.262*cos(x0) + 0.115*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07574328035116196\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07561385631561279\n",
      "expression length:\t 2\n",
      "Result stage 96: 2.223*cos(x0) + 0.113*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07548663020133972\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07536173611879349\n",
      "expression length:\t 2\n",
      "Result stage 97: 2.185*cos(x0) + 0.111*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0752396360039711\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0751204565167427\n",
      "expression length:\t 2\n",
      "Result stage 98: 2.147*cos(x0) + 0.109*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07500478625297546\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07489300519227982\n",
      "expression length:\t 2\n",
      "Result stage 99: 2.109*cos(x0) + 0.107*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07478542625904083\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07468267530202866\n",
      "expression length:\t 2\n",
      "Result stage 100: 2.071*cos(x0) + 0.105*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07458511739969254\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.07449337095022202\n",
      "expression length:\t 2\n",
      "Result stage 101: 2.033*cos(x0) + 0.104*x0_t**2 + \n"
     ]
    }
   ],
   "source": [
    "for stage in range(100):\n",
    "    \n",
    "    #Redefine computation after thresholding\n",
    "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=False)\n",
    "    Eta = Eta.to(device)\n",
    "    Zeta = Zeta.to(device)\n",
    "    Delta = Delta.to(device)\n",
    "\n",
    "    #Training\n",
    "    Epoch = 100\n",
    "    i = 1\n",
    "    lr = 1e-4\n",
    "    if(stage==1):\n",
    "        lam = 0\n",
    "    else:\n",
    "        lam = 0.1\n",
    "    temp = 1000\n",
    "    while(i<=Epoch):   \n",
    "        xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "        if i %50 == 0:\n",
    "            print(\"\\n\")\n",
    "            print(\"Stage \",stage)\n",
    "            print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "            print(\"Learning rate : \", lr)\n",
    "            print(\"Average loss : \" , lossitem)\n",
    "        temp = lossitem\n",
    "        if(temp <=1e-6):\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    ## Thresholding small indices ##\n",
    "    threshold = 1e-1\n",
    "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "    expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "    prevxi_L = xi_L.clone().detach()\n",
    "    mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "    ## obtaining analytical model\n",
    "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
    "    L = HL.generateExpression(xi_Lcpu,expr)\n",
    "    print(\"expression length:\\t\",len(xi_L))\n",
    "    print(\"Result stage \" + str(stage+2) + \":\" , L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda7f63-7451-4f1f-8bb8-6d994abdedee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8813673-d9ae-4769-9aa8-dc6b89ba4f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
