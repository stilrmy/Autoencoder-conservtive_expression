{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21abee7a-e86d-4999-a297-36f121b1dd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8377208772180885e-53\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "\n",
    "from sympy import symbols, simplify, derive_by_array\n",
    "from scipy.integrate import solve_ivp\n",
    "from xLSINDy_sp import *\n",
    "from sympy.physics.mechanics import *\n",
    "from sympy import *\n",
    "from Data_generator_py import image_process\n",
    "import sympy\n",
    "import torch\n",
    "import sys\n",
    "import HLsearch as HL\n",
    "import example_pendulum\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3931f0-8bba-44e2-8d56-b263f77bac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "environment = \"server\"\n",
    "sample_size = 10\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e864928-be31-4c73-b320-bfe0a2143ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4998, 2)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(r'../../../HLsearch/')\n",
    "#Saving Directory\n",
    "if environment == 'laptop':\n",
    "    root_dir =R'C:\\Users\\87106\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'desktop':\n",
    "    root_dir = R'E:\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'server':\n",
    "    root_dir = R'/mnt/ssd1/stilrmy/Autoencoder-conservtive_expression'\n",
    "x,dx,ddx = image_process(sample_size)\n",
    "X = []\n",
    "Xdot = []\n",
    "for i in range(len(x)):\n",
    "    temp_list = [float(x[i]),float(dx[i])]\n",
    "    X.append(temp_list)\n",
    "    temp_list = [float(dx[i]),float(ddx[i])]\n",
    "    Xdot.append(temp_list)\n",
    "X = np.vstack(X)\n",
    "print(X.shape)\n",
    "Xdot = np.vstack(Xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8250a64-b76e-48f3-810e-28b181740486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states are: (x0, x0_t)\n",
      "states derivatives are:  (x0_t, x0_tt)\n"
     ]
    }
   ],
   "source": [
    "states_dim = 2\n",
    "states = ()\n",
    "states_dot = ()\n",
    "for i in range(states_dim):\n",
    "    if(i<states_dim//2):\n",
    "        states = states + (symbols('x{}'.format(i)),)\n",
    "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
    "    else:\n",
    "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
    "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
    "print('states are:',states)\n",
    "print('states derivatives are: ', states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c6dddc-ac3c-4e37-91dc-ac6bab075c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn from sympy to str\n",
    "states_sym = states\n",
    "states_dot_sym = states_dot\n",
    "states = list(str(descr) for descr in states)\n",
    "states_dot = list(str(descr) for descr in states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f71e4d-17ce-4af8-9d22-dc28cfc93f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'x0*x0_t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build function expression for the library in str\n",
    "expr= HL.buildFunctionExpressions(2,states_dim,states,use_sine=True)\n",
    "#expr=['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n",
    "\"a list of candidate function\"\n",
    "print(expr)\n",
    "expr.pop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ad60fa-1b07-423b-b96d-be7e84e9f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=True)\n",
    "Eta = Eta.to(device)\n",
    "Zeta = Zeta.to(device)\n",
    "Delta = Delta.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e559dd-c8b0-4041-b1f8-64e6124095be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(len(expr),device=device)\n",
    "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
    "prevxi_L = xi_L.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8134ecd-8d51-42b3-bd18-0a0f50901312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, targ):\n",
    "    loss = torch.mean((pred - targ)**2) \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c008f0b2-21b3-4433-afbf-8353931150dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(w, alpha):\n",
    "    clipped = torch.minimum(w,alpha)\n",
    "    clipped = torch.maximum(clipped,-alpha)\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16edbe1-038d-45e5-b403-07e08b441d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxL1norm(w_hat, alpha):\n",
    "    if(torch.is_tensor(alpha)==False):\n",
    "        alpha = torch.tensor(alpha)\n",
    "    w = w_hat - clip(w_hat,alpha)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8dffb9-95ba-4ef5-8987-6cc6a2e20a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(coef, prevcoef, Zeta, Eta, Delta,xdot, bs, lr, lam):\n",
    "    loss_list = []\n",
    "    tl = xdot.shape[0]\n",
    "    n = xdot.shape[1]\n",
    "    momentum = True\n",
    "    #if(torch.is_tensor(xdot)==False):\n",
    "        #xdot = torch.from_numpy(xdot).to(device).float()\n",
    "    v = coef.clone().detach().to(device).requires_grad_(True)\n",
    "    prev = prevcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    for i in range(tl//bs):\n",
    "        #computing acceleration with momentum\n",
    "        #if (momentum == True):\n",
    "            #vhat = (v + ((i - 1) / (i + 2)) * (v - prev)).clone().detach().requires_grad_(True)\n",
    "       # else:\n",
    "        vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "        prev = v\n",
    "        #Computing loss\n",
    "        zeta = Zeta[:,i*bs:(i+1)*bs]\n",
    "        eta = Eta[:,i*bs:(i+1)*bs]\n",
    "        delta = Delta[:,i*bs:(i+1)*bs]\n",
    "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
    "        #forward\n",
    "        q_tt_pred = lagrangianforward(vhat,zeta,eta,delta,x_t,device)\n",
    "        q_tt_true = xdot[i*bs:(i+1)*bs,n//2:].T\n",
    "        q_tt_true = torch.from_numpy(q_tt_true).to(device).float()\n",
    "        lossval = loss(q_tt_pred, q_tt_true)\n",
    "        lossval.requires_grad_(True)\n",
    "        #Backpropagation\n",
    "        lossval.backward()\n",
    "        with torch.no_grad():\n",
    "            v = vhat - lr * vhat.grad\n",
    "            v = proxL1norm(v, lr * lam)\n",
    "            # Manually zero the gradients after updating weights\n",
    "            vhat.grad = None\n",
    "        loss_list.append(lossval.item())\n",
    "    return v, prev, torch.tensor(loss_list).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e9ace20-122c-4072-9ccf-9a3bd554abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 20/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05594063922762871\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 40/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.05586989223957062\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 60/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0557996928691864\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 80/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.055729568004608154\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 100/100\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.0556594543159008\n"
     ]
    }
   ],
   "source": [
    "Epoch = 100\n",
    "i = 1\n",
    "lr = 1e-3\n",
    "lam = 0.1\n",
    "temp = 1000\n",
    "while(i<=Epoch):\n",
    "    xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "    if i %20 == 0:\n",
    "        print(\"\\n\")\n",
    "        print(\"Stage 1\")\n",
    "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "        print(\"Learning rate : \", lr)\n",
    "        print(\"Average loss : \" , lossitem)\n",
    "    if(temp <=5e-3):\n",
    "        break\n",
    "    if(temp <=1e-1):\n",
    "        lr = 1e-5\n",
    "    temp = lossitem\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04e39cf-6dac-45ba-8c38-8bfeaf4626b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "-1.21*x0 + -8.86*sin(x0) + 10.0*cos(x0) + 7.42*x0**2 + 4.39*x0_t**2 + -2.88*x0*sin(x0) + 5.26*x0_t*sin(x0) + -6.1*sin(x0)**2 + 7.87*x0*cos(x0) + -7.11*x0_t*cos(x0) + -8.01*sin(x0)*cos(x0) + -0.36*cos(x0)**2 + \n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-2\n",
    "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "## obtaining analytical model\n",
    "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
    "L = HL.generateExpression(xi_Lcpu,expr)\n",
    "print(len(xi_L))\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fbbf84e-74af-4e42-a56c-c701b029dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.05390214920043945\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.05213889107108116\n",
      "expression length:\t 8\n",
      "Result stage 2: 1.471*x0 + -2.19*sin(x0) + 11.872*cos(x0) + 0.606*x0_t**2 + 1.347*x0_t*sin(x0) + 2.153*x0*cos(x0) + -3.192*x0_t*cos(x0) + -1.677*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.05118303745985031\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.05033239349722862\n",
      "expression length:\t 8\n",
      "Result stage 3: 1.481*x0 + -2.194*sin(x0) + 11.875*cos(x0) + 0.606*x0_t**2 + 1.347*x0_t*sin(x0) + 2.137*x0*cos(x0) + -3.192*x0_t*cos(x0) + -1.66*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.048715293407440186\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.04710254818201065\n",
      "expression length:\t 8\n",
      "Result stage 4: 1.451*x0 + -2.159*sin(x0) + 11.839*cos(x0) + 0.604*x0_t**2 + 1.308*x0_t*sin(x0) + 2.082*x0*cos(x0) + -3.153*x0_t*cos(x0) + -1.604*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.04551731050014496\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.04395957663655281\n",
      "expression length:\t 8\n",
      "Result stage 5: 1.423*x0 + -2.123*sin(x0) + 11.803*cos(x0) + 0.602*x0_t**2 + 1.269*x0_t*sin(x0) + 2.027*x0*cos(x0) + -3.114*x0_t*cos(x0) + -1.549*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0424305684864521\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.04093063622713089\n",
      "expression length:\t 8\n",
      "Result stage 6: 1.395*x0 + -2.087*sin(x0) + 11.767*cos(x0) + 0.6*x0_t**2 + 1.23*x0_t*sin(x0) + 1.972*x0*cos(x0) + -3.075*x0_t*cos(x0) + -1.495*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.03946087881922722\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.03802104666829109\n",
      "expression length:\t 8\n",
      "Result stage 7: 1.368*x0 + -2.05*sin(x0) + 11.73*cos(x0) + 0.598*x0_t**2 + 1.191*x0_t*sin(x0) + 1.917*x0*cos(x0) + -3.036*x0_t*cos(x0) + -1.441*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.03661240264773369\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.03523492068052292\n",
      "expression length:\t 8\n",
      "Result stage 8: 1.341*x0 + -2.013*sin(x0) + 11.694*cos(x0) + 0.596*x0_t**2 + 1.152*x0_t*sin(x0) + 1.862*x0*cos(x0) + -2.997*x0_t*cos(x0) + -1.389*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.033889055252075195\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.03257455676794052\n",
      "expression length:\t 8\n",
      "Result stage 9: 1.314*x0 + -1.976*sin(x0) + 11.657*cos(x0) + 0.594*x0_t**2 + 1.113*x0_t*sin(x0) + 1.808*x0*cos(x0) + -2.958*x0_t*cos(x0) + -1.337*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.03129081428050995\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.030038222670555115\n",
      "expression length:\t 8\n",
      "Result stage 10: 1.287*x0 + -1.938*sin(x0) + 11.62*cos(x0) + 0.592*x0_t**2 + 1.074*x0_t*sin(x0) + 1.754*x0*cos(x0) + -2.919*x0_t*cos(x0) + -1.285*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.02881571277976036\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.02762359008193016\n",
      "expression length:\t 8\n",
      "Result stage 11: 1.26*x0 + -1.901*sin(x0) + 11.583*cos(x0) + 0.591*x0_t**2 + 1.035*x0_t*sin(x0) + 1.7*x0*cos(x0) + -2.88*x0_t*cos(x0) + -1.234*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.02646166831254959\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.02532944455742836\n",
      "expression length:\t 8\n",
      "Result stage 12: 1.233*x0 + -1.863*sin(x0) + 11.546*cos(x0) + 0.589*x0_t**2 + 0.996*x0_t*sin(x0) + 1.647*x0*cos(x0) + -2.841*x0_t*cos(x0) + -1.184*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.024226533249020576\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.023153405636548996\n",
      "expression length:\t 8\n",
      "Result stage 13: 1.206*x0 + -1.824*sin(x0) + 11.509*cos(x0) + 0.587*x0_t**2 + 0.957*x0_t*sin(x0) + 1.594*x0*cos(x0) + -2.802*x0_t*cos(x0) + -1.135*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0221088919788599\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.02109278179705143\n",
      "expression length:\t 8\n",
      "Result stage 14: 1.178*x0 + -1.786*sin(x0) + 11.471*cos(x0) + 0.585*x0_t**2 + 0.918*x0_t*sin(x0) + 1.541*x0*cos(x0) + -2.763*x0_t*cos(x0) + -1.086*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.02010541968047619\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.019147025421261787\n",
      "expression length:\t 8\n",
      "Result stage 15: 1.151*x0 + -1.748*sin(x0) + 11.434*cos(x0) + 0.583*x0_t**2 + 0.879*x0_t*sin(x0) + 1.489*x0*cos(x0) + -2.724*x0_t*cos(x0) + -1.037*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.018215835094451904\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.017312288284301758\n",
      "expression length:\t 8\n",
      "Result stage 16: 1.123*x0 + -1.709*sin(x0) + 11.396*cos(x0) + 0.581*x0_t**2 + 0.839*x0_t*sin(x0) + 1.437*x0*cos(x0) + -2.684*x0_t*cos(x0) + -0.989*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.016435623168945312\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.015585445798933506\n",
      "expression length:\t 8\n",
      "Result stage 17: 1.096*x0 + -1.67*sin(x0) + 11.358*cos(x0) + 0.579*x0_t**2 + 0.8*x0_t*sin(x0) + 1.385*x0*cos(x0) + -2.645*x0_t*cos(x0) + -0.942*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.014762002974748611\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.013964523561298847\n",
      "expression length:\t 8\n",
      "Result stage 18: 1.068*x0 + -1.63*sin(x0) + 11.32*cos(x0) + 0.577*x0_t**2 + 0.761*x0_t*sin(x0) + 1.334*x0*cos(x0) + -2.606*x0_t*cos(x0) + -0.895*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.013193316757678986\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.012447800487279892\n",
      "expression length:\t 8\n",
      "Result stage 19: 1.04*x0 + -1.591*sin(x0) + 11.282*cos(x0) + 0.575*x0_t**2 + 0.722*x0_t*sin(x0) + 1.283*x0*cos(x0) + -2.567*x0_t*cos(x0) + -0.849*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.011727632954716682\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.011033119633793831\n",
      "expression length:\t 8\n",
      "Result stage 20: 1.012*x0 + -1.551*sin(x0) + 11.244*cos(x0) + 0.573*x0_t**2 + 0.683*x0_t*sin(x0) + 1.233*x0*cos(x0) + -2.528*x0_t*cos(x0) + -0.804*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.010363112203776836\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.009717538952827454\n",
      "expression length:\t 8\n",
      "Result stage 21: 0.984*x0 + -1.512*sin(x0) + 11.205*cos(x0) + 0.571*x0_t**2 + 0.644*x0_t*sin(x0) + 1.183*x0*cos(x0) + -2.489*x0_t*cos(x0) + -0.759*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.009096179157495499\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.008498524315655231\n",
      "expression length:\t 8\n",
      "Result stage 22: 0.956*x0 + -1.472*sin(x0) + 11.167*cos(x0) + 0.569*x0_t**2 + 0.605*x0_t*sin(x0) + 1.133*x0*cos(x0) + -2.45*x0_t*cos(x0) + -0.714*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.007924509234726429\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.007373932283371687\n",
      "expression length:\t 8\n",
      "Result stage 23: 0.928*x0 + -1.431*sin(x0) + 11.128*cos(x0) + 0.567*x0_t**2 + 0.566*x0_t*sin(x0) + 1.084*x0*cos(x0) + -2.411*x0_t*cos(x0) + -0.67*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.006846539676189423\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0063416678458452225\n",
      "expression length:\t 8\n",
      "Result stage 24: 0.9*x0 + -1.391*sin(x0) + 11.089*cos(x0) + 0.565*x0_t**2 + 0.527*x0_t*sin(x0) + 1.035*x0*cos(x0) + -2.372*x0_t*cos(x0) + -0.627*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.00585935078561306\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.005398980807512999\n",
      "expression length:\t 8\n",
      "Result stage 25: 0.872*x0 + -1.35*sin(x0) + 11.051*cos(x0) + 0.563*x0_t**2 + 0.488*x0_t*sin(x0) + 0.986*x0*cos(x0) + -2.333*x0_t*cos(x0) + -0.584*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.004960361868143082\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.004543466493487358\n",
      "expression length:\t 8\n",
      "Result stage 26: 0.843*x0 + -1.31*sin(x0) + 11.012*cos(x0) + 0.561*x0_t**2 + 0.449*x0_t*sin(x0) + 0.938*x0*cos(x0) + -2.294*x0_t*cos(x0) + -0.542*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.004147796425968409\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0037731463089585304\n",
      "expression length:\t 8\n",
      "Result stage 27: 0.815*x0 + -1.268*sin(x0) + 10.974*cos(x0) + 0.559*x0_t**2 + 0.41*x0_t*sin(x0) + 0.89*x0*cos(x0) + -2.255*x0_t*cos(x0) + -0.501*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0034192693419754505\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.003085778094828129\n",
      "expression length:\t 8\n",
      "Result stage 28: 0.786*x0 + -1.227*sin(x0) + 10.935*cos(x0) + 0.557*x0_t**2 + 0.371*x0_t*sin(x0) + 0.842*x0*cos(x0) + -2.216*x0_t*cos(x0) + -0.46*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0027725871186703444\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.002479284768924117\n",
      "expression length:\t 8\n",
      "Result stage 29: 0.758*x0 + -1.186*sin(x0) + 10.896*cos(x0) + 0.555*x0_t**2 + 0.332*x0_t*sin(x0) + 0.795*x0*cos(x0) + -2.177*x0_t*cos(x0) + -0.419*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0022056852467358112\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0019512302242219448\n",
      "expression length:\t 8\n",
      "Result stage 30: 0.729*x0 + -1.144*sin(x0) + 10.857*cos(x0) + 0.553*x0_t**2 + 0.293*x0_t*sin(x0) + 0.748*x0*cos(x0) + -2.138*x0_t*cos(x0) + -0.379*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.001715891994535923\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0014994455268606544\n",
      "expression length:\t 8\n",
      "Result stage 31: 0.7*x0 + -1.102*sin(x0) + 10.819*cos(x0) + 0.551*x0_t**2 + 0.254*x0_t*sin(x0) + 0.701*x0*cos(x0) + -2.099*x0_t*cos(x0) + -0.34*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0013013298157602549\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0011216282146051526\n",
      "expression length:\t 8\n",
      "Result stage 32: 0.671*x0 + -1.06*sin(x0) + 10.78*cos(x0) + 0.549*x0_t**2 + 0.215*x0_t*sin(x0) + 0.655*x0*cos(x0) + -2.06*x0_t*cos(x0) + -0.301*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0009598647011443973\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0008157363627105951\n",
      "expression length:\t 8\n",
      "Result stage 33: 0.642*x0 + -1.018*sin(x0) + 10.741*cos(x0) + 0.547*x0_t**2 + 0.176*x0_t*sin(x0) + 0.609*x0*cos(x0) + -2.021*x0_t*cos(x0) + -0.263*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0006890496588312089\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0005794459721073508\n",
      "expression length:\t 8\n",
      "Result stage 34: 0.613*x0 + -0.976*sin(x0) + 10.702*cos(x0) + 0.545*x0_t**2 + 0.137*x0_t*sin(x0) + 0.564*x0*cos(x0) + -1.982*x0_t*cos(x0) + -0.225*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.00048674465506337583\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.00041066782432608306\n",
      "expression length:\t 7\n",
      "Result stage 35: 0.584*x0 + -0.933*sin(x0) + 10.663*cos(x0) + 0.543*x0_t**2 + 0.519*x0*cos(x0) + -1.942*x0_t*cos(x0) + -0.188*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.00035090468009002507\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.000307231122860685\n",
      "expression length:\t 7\n",
      "Result stage 36: 0.555*x0 + -0.89*sin(x0) + 10.624*cos(x0) + 0.541*x0_t**2 + 0.474*x0*cos(x0) + -1.903*x0_t*cos(x0) + -0.152*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.00027920588036067784\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0002667894004844129\n",
      "expression length:\t 7\n",
      "Result stage 37: 0.525*x0 + -0.847*sin(x0) + 10.585*cos(x0) + 0.539*x0_t**2 + 0.429*x0*cos(x0) + -1.864*x0_t*cos(x0) + -0.116*sin(x0)*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.00026953074848279357\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0002873136836569756\n",
      "expression length:\t 6\n",
      "Result stage 38: 0.496*x0 + -0.804*sin(x0) + 10.547*cos(x0) + 0.537*x0_t**2 + 0.385*x0*cos(x0) + -1.825*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0018003308214247227\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0009138308232650161\n",
      "expression length:\t 6\n",
      "Result stage 39: 0.466*x0 + -0.769*sin(x0) + 10.508*cos(x0) + 0.535*x0_t**2 + 0.321*x0*cos(x0) + -1.786*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0006484996993094683\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.00062496995087713\n",
      "expression length:\t 6\n",
      "Result stage 40: 0.438*x0 + -0.724*sin(x0) + 10.469*cos(x0) + 0.533*x0_t**2 + 0.28*x0*cos(x0) + -1.747*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0006835764506831765\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0007587318541482091\n",
      "expression length:\t 6\n",
      "Result stage 41: 0.409*x0 + -0.675*sin(x0) + 10.43*cos(x0) + 0.531*x0_t**2 + 0.249*x0*cos(x0) + -1.708*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0008250790415331721\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0008760286727920175\n",
      "expression length:\t 6\n",
      "Result stage 42: 0.379*x0 + -0.624*sin(x0) + 10.391*cos(x0) + 0.529*x0_t**2 + 0.223*x0*cos(x0) + -1.669*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0009114286513067782\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0009333686903119087\n",
      "expression length:\t 6\n",
      "Result stage 43: 0.348*x0 + -0.573*sin(x0) + 10.352*cos(x0) + 0.527*x0_t**2 + 0.2*x0*cos(x0) + -1.63*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0009446640033274889\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0009478028514422476\n",
      "expression length:\t 6\n",
      "Result stage 44: 0.317*x0 + -0.522*sin(x0) + 10.313*cos(x0) + 0.525*x0_t**2 + 0.178*x0*cos(x0) + -1.591*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0009454325190745294\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0009388010366819799\n",
      "expression length:\t 6\n",
      "Result stage 45: 0.285*x0 + -0.471*sin(x0) + 10.274*cos(x0) + 0.523*x0_t**2 + 0.158*x0*cos(x0) + -1.552*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.000929256493691355\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0009179502376355231\n",
      "expression length:\t 6\n",
      "Result stage 46: 0.254*x0 + -0.42*sin(x0) + 10.235*cos(x0) + 0.521*x0_t**2 + 0.137*x0*cos(x0) + -1.513*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0009052362875081599\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.000892133975867182\n",
      "expression length:\t 6\n",
      "Result stage 47: 0.222*x0 + -0.369*sin(x0) + 10.196*cos(x0) + 0.519*x0_t**2 + 0.117*x0*cos(x0) + -1.474*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0008789962739683688\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0008660864550620317\n",
      "expression length:\t 5\n",
      "Result stage 48: 0.19*x0 + -0.318*sin(x0) + 10.156*cos(x0) + 0.517*x0_t**2 + -1.435*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.010692543350160122\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.008295281790196896\n",
      "expression length:\t 5\n",
      "Result stage 49: 0.154*x0 + -0.247*sin(x0) + 10.118*cos(x0) + 0.516*x0_t**2 + -1.396*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.006326985079795122\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0047033983282744884\n",
      "expression length:\t 5\n",
      "Result stage 50: 0.116*x0 + -0.185*sin(x0) + 10.079*cos(x0) + 0.514*x0_t**2 + -1.357*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.00337648275308311\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0023126956075429916\n",
      "expression length:\t 4\n",
      "Result stage 51: -0.128*sin(x0) + 10.04*cos(x0) + 0.511*x0_t**2 + -1.318*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.004017180297523737\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  0.0015419767005369067\n",
      "expression length:\t 3\n",
      "Result stage 52: 10.001*cos(x0) + 0.509*x0_t**2 + -1.279*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.5114862435439136e-05\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.5066450032463763e-05\n",
      "expression length:\t 3\n",
      "Result stage 53: 9.962*cos(x0) + 0.507*x0_t**2 + -1.24*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.5000989151303656e-05\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.495534525020048e-05\n",
      "expression length:\t 3\n",
      "Result stage 54: 9.923*cos(x0) + 0.505*x0_t**2 + -1.2*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4909344827174209e-05\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4847214515611995e-05\n",
      "expression length:\t 3\n",
      "Result stage 55: 9.884*cos(x0) + 0.503*x0_t**2 + -1.161*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4795737115491647e-05\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4749965885130223e-05\n",
      "expression length:\t 3\n",
      "Result stage 56: 9.845*cos(x0) + 0.501*x0_t**2 + -1.122*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4700116480526049e-05\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4641582311014645e-05\n",
      "expression length:\t 3\n",
      "Result stage 57: 9.806*cos(x0) + 0.5*x0_t**2 + -1.083*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.459043960494455e-05\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4541512427967973e-05\n",
      "expression length:\t 3\n",
      "Result stage 58: 9.767*cos(x0) + 0.498*x0_t**2 + -1.044*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4495206414721906e-05\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4444675798586104e-05\n",
      "expression length:\t 3\n",
      "Result stage 59: 9.728*cos(x0) + 0.496*x0_t**2 + -1.005*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4399091924133245e-05\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.435048397979699e-05\n",
      "expression length:\t 3\n",
      "Result stage 60: 9.689*cos(x0) + 0.494*x0_t**2 + -0.966*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4304497199191246e-05\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4255099813453853e-05\n",
      "expression length:\t 3\n",
      "Result stage 61: 9.649*cos(x0) + 0.492*x0_t**2 + -0.927*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4200904843164608e-05\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4151655705063604e-05\n",
      "expression length:\t 3\n",
      "Result stage 62: 9.61*cos(x0) + 0.49*x0_t**2 + -0.888*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.410980894434033e-05\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.4059066415939014e-05\n",
      "expression length:\t 3\n",
      "Result stage 63: 9.571*cos(x0) + 0.488*x0_t**2 + -0.849*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.400651854055468e-05\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3954652786196675e-05\n",
      "expression length:\t 3\n",
      "Result stage 64: 9.532*cos(x0) + 0.486*x0_t**2 + -0.81*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3907425454817712e-05\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3862082596460823e-05\n",
      "expression length:\t 3\n",
      "Result stage 65: 9.493*cos(x0) + 0.484*x0_t**2 + -0.771*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.381184392812429e-05\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.376430554955732e-05\n",
      "expression length:\t 3\n",
      "Result stage 66: 9.454*cos(x0) + 0.482*x0_t**2 + -0.732*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3714618944504764e-05\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3667675375472754e-05\n",
      "expression length:\t 3\n",
      "Result stage 67: 9.415*cos(x0) + 0.48*x0_t**2 + -0.693*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3616125215776265e-05\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3573589058069047e-05\n",
      "expression length:\t 3\n",
      "Result stage 68: 9.376*cos(x0) + 0.478*x0_t**2 + -0.654*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3520494576368947e-05\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3474734259943943e-05\n",
      "expression length:\t 3\n",
      "Result stage 69: 9.337*cos(x0) + 0.476*x0_t**2 + -0.615*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3426037185126916e-05\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3380335076362826e-05\n",
      "expression length:\t 3\n",
      "Result stage 70: 9.297*cos(x0) + 0.474*x0_t**2 + -0.576*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3333397873793729e-05\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3283975022204686e-05\n",
      "expression length:\t 3\n",
      "Result stage 71: 9.258*cos(x0) + 0.472*x0_t**2 + -0.537*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3236241102276836e-05\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3190731806389522e-05\n",
      "expression length:\t 3\n",
      "Result stage 72: 9.219*cos(x0) + 0.47*x0_t**2 + -0.498*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3144789591024164e-05\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3092589142615907e-05\n",
      "expression length:\t 3\n",
      "Result stage 73: 9.18*cos(x0) + 0.468*x0_t**2 + -0.458*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.3048683285887819e-05\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2997710655326955e-05\n",
      "expression length:\t 3\n",
      "Result stage 74: 9.141*cos(x0) + 0.466*x0_t**2 + -0.419*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2957353646925185e-05\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2909401448268909e-05\n",
      "expression length:\t 3\n",
      "Result stage 75: 9.102*cos(x0) + 0.464*x0_t**2 + -0.38*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2861161849286873e-05\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2811897249775939e-05\n",
      "expression length:\t 3\n",
      "Result stage 76: 9.063*cos(x0) + 0.462*x0_t**2 + -0.341*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2766427971655503e-05\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.271711244044127e-05\n",
      "expression length:\t 3\n",
      "Result stage 77: 9.024*cos(x0) + 0.46*x0_t**2 + -0.302*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2676499864028301e-05\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2631441677513067e-05\n",
      "expression length:\t 3\n",
      "Result stage 78: 8.985*cos(x0) + 0.458*x0_t**2 + -0.263*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2587211131176446e-05\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2537820111901965e-05\n",
      "expression length:\t 3\n",
      "Result stage 79: 8.946*cos(x0) + 0.456*x0_t**2 + -0.224*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2496411727624945e-05\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2451656402845401e-05\n",
      "expression length:\t 3\n",
      "Result stage 80: 8.906*cos(x0) + 0.454*x0_t**2 + -0.185*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2405064808262978e-05\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2359575521259103e-05\n",
      "expression length:\t 3\n",
      "Result stage 81: 8.867*cos(x0) + 0.452*x0_t**2 + -0.146*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2311841601331253e-05\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2274087566765957e-05\n",
      "expression length:\t 3\n",
      "Result stage 82: 8.828*cos(x0) + 0.45*x0_t**2 + -0.107*x0_t*cos(x0) + \n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2227356819494162e-05\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2182700629637111e-05\n",
      "expression length:\t 2\n",
      "Result stage 83: 8.789*cos(x0) + 0.448*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2137155863456428e-05\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2089473784726579e-05\n",
      "expression length:\t 2\n",
      "Result stage 84: 8.75*cos(x0) + 0.446*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.204872751259245e-05\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.2001139111816883e-05\n",
      "expression length:\t 2\n",
      "Result stage 85: 8.711*cos(x0) + 0.444*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1959061339439359e-05\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1910919965885114e-05\n",
      "expression length:\t 2\n",
      "Result stage 86: 8.672*cos(x0) + 0.442*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1868107321788557e-05\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1822824490081985e-05\n",
      "expression length:\t 2\n",
      "Result stage 87: 8.633*cos(x0) + 0.44*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1775344319175929e-05\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1733897736121435e-05\n",
      "expression length:\t 2\n",
      "Result stage 88: 8.594*cos(x0) + 0.438*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1689779967127834e-05\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.164821605925681e-05\n",
      "expression length:\t 2\n",
      "Result stage 89: 8.555*cos(x0) + 0.436*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1606472980929539e-05\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1565165550564416e-05\n",
      "expression length:\t 2\n",
      "Result stage 90: 8.515*cos(x0) + 0.434*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1518800420162734e-05\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1477966836537234e-05\n",
      "expression length:\t 2\n",
      "Result stage 91: 8.476*cos(x0) + 0.432*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.143369900091784e-05\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1387181984900963e-05\n",
      "expression length:\t 2\n",
      "Result stage 92: 8.437*cos(x0) + 0.43*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1341928257024847e-05\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1297801393084228e-05\n",
      "expression length:\t 2\n",
      "Result stage 93: 8.398*cos(x0) + 0.428*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.126019469666062e-05\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1218021427339409e-05\n",
      "expression length:\t 2\n",
      "Result stage 94: 8.359*cos(x0) + 0.426*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1179878129041754e-05\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1137269211758394e-05\n",
      "expression length:\t 2\n",
      "Result stage 95: 8.32*cos(x0) + 0.424*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1092552085756324e-05\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.10507307908847e-05\n",
      "expression length:\t 2\n",
      "Result stage 96: 8.281*cos(x0) + 0.422*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.1008522960764822e-05\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.0963080057990737e-05\n",
      "expression length:\t 2\n",
      "Result stage 97: 8.242*cos(x0) + 0.42*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.0924843991233502e-05\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.0881799425987992e-05\n",
      "expression length:\t 2\n",
      "Result stage 98: 8.203*cos(x0) + 0.418*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.0839176866284106e-05\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.0798116818477865e-05\n",
      "expression length:\t 2\n",
      "Result stage 99: 8.164*cos(x0) + 0.416*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.0756609299278352e-05\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.0712763469200581e-05\n",
      "expression length:\t 2\n",
      "Result stage 100: 8.124*cos(x0) + 0.414*x0_t**2 + \n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 50/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.0669757102732547e-05\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 100/100\n",
      "Learning rate :  0.0001\n",
      "Average loss :  1.0629317330312915e-05\n",
      "expression length:\t 2\n",
      "Result stage 101: 8.085*cos(x0) + 0.412*x0_t**2 + \n"
     ]
    }
   ],
   "source": [
    "for stage in range(100):\n",
    "    \n",
    "    #Redefine computation after thresholding\n",
    "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=False)\n",
    "    Eta = Eta.to(device)\n",
    "    Zeta = Zeta.to(device)\n",
    "    Delta = Delta.to(device)\n",
    "\n",
    "    #Training\n",
    "    Epoch = 200\n",
    "    i = 1\n",
    "    lr = 1e-4\n",
    "    if(stage==1):\n",
    "        lam = 0\n",
    "    else:\n",
    "        lam = 0.1\n",
    "    temp = 1000\n",
    "    while(i<=Epoch):   \n",
    "        xi_L , prevxi_L, lossitem= training_loop(xi_L,prevxi_L,Zeta,Eta,Delta,Xdot,128,lr=lr,lam=lam)\n",
    "        if i %50 == 0:\n",
    "            print(\"\\n\")\n",
    "            print(\"Stage \",stage)\n",
    "            print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "            print(\"Learning rate : \", lr)\n",
    "            print(\"Average loss : \" , lossitem)\n",
    "        temp = lossitem\n",
    "        if(temp <=1e-6):\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    ## Thresholding small indices ##\n",
    "    threshold = 1e-1\n",
    "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "    expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "    prevxi_L = xi_L.clone().detach()\n",
    "    mask = torch.ones(len(expr),device=device)\n",
    "\n",
    "    ## obtaining analytical model\n",
    "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=3)\n",
    "    L = HL.generateExpression(xi_Lcpu,expr)\n",
    "    print(\"expression length:\\t\",len(xi_L))\n",
    "    print(\"Result stage \" + str(stage+2) + \":\" , L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda7f63-7451-4f1f-8bb8-6d994abdedee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8813673-d9ae-4769-9aa8-dc6b89ba4f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
