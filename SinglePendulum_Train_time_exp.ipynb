{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21abee7a-e86d-4999-a297-36f121b1dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "\n",
    "from sympy import symbols, simplify, derive_by_array\n",
    "from scipy.integrate import solve_ivp\n",
    "from xLSINDy_sp import *\n",
    "from sympy.physics.mechanics import *\n",
    "from sympy import *\n",
    "from Data_generator_py import image_process\n",
    "import sympy\n",
    "import torch;torch.random.manual_seed(66)\n",
    "import sys\n",
    "import HLsearch as HL\n",
    "import example_pendulum\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a3931f0-8bba-44e2-8d56-b263f77bac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "environment = \"server\"\n",
    "sample_size = 10\n",
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e864928-be31-4c73-b320-bfe0a2143ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of t:  float64\n",
      "(4998, 2)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(r'../../../HLsearch/')\n",
    "#Saving Directory\n",
    "momentum = True\n",
    "params = {}\n",
    "params['adding_noise'] = False\n",
    "params['noise_type'] = 'image_noise'\n",
    "params['noiselevel'] = 1e-1\n",
    "params['changing_length'] = False\n",
    "params['c'] = 1.4e-4\n",
    "params['g'] = 9.81\n",
    "params['l'] = 1\n",
    "if environment == 'laptop':\n",
    "    root_dir =R'C:\\Users\\87106\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'desktop':\n",
    "    root_dir = R'E:\\OneDrive\\sindy\\progress'\n",
    "elif environment == 'server':\n",
    "    root_dir = R'/mnt/ssd1/stilrmy/Autoencoder-conservtive_expression'\n",
    "x,dx,ddx,t = image_process(sample_size,params)\n",
    "X = []\n",
    "Xdot = []\n",
    "for i in range(len(x)):\n",
    "    temp_list = [float(x[i]),float(dx[i])]\n",
    "    X.append(temp_list)\n",
    "    temp_list = [float(dx[i]),float(ddx[i])]\n",
    "    Xdot.append(temp_list)\n",
    "X = np.vstack(X)\n",
    "print(X.shape)\n",
    "Xdot = np.vstack(Xdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2468f670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "temp = t\n",
    "for i in range(sample_size-1):\n",
    "    temp = np.append(temp,t)\n",
    "print(temp.shape)\n",
    "t = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8250a64-b76e-48f3-810e-28b181740486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states are: (x0, x0_t)\n",
      "states derivatives are:  (x0_t, x0_tt)\n"
     ]
    }
   ],
   "source": [
    "states_dim = 2\n",
    "states = ()\n",
    "states_dot = ()\n",
    "for i in range(states_dim):\n",
    "    if(i<states_dim//2):\n",
    "        states = states + (symbols('x{}'.format(i)),)\n",
    "        states_dot = states_dot + (symbols('x{}_t'.format(i)),)\n",
    "    else:\n",
    "        states = states + (symbols('x{}_t'.format(i-states_dim//2)),)\n",
    "        states_dot = states_dot + (symbols('x{}_tt'.format(i-states_dim//2)),)\n",
    "print('states are:',states)\n",
    "print('states derivatives are: ', states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9c6dddc-ac3c-4e37-91dc-ac6bab075c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn from sympy to str\n",
    "states_sym = states\n",
    "states_dot_sym = states_dot\n",
    "states = list(str(descr) for descr in states)\n",
    "states_dot = list(str(descr) for descr in states_dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36f71e4d-17ce-4af8-9d22-dc28cfc93f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'x0*x0_t'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build function expression for the library in str\n",
    "expr= HL.buildFunctionExpressions(2,states_dim,states,use_sine=True)\n",
    "#expr=['x0', 'x0_t', 'sin(x0)', 'cos(x0)', 'x0**2', 'x0*x0_t', 'x0_t**2', 'x0*sin(x0)', 'x0_t*sin(x0)', 'sin(x0)**2', 'x0*cos(x0)', 'x0_t*cos(x0)', 'sin(x0)*cos(x0)', 'cos(x0)**2']\n",
    "\"a list of candidate function\"\n",
    "print(expr)\n",
    "expr.pop(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1ad60fa-1b07-423b-b96d-be7e84e9f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=False)\n",
    "Eta = Eta.to(device)\n",
    "Zeta = Zeta.to(device)\n",
    "Delta = Delta.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92e559dd-c8b0-4041-b1f8-64e6124095be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coefficients for the Lagrangian\n",
    "mask = torch.ones(len(expr),device=device)\n",
    "xi_L = torch.ones(len(expr), device=device).data.uniform_(-10,10)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "c = torch.ones(1,device=device)*-0.1\n",
    "prec = c.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8134ecd-8d51-42b3-bd18-0a0f50901312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, targ):\n",
    "    loss = torch.mean((pred - targ)**2) \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c008f0b2-21b3-4433-afbf-8353931150dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(w, alpha):\n",
    "    clipped = torch.minimum(w,alpha)\n",
    "    clipped = torch.maximum(clipped,-alpha)\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c16edbe1-038d-45e5-b403-07e08b441d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxL1norm(w_hat, alpha):\n",
    "    if(torch.is_tensor(alpha)==False):\n",
    "        alpha = torch.tensor(alpha)\n",
    "    w = w_hat - clip(w_hat,alpha)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d8dffb9-95ba-4ef5-8987-6cc6a2e20a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(coef, c, prevcoef, prec, Zeta, Eta, Delta, xdot, t, bs, lr, lam):\n",
    "    loss_list = []\n",
    "    tl = xdot.shape[0]\n",
    "    n = xdot.shape[1]\n",
    "    momentum = True\n",
    "    #if(torch.is_tensor(xdot)==False):\n",
    "        #xdot = torch.from_numpy(xdot).to(device).float()\n",
    "    v = coef.clone().detach().to(device).requires_grad_(True)\n",
    "    c = c.clone().detach().to(device).requires_grad_(True)\n",
    "    prev = prevcoef.clone().detach().to(device).requires_grad_(True)\n",
    "    prec = prec.clone().detach().to(device).requires_grad_(True)\n",
    "    q_tt_pred = 2\n",
    "    for i in range(tl//bs):\n",
    "        #computing acceleration with momentum\n",
    "        if (momentum == True):\n",
    "            vhat = (v + ((i - 1) / (i + 2)) * (v - prev)).clone().detach().requires_grad_(True)\n",
    "            chat = (c + ((i - 1) / (i + 2)) * (c - prec)).clone().detach().requires_grad_(True)\n",
    "        else:\n",
    "            vhat = v.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "            chat = c.requires_grad_(True).clone().detach().requires_grad_(True)\n",
    "        prev = v\n",
    "        prec = c\n",
    "        #Computing loss\n",
    "        zeta = Zeta[:,i*bs:(i+1)*bs]\n",
    "        eta = Eta[:,i*bs:(i+1)*bs]\n",
    "        delta = Delta[:,i*bs:(i+1)*bs]\n",
    "        x_t = xdot[i*bs:(i+1)*bs,:]\n",
    "        t_temp = t[i*bs:(i+1)*bs]\n",
    "        t_temp = torch.from_numpy(t_temp).to(device).float()\n",
    "        #forward\n",
    "        temp = lagrangianforward(vhat,zeta,eta,delta,x_t,device)\n",
    "        q_tt_pred = torch.exp(chat*t_temp)*temp\n",
    "        q_tt_true = xdot[i*bs:(i+1)*bs,n//2:].T\n",
    "        q_tt_true = torch.from_numpy(q_tt_true).to(device).float()\n",
    "        lossval = loss(q_tt_pred, q_tt_true)\n",
    "        lossval.requires_grad_(True)\n",
    "        #Backpropagation\n",
    "        lossval.backward()\n",
    "        with torch.no_grad():\n",
    "            v = vhat - lr * vhat.grad\n",
    "            v = proxL1norm(v, lr * lam)\n",
    "            vhat.grad = None\n",
    "            c = chat - lr * chat.grad\n",
    "            c = proxL1norm(c, lr * lam)\n",
    "            c.grad = None\n",
    "        loss_list.append(lossval.item())\n",
    "    return v, c, prev, prec, torch.tensor(loss_list).mean().item(),tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e9ace20-122c-4072-9ccf-9a3bd554abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 20/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40763854980469\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 40/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.4074592590332\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 60/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40727615356445\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 80/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.4070930480957\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 100/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40691375732422\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 120/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40673065185547\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 140/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.406551361083984\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 160/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40636444091797\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 180/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40618133544922\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 200/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40599822998047\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 220/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40581512451172\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 240/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40563201904297\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 260/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40544509887695\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 280/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40526580810547\n",
      "\n",
      "\n",
      "Stage 1\n",
      "Epoch 300/310\n",
      "Learning rate :  1e-05\n",
      "Average loss :  48.40507888793945\n"
     ]
    }
   ],
   "source": [
    "Epoch = 310\n",
    "i = 1\n",
    "lr = 1e-5\n",
    "lam = 0.1\n",
    "temp = 1000\n",
    "while(i<=Epoch):\n",
    "    xi_L , c, prevxi_L, prec, lossitem, q= training_loop(xi_L,c,prevxi_L,prec,Zeta,Eta,Delta,Xdot, t, 500,lr,lam)\n",
    "    if i %20 == 0:\n",
    "        print(\"\\n\")\n",
    "        print(\"Stage 1\")\n",
    "        print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "        print(\"Learning rate : \", lr)\n",
    "        print(\"Average loss : \" , lossitem)\n",
    "    if(temp <=5e-3):\n",
    "        break\n",
    "    if(temp <=1e-1):\n",
    "        lr = 1e-5\n",
    "    temp = lossitem\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c04e39cf-6dac-45ba-8c38-8bfeaf4626b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "exp([-6.6494])\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-2\n",
    "surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "expr = np.array(expr)[surv_index].tolist()\n",
    "\n",
    "xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "prevxi_L = xi_L.clone().detach()\n",
    "prec = c.clone().detach()\n",
    "\n",
    "## obtaining analytical model\n",
    "xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
    "c_cpu = np.around(c.detach().cpu().numpy(),decimals=4)\n",
    "L = HL.generateExpression(xi_Lcpu,expr)\n",
    "print(len(xi_L))\n",
    "print('exp({}*t)'.format(c_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fbbf84e-74af-4e42-a56c-c701b029dbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.57457733154297\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.57412338256836\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.573673248291016\n",
      "\n",
      "\n",
      "Stage  0\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.573219299316406\n",
      "expression length:\t 12\n",
      "Result stage 2: 7.09*x0 + -7.29*x0_t + -4.89*sin(x0) + 8.98*cos(x0) + 1.34*x0**2 + 9.0*x0_t**2 + 8.68*x0*sin(x0) + 4.62*x0_t*sin(x0) + -7.06*sin(x0)**2 + 3.11*x0_t*cos(x0) + 0.69*sin(x0)*cos(x0) + 1.19*cos(x0)**2\n",
      "exp([-6.6105])\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.572975158691406\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.57277297973633\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.572566986083984\n",
      "\n",
      "\n",
      "Stage  1\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.57236099243164\n",
      "expression length:\t 12\n",
      "Result stage 3: 7.1*x0 + -7.29*x0_t + -4.89*sin(x0) + 8.99*cos(x0) + 1.33*x0**2 + 9.01*x0_t**2 + 8.67*x0*sin(x0) + 4.62*x0_t*sin(x0) + -7.06*sin(x0)**2 + 3.11*x0_t*cos(x0) + 0.69*sin(x0)*cos(x0) + 1.19*cos(x0)**2\n",
      "exp([-6.6139])\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.57190704345703\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.57145309448242\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.57100296020508\n",
      "\n",
      "\n",
      "Stage  2\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.57053756713867\n",
      "expression length:\t 12\n",
      "Result stage 4: 7.06*x0 + -7.26*x0_t + -4.85*sin(x0) + 8.97*cos(x0) + 1.28*x0**2 + 8.98*x0_t**2 + 8.64*x0*sin(x0) + 4.58*x0_t*sin(x0) + -7.02*sin(x0)**2 + 3.07*x0_t*cos(x0) + 0.65*sin(x0)*cos(x0) + 1.15*cos(x0)**2\n",
      "exp([-6.575])\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5700798034668\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56962203979492\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56915283203125\n",
      "\n",
      "\n",
      "Stage  3\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56869125366211\n",
      "expression length:\t 12\n",
      "Result stage 5: 7.02*x0 + -7.22*x0_t + -4.81*sin(x0) + 8.94*cos(x0) + 1.22*x0**2 + 8.95*x0_t**2 + 8.61*x0*sin(x0) + 4.54*x0_t*sin(x0) + -6.98*sin(x0)**2 + 3.03*x0_t*cos(x0) + 0.6*sin(x0)*cos(x0) + 1.11*cos(x0)**2\n",
      "exp([-6.5359])\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56822204589844\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56775665283203\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.567283630371094\n",
      "\n",
      "\n",
      "Stage  4\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56681442260742\n",
      "expression length:\t 12\n",
      "Result stage 6: 6.98*x0 + -7.18*x0_t + -4.77*sin(x0) + 8.91*cos(x0) + 1.17*x0**2 + 8.92*x0_t**2 + 8.57*x0*sin(x0) + 4.5*x0_t*sin(x0) + -6.94*sin(x0)**2 + 2.99*x0_t*cos(x0) + 0.56*sin(x0)*cos(x0) + 1.07*cos(x0)**2\n",
      "exp([-6.4967])\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56633377075195\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56585693359375\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56538391113281\n",
      "\n",
      "\n",
      "Stage  5\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56489944458008\n",
      "expression length:\t 12\n",
      "Result stage 7: 6.94*x0 + -7.14*x0_t + -4.73*sin(x0) + 8.88*cos(x0) + 1.11*x0**2 + 8.89*x0_t**2 + 8.54*x0*sin(x0) + 4.46*x0_t*sin(x0) + -6.9*sin(x0)**2 + 2.95*x0_t*cos(x0) + 0.52*sin(x0)*cos(x0) + 1.03*cos(x0)**2\n",
      "exp([-6.4574])\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56441879272461\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56393051147461\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.563438415527344\n",
      "\n",
      "\n",
      "Stage  6\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.562957763671875\n",
      "expression length:\t 12\n",
      "Result stage 8: 6.9*x0 + -7.1*x0_t + -4.69*sin(x0) + 8.85*cos(x0) + 1.06*x0**2 + 8.87*x0_t**2 + 8.51*x0*sin(x0) + 4.42*x0_t*sin(x0) + -6.86*sin(x0)**2 + 2.91*x0_t*cos(x0) + 0.48*sin(x0)*cos(x0) + 0.99*cos(x0)**2\n",
      "exp([-6.4182])\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56246566772461\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56196975708008\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56147766113281\n",
      "\n",
      "\n",
      "Stage  7\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56098175048828\n",
      "expression length:\t 12\n",
      "Result stage 9: 6.86*x0 + -7.06*x0_t + -4.65*sin(x0) + 8.83*cos(x0) + 1.0*x0**2 + 8.84*x0_t**2 + 8.48*x0*sin(x0) + 4.38*x0_t*sin(x0) + -6.82*sin(x0)**2 + 2.87*x0_t*cos(x0) + 0.43*sin(x0)*cos(x0) + 0.95*cos(x0)**2\n",
      "exp([-6.3789])\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.56046676635742\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5599365234375\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55940628051758\n",
      "\n",
      "\n",
      "Stage  8\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55888366699219\n",
      "expression length:\t 12\n",
      "Result stage 10: 6.82*x0 + -7.02*x0_t + -4.61*sin(x0) + 8.8*cos(x0) + 0.95*x0**2 + 8.81*x0_t**2 + 8.45*x0*sin(x0) + 4.34*x0_t*sin(x0) + -6.78*sin(x0)**2 + 2.83*x0_t*cos(x0) + 0.39*sin(x0)*cos(x0) + 0.91*cos(x0)**2\n",
      "exp([-6.3396])\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.558349609375\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55781936645508\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55727767944336\n",
      "\n",
      "\n",
      "Stage  9\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55674743652344\n",
      "expression length:\t 12\n",
      "Result stage 11: 6.78*x0 + -6.98*x0_t + -4.57*sin(x0) + 8.77*cos(x0) + 0.89*x0**2 + 8.78*x0_t**2 + 8.42*x0*sin(x0) + 4.31*x0_t*sin(x0) + -6.74*sin(x0)**2 + 2.79*x0_t*cos(x0) + 0.35*sin(x0)*cos(x0) + 0.86*cos(x0)**2\n",
      "exp([-6.3003])\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55620574951172\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.555660247802734\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55510711669922\n",
      "\n",
      "\n",
      "Stage  10\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5545539855957\n",
      "expression length:\t 12\n",
      "Result stage 12: 6.74*x0 + -6.94*x0_t + -4.53*sin(x0) + 8.74*cos(x0) + 0.83*x0**2 + 8.75*x0_t**2 + 8.39*x0*sin(x0) + 4.27*x0_t*sin(x0) + -6.7*sin(x0)**2 + 2.75*x0_t*cos(x0) + 0.31*sin(x0)*cos(x0) + 0.82*cos(x0)**2\n",
      "exp([-6.261])\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.553993225097656\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.553436279296875\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55287170410156\n",
      "\n",
      "\n",
      "Stage  11\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55230712890625\n",
      "expression length:\t 12\n",
      "Result stage 13: 6.7*x0 + -6.9*x0_t + -4.49*sin(x0) + 8.72*cos(x0) + 0.77*x0**2 + 8.72*x0_t**2 + 8.35*x0*sin(x0) + 4.23*x0_t*sin(x0) + -6.66*sin(x0)**2 + 2.71*x0_t*cos(x0) + 0.26*sin(x0)*cos(x0) + 0.78*cos(x0)**2\n",
      "exp([-6.2217])\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55174255371094\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55117416381836\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.550601959228516\n",
      "\n",
      "\n",
      "Stage  12\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.55002975463867\n",
      "expression length:\t 12\n",
      "Result stage 14: 6.66*x0 + -6.86*x0_t + -4.45*sin(x0) + 8.69*cos(x0) + 0.72*x0**2 + 8.69*x0_t**2 + 8.32*x0*sin(x0) + 4.19*x0_t*sin(x0) + -6.62*sin(x0)**2 + 2.67*x0_t*cos(x0) + 0.22*sin(x0)*cos(x0) + 0.74*cos(x0)**2\n",
      "exp([-6.1823])\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.549461364746094\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.54887390136719\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.54829406738281\n",
      "\n",
      "\n",
      "Stage  13\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.547706604003906\n",
      "expression length:\t 12\n",
      "Result stage 15: 6.62*x0 + -6.83*x0_t + -4.41*sin(x0) + 8.66*cos(x0) + 0.66*x0**2 + 8.66*x0_t**2 + 8.29*x0*sin(x0) + 4.15*x0_t*sin(x0) + -6.58*sin(x0)**2 + 2.63*x0_t*cos(x0) + 0.18*sin(x0)*cos(x0) + 0.7*cos(x0)**2\n",
      "exp([-6.1427])\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.547119140625\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.546531677246094\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.54594039916992\n",
      "\n",
      "\n",
      "Stage  14\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.54534912109375\n",
      "expression length:\t 12\n",
      "Result stage 16: 6.58*x0 + -6.79*x0_t + -4.37*sin(x0) + 8.63*cos(x0) + 0.6*x0**2 + 8.63*x0_t**2 + 8.25*x0*sin(x0) + 4.11*x0_t*sin(x0) + -6.54*sin(x0)**2 + 2.59*x0_t*cos(x0) + 0.14*sin(x0)*cos(x0) + 0.65*cos(x0)**2\n",
      "exp([-6.103])\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.54475784301758\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.54415512084961\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.543556213378906\n",
      "\n",
      "\n",
      "Stage  15\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.54295349121094\n",
      "expression length:\t 11\n",
      "Result stage 17: 6.54*x0 + -6.75*x0_t + -4.33*sin(x0) + 8.6*cos(x0) + 0.54*x0**2 + 8.6*x0_t**2 + 8.22*x0*sin(x0) + 4.07*x0_t*sin(x0) + -6.5*sin(x0)**2 + 2.55*x0_t*cos(x0) + 0.61*cos(x0)**2\n",
      "exp([-6.0633])\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.54219436645508\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.541603088378906\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.54100799560547\n",
      "\n",
      "\n",
      "Stage  16\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.54042053222656\n",
      "expression length:\t 11\n",
      "Result stage 18: 6.5*x0 + -6.71*x0_t + -4.29*sin(x0) + 8.58*cos(x0) + 0.48*x0**2 + 8.57*x0_t**2 + 8.19*x0*sin(x0) + 4.03*x0_t*sin(x0) + -6.47*sin(x0)**2 + 2.51*x0_t*cos(x0) + 0.57*cos(x0)**2\n",
      "exp([-6.0236])\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53982162475586\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53921890258789\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53861618041992\n",
      "\n",
      "\n",
      "Stage  17\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53800964355469\n",
      "expression length:\t 11\n",
      "Result stage 19: 6.46*x0 + -6.67*x0_t + -4.25*sin(x0) + 8.55*cos(x0) + 0.43*x0**2 + 8.55*x0_t**2 + 8.15*x0*sin(x0) + 3.99*x0_t*sin(x0) + -6.43*sin(x0)**2 + 2.47*x0_t*cos(x0) + 0.53*cos(x0)**2\n",
      "exp([-5.984])\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53739929199219\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53678894042969\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53617858886719\n",
      "\n",
      "\n",
      "Stage  18\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53556442260742\n",
      "expression length:\t 11\n",
      "Result stage 20: 6.42*x0 + -6.63*x0_t + -4.21*sin(x0) + 8.52*cos(x0) + 0.37*x0**2 + 8.52*x0_t**2 + 8.12*x0*sin(x0) + 3.95*x0_t*sin(x0) + -6.39*sin(x0)**2 + 2.43*x0_t*cos(x0) + 0.49*cos(x0)**2\n",
      "exp([-5.944])\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53493881225586\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53431701660156\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.533695220947266\n",
      "\n",
      "\n",
      "Stage  19\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53306198120117\n",
      "expression length:\t 11\n",
      "Result stage 21: 6.38*x0 + -6.59*x0_t + -4.17*sin(x0) + 8.49*cos(x0) + 0.31*x0**2 + 8.49*x0_t**2 + 8.09*x0*sin(x0) + 3.91*x0_t*sin(x0) + -6.35*sin(x0)**2 + 2.39*x0_t*cos(x0) + 0.44*cos(x0)**2\n",
      "exp([-5.9039])\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.532432556152344\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53179931640625\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.531166076660156\n",
      "\n",
      "\n",
      "Stage  20\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.53052520751953\n",
      "expression length:\t 11\n",
      "Result stage 22: 6.34*x0 + -6.55*x0_t + -4.13*sin(x0) + 8.46*cos(x0) + 0.25*x0**2 + 8.45*x0_t**2 + 8.05*x0*sin(x0) + 3.87*x0_t*sin(x0) + -6.31*sin(x0)**2 + 2.35*x0_t*cos(x0) + 0.4*cos(x0)**2\n",
      "exp([-5.8639])\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.52988815307617\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.52924728393555\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.52861022949219\n",
      "\n",
      "\n",
      "Stage  21\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.52796173095703\n",
      "expression length:\t 11\n",
      "Result stage 23: 6.3*x0 + -6.51*x0_t + -4.09*sin(x0) + 8.44*cos(x0) + 0.19*x0**2 + 8.42*x0_t**2 + 8.02*x0*sin(x0) + 3.83*x0_t*sin(x0) + -6.27*sin(x0)**2 + 2.31*x0_t*cos(x0) + 0.36*cos(x0)**2\n",
      "exp([-5.8238])\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.527313232421875\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.52666473388672\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.52599334716797\n",
      "\n",
      "\n",
      "Stage  22\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.52531433105469\n",
      "expression length:\t 11\n",
      "Result stage 24: 6.26*x0 + -6.47*x0_t + -4.05*sin(x0) + 8.41*cos(x0) + 0.13*x0**2 + 8.39*x0_t**2 + 7.98*x0*sin(x0) + 3.79*x0_t*sin(x0) + -6.23*sin(x0)**2 + 2.27*x0_t*cos(x0) + 0.32*cos(x0)**2\n",
      "exp([-5.7838])\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.524627685546875\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.52394485473633\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.52325439453125\n",
      "\n",
      "\n",
      "Stage  23\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.52255630493164\n",
      "expression length:\t 10\n",
      "Result stage 25: 6.23*x0 + -6.43*x0_t + -4.01*sin(x0) + 8.38*cos(x0) + 8.36*x0_t**2 + 7.94*x0*sin(x0) + 3.75*x0_t*sin(x0) + -6.19*sin(x0)**2 + 2.23*x0_t*cos(x0) + 0.28*cos(x0)**2\n",
      "exp([-5.7437])\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51931381225586\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51924514770508\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5191764831543\n",
      "\n",
      "\n",
      "Stage  24\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.519107818603516\n",
      "expression length:\t 10\n",
      "Result stage 26: 6.19*x0 + -6.4*x0_t + -3.97*sin(x0) + 8.35*cos(x0) + 8.33*x0_t**2 + 7.9*x0*sin(x0) + 3.71*x0_t*sin(x0) + -6.15*sin(x0)**2 + 2.19*x0_t*cos(x0) + 0.24*cos(x0)**2\n",
      "exp([-5.7037])\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.519039154052734\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51897048950195\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51890563964844\n",
      "\n",
      "\n",
      "Stage  25\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.518836975097656\n",
      "expression length:\t 10\n",
      "Result stage 27: 6.15*x0 + -6.36*x0_t + -3.93*sin(x0) + 8.33*cos(x0) + 8.3*x0_t**2 + 7.86*x0*sin(x0) + 3.67*x0_t*sin(x0) + -6.11*sin(x0)**2 + 2.15*x0_t*cos(x0) + 0.19*cos(x0)**2\n",
      "exp([-5.6636])\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.518760681152344\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51869201660156\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51862335205078\n",
      "\n",
      "\n",
      "Stage  26\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5185546875\n",
      "expression length:\t 10\n",
      "Result stage 28: 6.11*x0 + -6.32*x0_t + -3.89*sin(x0) + 8.3*cos(x0) + 8.27*x0_t**2 + 7.82*x0*sin(x0) + 3.63*x0_t*sin(x0) + -6.07*sin(x0)**2 + 2.11*x0_t*cos(x0) + 0.15*cos(x0)**2\n",
      "exp([-5.6236])\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51848602294922\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51841354370117\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.518348693847656\n",
      "\n",
      "\n",
      "Stage  27\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51827621459961\n",
      "expression length:\t 10\n",
      "Result stage 29: 6.07*x0 + -6.28*x0_t + -3.85*sin(x0) + 8.27*cos(x0) + 8.24*x0_t**2 + 7.77*x0*sin(x0) + 3.59*x0_t*sin(x0) + -6.03*sin(x0)**2 + 2.07*x0_t*cos(x0) + 0.11*cos(x0)**2\n",
      "exp([-5.5835])\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51820755004883\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51813507080078\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.518062591552734\n",
      "\n",
      "\n",
      "Stage  28\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51799392700195\n",
      "expression length:\t 9\n",
      "Result stage 30: 6.03*x0 + -6.24*x0_t + -3.81*sin(x0) + 8.24*cos(x0) + 8.2*x0_t**2 + 7.73*x0*sin(x0) + 3.55*x0_t*sin(x0) + -5.99*sin(x0)**2 + 2.03*x0_t*cos(x0)\n",
      "exp([-5.5435])\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51802444458008\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51793670654297\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51784896850586\n",
      "\n",
      "\n",
      "Stage  29\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51776123046875\n",
      "expression length:\t 9\n",
      "Result stage 31: 5.99*x0 + -6.2*x0_t + -3.77*sin(x0) + 8.21*cos(x0) + 8.17*x0_t**2 + 7.69*x0*sin(x0) + 3.51*x0_t*sin(x0) + -5.95*sin(x0)**2 + 1.99*x0_t*cos(x0)\n",
      "exp([-5.5034])\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51767349243164\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51758575439453\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.517494201660156\n",
      "\n",
      "\n",
      "Stage  30\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51740264892578\n",
      "expression length:\t 9\n",
      "Result stage 32: 5.95*x0 + -6.16*x0_t + -3.73*sin(x0) + 8.19*cos(x0) + 8.14*x0_t**2 + 7.65*x0*sin(x0) + 3.47*x0_t*sin(x0) + -5.91*sin(x0)**2 + 1.95*x0_t*cos(x0)\n",
      "exp([-5.4633])\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51731491088867\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51721954345703\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51713180541992\n",
      "\n",
      "\n",
      "Stage  31\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51704406738281\n",
      "expression length:\t 9\n",
      "Result stage 33: 5.91*x0 + -6.12*x0_t + -3.69*sin(x0) + 8.16*cos(x0) + 8.11*x0_t**2 + 7.61*x0*sin(x0) + 3.43*x0_t*sin(x0) + -5.87*sin(x0)**2 + 1.91*x0_t*cos(x0)\n",
      "exp([-5.4233])\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51694869995117\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51686096191406\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51676559448242\n",
      "\n",
      "\n",
      "Stage  32\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51667785644531\n",
      "expression length:\t 9\n",
      "Result stage 34: 5.87*x0 + -6.08*x0_t + -3.64*sin(x0) + 8.13*cos(x0) + 8.08*x0_t**2 + 7.57*x0*sin(x0) + 3.39*x0_t*sin(x0) + -5.83*sin(x0)**2 + 1.87*x0_t*cos(x0)\n",
      "exp([-5.3832])\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51658248901367\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51648712158203\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.516395568847656\n",
      "\n",
      "\n",
      "Stage  33\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.516300201416016\n",
      "expression length:\t 9\n",
      "Result stage 35: 5.83*x0 + -6.04*x0_t + -3.6*sin(x0) + 8.1*cos(x0) + 8.05*x0_t**2 + 7.53*x0*sin(x0) + 3.35*x0_t*sin(x0) + -5.79*sin(x0)**2 + 1.83*x0_t*cos(x0)\n",
      "exp([-5.3432])\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51620864868164\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51611328125\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51601791381836\n",
      "\n",
      "\n",
      "Stage  34\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51592254638672\n",
      "expression length:\t 9\n",
      "Result stage 36: 5.79*x0 + -6.0*x0_t + -3.56*sin(x0) + 8.07*cos(x0) + 8.01*x0_t**2 + 7.48*x0*sin(x0) + 3.31*x0_t*sin(x0) + -5.75*sin(x0)**2 + 1.79*x0_t*cos(x0)\n",
      "exp([-5.3031])\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.515830993652344\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51573181152344\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.515621185302734\n",
      "\n",
      "\n",
      "Stage  35\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5155143737793\n",
      "expression length:\t 9\n",
      "Result stage 37: 5.75*x0 + -5.96*x0_t + -3.52*sin(x0) + 8.05*cos(x0) + 7.98*x0_t**2 + 7.44*x0*sin(x0) + 3.27*x0_t*sin(x0) + -5.71*sin(x0)**2 + 1.75*x0_t*cos(x0)\n",
      "exp([-5.2631])\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51540756225586\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51530075073242\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51518630981445\n",
      "\n",
      "\n",
      "Stage  36\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.515079498291016\n",
      "expression length:\t 9\n",
      "Result stage 38: 5.72*x0 + -5.93*x0_t + -3.48*sin(x0) + 8.02*cos(x0) + 7.94*x0_t**2 + 7.4*x0*sin(x0) + 3.23*x0_t*sin(x0) + -5.67*sin(x0)**2 + 1.71*x0_t*cos(x0)\n",
      "exp([-5.223])\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51496887207031\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.514862060546875\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51476287841797\n",
      "\n",
      "\n",
      "Stage  37\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51469039916992\n",
      "expression length:\t 9\n",
      "Result stage 39: 5.68*x0 + -5.89*x0_t + -3.44*sin(x0) + 7.99*cos(x0) + 7.9*x0_t**2 + 7.36*x0*sin(x0) + 3.19*x0_t*sin(x0) + -5.63*sin(x0)**2 + 1.67*x0_t*cos(x0)\n",
      "exp([-5.183])\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51461410522461\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5145378112793\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51446533203125\n",
      "\n",
      "\n",
      "Stage  38\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51439666748047\n",
      "expression length:\t 9\n",
      "Result stage 40: 5.64*x0 + -5.85*x0_t + -3.4*sin(x0) + 7.95*cos(x0) + 7.86*x0_t**2 + 7.32*x0*sin(x0) + 3.15*x0_t*sin(x0) + -5.59*sin(x0)**2 + 1.63*x0_t*cos(x0)\n",
      "exp([-5.1429])\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.514320373535156\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.514244079589844\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5141716003418\n",
      "\n",
      "\n",
      "Stage  39\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51409912109375\n",
      "expression length:\t 9\n",
      "Result stage 41: 5.6*x0 + -5.81*x0_t + -3.36*sin(x0) + 7.91*cos(x0) + 7.82*x0_t**2 + 7.28*x0*sin(x0) + 3.11*x0_t*sin(x0) + -5.55*sin(x0)**2 + 1.59*x0_t*cos(x0)\n",
      "exp([-5.1029])\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51402282714844\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51394271850586\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51387023925781\n",
      "\n",
      "\n",
      "Stage  40\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51378631591797\n",
      "expression length:\t 9\n",
      "Result stage 42: 5.56*x0 + -5.77*x0_t + -3.32*sin(x0) + 7.87*cos(x0) + 7.78*x0_t**2 + 7.24*x0*sin(x0) + 3.07*x0_t*sin(x0) + -5.51*sin(x0)**2 + 1.55*x0_t*cos(x0)\n",
      "exp([-5.0628])\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.513710021972656\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51362991333008\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5135498046875\n",
      "\n",
      "\n",
      "Stage  41\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51346969604492\n",
      "expression length:\t 9\n",
      "Result stage 43: 5.52*x0 + -5.73*x0_t + -3.28*sin(x0) + 7.84*cos(x0) + 7.73*x0_t**2 + 7.2*x0*sin(x0) + 3.03*x0_t*sin(x0) + -5.47*sin(x0)**2 + 1.51*x0_t*cos(x0)\n",
      "exp([-5.0227])\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51338577270508\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.513301849365234\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.513221740722656\n",
      "\n",
      "\n",
      "Stage  42\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51313781738281\n",
      "expression length:\t 9\n",
      "Result stage 44: 5.48*x0 + -5.69*x0_t + -3.24*sin(x0) + 7.8*cos(x0) + 7.69*x0_t**2 + 7.15*x0*sin(x0) + 2.99*x0_t*sin(x0) + -5.43*sin(x0)**2 + 1.47*x0_t*cos(x0)\n",
      "exp([-4.9827])\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51305389404297\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51297378540039\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51288986206055\n",
      "\n",
      "\n",
      "Stage  43\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5128059387207\n",
      "expression length:\t 9\n",
      "Result stage 45: 5.44*x0 + -5.65*x0_t + -3.19*sin(x0) + 7.76*cos(x0) + 7.65*x0_t**2 + 7.11*x0*sin(x0) + 2.95*x0_t*sin(x0) + -5.39*sin(x0)**2 + 1.43*x0_t*cos(x0)\n",
      "exp([-4.9426])\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51271438598633\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.512630462646484\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.512542724609375\n",
      "\n",
      "\n",
      "Stage  44\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.512447357177734\n",
      "expression length:\t 9\n",
      "Result stage 46: 5.4*x0 + -5.61*x0_t + -3.15*sin(x0) + 7.73*cos(x0) + 7.61*x0_t**2 + 7.07*x0*sin(x0) + 2.91*x0_t*sin(x0) + -5.35*sin(x0)**2 + 1.39*x0_t*cos(x0)\n",
      "exp([-4.9026])\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51236343383789\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51226806640625\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.512184143066406\n",
      "\n",
      "\n",
      "Stage  45\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51209259033203\n",
      "expression length:\t 9\n",
      "Result stage 47: 5.36*x0 + -5.57*x0_t + -3.11*sin(x0) + 7.69*cos(x0) + 7.57*x0_t**2 + 7.03*x0*sin(x0) + 2.87*x0_t*sin(x0) + -5.31*sin(x0)**2 + 1.35*x0_t*cos(x0)\n",
      "exp([-4.8625])\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.511993408203125\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.511905670166016\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.511817932128906\n",
      "\n",
      "\n",
      "Stage  46\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51172637939453\n",
      "expression length:\t 9\n",
      "Result stage 48: 5.32*x0 + -5.53*x0_t + -3.07*sin(x0) + 7.65*cos(x0) + 7.53*x0_t**2 + 6.98*x0*sin(x0) + 2.83*x0_t*sin(x0) + -5.27*sin(x0)**2 + 1.31*x0_t*cos(x0)\n",
      "exp([-4.8225])\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51163101196289\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.511531829833984\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.511436462402344\n",
      "\n",
      "\n",
      "Stage  47\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51134490966797\n",
      "expression length:\t 9\n",
      "Result stage 49: 5.28*x0 + -5.5*x0_t + -3.03*sin(x0) + 7.62*cos(x0) + 7.49*x0_t**2 + 6.94*x0*sin(x0) + 2.79*x0_t*sin(x0) + -5.24*sin(x0)**2 + 1.27*x0_t*cos(x0)\n",
      "exp([-4.7824])\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51124572753906\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51115036010742\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.511051177978516\n",
      "\n",
      "\n",
      "Stage  48\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.510955810546875\n",
      "expression length:\t 9\n",
      "Result stage 50: 5.24*x0 + -5.46*x0_t + -2.99*sin(x0) + 7.58*cos(x0) + 7.45*x0_t**2 + 6.9*x0*sin(x0) + 2.75*x0_t*sin(x0) + -5.2*sin(x0)**2 + 1.23*x0_t*cos(x0)\n",
      "exp([-4.7424])\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51085662841797\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51075744628906\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51066207885742\n",
      "\n",
      "\n",
      "Stage  49\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51055908203125\n",
      "expression length:\t 9\n",
      "Result stage 51: 5.2*x0 + -5.42*x0_t + -2.95*sin(x0) + 7.54*cos(x0) + 7.41*x0_t**2 + 6.86*x0*sin(x0) + 2.71*x0_t*sin(x0) + -5.16*sin(x0)**2 + 1.19*x0_t*cos(x0)\n",
      "exp([-4.7023])\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51046371459961\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5103645324707\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51026153564453\n",
      "\n",
      "\n",
      "Stage  50\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.510162353515625\n",
      "expression length:\t 9\n",
      "Result stage 52: 5.16*x0 + -5.38*x0_t + -2.91*sin(x0) + 7.51*cos(x0) + 7.37*x0_t**2 + 6.81*x0*sin(x0) + 2.67*x0_t*sin(x0) + -5.12*sin(x0)**2 + 1.15*x0_t*cos(x0)\n",
      "exp([-4.6623])\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.51006317138672\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50996017456055\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50986099243164\n",
      "\n",
      "\n",
      "Stage  51\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5097541809082\n",
      "expression length:\t 9\n",
      "Result stage 53: 5.12*x0 + -5.34*x0_t + -2.87*sin(x0) + 7.47*cos(x0) + 7.33*x0_t**2 + 6.77*x0*sin(x0) + 2.63*x0_t*sin(x0) + -5.08*sin(x0)**2 + 1.11*x0_t*cos(x0)\n",
      "exp([-4.622])\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50965118408203\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50954818725586\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50944519042969\n",
      "\n",
      "\n",
      "Stage  52\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50933837890625\n",
      "expression length:\t 9\n",
      "Result stage 54: 5.08*x0 + -5.3*x0_t + -2.83*sin(x0) + 7.43*cos(x0) + 7.29*x0_t**2 + 6.73*x0*sin(x0) + 2.59*x0_t*sin(x0) + -5.04*sin(x0)**2 + 1.07*x0_t*cos(x0)\n",
      "exp([-4.5816])\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.509239196777344\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50912857055664\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50902557373047\n",
      "\n",
      "\n",
      "Stage  53\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50891876220703\n",
      "expression length:\t 9\n",
      "Result stage 55: 5.04*x0 + -5.26*x0_t + -2.79*sin(x0) + 7.39*cos(x0) + 7.25*x0_t**2 + 6.68*x0*sin(x0) + 2.55*x0_t*sin(x0) + -5.0*sin(x0)**2 + 1.03*x0_t*cos(x0)\n",
      "exp([-4.5411])\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50881576538086\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.508705139160156\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50859832763672\n",
      "\n",
      "\n",
      "Stage  54\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.508487701416016\n",
      "expression length:\t 9\n",
      "Result stage 56: 5.01*x0 + -5.22*x0_t + -2.75*sin(x0) + 7.36*cos(x0) + 7.21*x0_t**2 + 6.64*x0*sin(x0) + 2.51*x0_t*sin(x0) + -4.96*sin(x0)**2 + 0.99*x0_t*cos(x0)\n",
      "exp([-4.5003])\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50837707519531\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.508262634277344\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50815200805664\n",
      "\n",
      "\n",
      "Stage  55\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5080451965332\n",
      "expression length:\t 9\n",
      "Result stage 57: 4.97*x0 + -5.18*x0_t + -2.7*sin(x0) + 7.32*cos(x0) + 7.16*x0_t**2 + 6.6*x0*sin(x0) + 2.47*x0_t*sin(x0) + -4.92*sin(x0)**2 + 0.95*x0_t*cos(x0)\n",
      "exp([-4.4594])\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.507930755615234\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50782012939453\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50770568847656\n",
      "\n",
      "\n",
      "Stage  56\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50758361816406\n",
      "expression length:\t 9\n",
      "Result stage 58: 4.93*x0 + -5.14*x0_t + -2.66*sin(x0) + 7.28*cos(x0) + 7.12*x0_t**2 + 6.55*x0*sin(x0) + 2.43*x0_t*sin(x0) + -4.88*sin(x0)**2 + 0.91*x0_t*cos(x0)\n",
      "exp([-4.4186])\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.507469177246094\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50735092163086\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50722885131836\n",
      "\n",
      "\n",
      "Stage  57\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.507110595703125\n",
      "expression length:\t 9\n",
      "Result stage 59: 4.89*x0 + -5.1*x0_t + -2.62*sin(x0) + 7.25*cos(x0) + 7.08*x0_t**2 + 6.51*x0*sin(x0) + 2.39*x0_t*sin(x0) + -4.84*sin(x0)**2 + 0.87*x0_t*cos(x0)\n",
      "exp([-4.3778])\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.506996154785156\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.506874084472656\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50675582885742\n",
      "\n",
      "\n",
      "Stage  58\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50663375854492\n",
      "expression length:\t 9\n",
      "Result stage 60: 4.85*x0 + -5.07*x0_t + -2.58*sin(x0) + 7.21*cos(x0) + 7.04*x0_t**2 + 6.46*x0*sin(x0) + 2.35*x0_t*sin(x0) + -4.8*sin(x0)**2 + 0.83*x0_t*cos(x0)\n",
      "exp([-4.337])\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50651168823242\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50638961791992\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50626754760742\n",
      "\n",
      "\n",
      "Stage  59\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.506141662597656\n",
      "expression length:\t 9\n",
      "Result stage 61: 4.81*x0 + -5.03*x0_t + -2.54*sin(x0) + 7.17*cos(x0) + 7.0*x0_t**2 + 6.42*x0*sin(x0) + 2.31*x0_t*sin(x0) + -4.76*sin(x0)**2 + 0.78*x0_t*cos(x0)\n",
      "exp([-4.2962])\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50601577758789\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.505889892578125\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50576400756836\n",
      "\n",
      "\n",
      "Stage  60\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50563430786133\n",
      "expression length:\t 9\n",
      "Result stage 62: 4.77*x0 + -4.99*x0_t + -2.5*sin(x0) + 7.14*cos(x0) + 6.96*x0_t**2 + 6.37*x0*sin(x0) + 2.27*x0_t*sin(x0) + -4.72*sin(x0)**2 + 0.74*x0_t*cos(x0)\n",
      "exp([-4.255])\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50550079345703\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50537872314453\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5052490234375\n",
      "\n",
      "\n",
      "Stage  61\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50511932373047\n",
      "expression length:\t 9\n",
      "Result stage 63: 4.73*x0 + -4.95*x0_t + -2.46*sin(x0) + 7.1*cos(x0) + 6.92*x0_t**2 + 6.33*x0*sin(x0) + 2.23*x0_t*sin(x0) + -4.68*sin(x0)**2 + 0.7*x0_t*cos(x0)\n",
      "exp([-4.2138])\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50498962402344\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50485610961914\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.504722595214844\n",
      "\n",
      "\n",
      "Stage  62\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50458908081055\n",
      "expression length:\t 9\n",
      "Result stage 64: 4.69*x0 + -4.91*x0_t + -2.42*sin(x0) + 7.06*cos(x0) + 6.88*x0_t**2 + 6.29*x0*sin(x0) + 2.19*x0_t*sin(x0) + -4.65*sin(x0)**2 + 0.66*x0_t*cos(x0)\n",
      "exp([-4.1726])\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.504459381103516\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50432205200195\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50418472290039\n",
      "\n",
      "\n",
      "Stage  63\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.504051208496094\n",
      "expression length:\t 9\n",
      "Result stage 65: 4.65*x0 + -4.87*x0_t + -2.38*sin(x0) + 7.03*cos(x0) + 6.84*x0_t**2 + 6.24*x0*sin(x0) + 2.15*x0_t*sin(x0) + -4.61*sin(x0)**2 + 0.62*x0_t*cos(x0)\n",
      "exp([-4.1314])\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.503910064697266\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50376892089844\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50362777709961\n",
      "\n",
      "\n",
      "Stage  64\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50348663330078\n",
      "expression length:\t 9\n",
      "Result stage 66: 4.61*x0 + -4.83*x0_t + -2.34*sin(x0) + 6.99*cos(x0) + 6.79*x0_t**2 + 6.2*x0*sin(x0) + 2.11*x0_t*sin(x0) + -4.57*sin(x0)**2 + 0.58*x0_t*cos(x0)\n",
      "exp([-4.0902])\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50334167480469\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50320053100586\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50305938720703\n",
      "\n",
      "\n",
      "Stage  65\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50291061401367\n",
      "expression length:\t 9\n",
      "Result stage 67: 4.57*x0 + -4.79*x0_t + -2.29*sin(x0) + 6.95*cos(x0) + 6.75*x0_t**2 + 6.15*x0*sin(x0) + 2.07*x0_t*sin(x0) + -4.53*sin(x0)**2 + 0.53*x0_t*cos(x0)\n",
      "exp([-4.049])\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50276184082031\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50261306762695\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.502464294433594\n",
      "\n",
      "\n",
      "Stage  66\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.502315521240234\n",
      "expression length:\t 9\n",
      "Result stage 68: 4.53*x0 + -4.75*x0_t + -2.25*sin(x0) + 6.92*cos(x0) + 6.71*x0_t**2 + 6.11*x0*sin(x0) + 2.03*x0_t*sin(x0) + -4.49*sin(x0)**2 + 0.49*x0_t*cos(x0)\n",
      "exp([-4.0078])\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50216293334961\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.502010345458984\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.501853942871094\n",
      "\n",
      "\n",
      "Stage  67\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.5016975402832\n",
      "expression length:\t 9\n",
      "Result stage 69: 4.49*x0 + -4.71*x0_t + -2.21*sin(x0) + 6.88*cos(x0) + 6.67*x0_t**2 + 6.06*x0*sin(x0) + 1.99*x0_t*sin(x0) + -4.45*sin(x0)**2 + 0.45*x0_t*cos(x0)\n",
      "exp([-3.9649])\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50154113769531\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50138473510742\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.501220703125\n",
      "\n",
      "\n",
      "Stage  68\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50105285644531\n",
      "expression length:\t 9\n",
      "Result stage 70: 4.45*x0 + -4.67*x0_t + -2.17*sin(x0) + 6.84*cos(x0) + 6.63*x0_t**2 + 6.02*x0*sin(x0) + 1.95*x0_t*sin(x0) + -4.41*sin(x0)**2 + 0.41*x0_t*cos(x0)\n",
      "exp([-3.9216])\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50088119506836\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.500709533691406\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50053787231445\n",
      "\n",
      "\n",
      "Stage  69\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.500362396240234\n",
      "expression length:\t 9\n",
      "Result stage 71: 4.41*x0 + -4.64*x0_t + -2.12*sin(x0) + 6.81*cos(x0) + 6.58*x0_t**2 + 5.97*x0*sin(x0) + 1.91*x0_t*sin(x0) + -4.37*sin(x0)**2 + 0.36*x0_t*cos(x0)\n",
      "exp([-3.8783])\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.50018310546875\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.500003814697266\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49981689453125\n",
      "\n",
      "\n",
      "Stage  70\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.499637603759766\n",
      "expression length:\t 9\n",
      "Result stage 72: 4.37*x0 + -4.6*x0_t + -2.08*sin(x0) + 6.77*cos(x0) + 6.54*x0_t**2 + 5.93*x0*sin(x0) + 1.87*x0_t*sin(x0) + -4.33*sin(x0)**2 + 0.32*x0_t*cos(x0)\n",
      "exp([-3.835])\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49945068359375\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.499263763427734\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49907684326172\n",
      "\n",
      "\n",
      "Stage  71\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49889373779297\n",
      "expression length:\t 9\n",
      "Result stage 73: 4.33*x0 + -4.56*x0_t + -2.04*sin(x0) + 6.74*cos(x0) + 6.5*x0_t**2 + 5.88*x0*sin(x0) + 1.83*x0_t*sin(x0) + -4.29*sin(x0)**2 + 0.28*x0_t*cos(x0)\n",
      "exp([-3.7917])\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49869918823242\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49850845336914\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49831771850586\n",
      "\n",
      "\n",
      "Stage  72\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49812316894531\n",
      "expression length:\t 9\n",
      "Result stage 74: 4.29*x0 + -4.52*x0_t + -2.0*sin(x0) + 6.7*cos(x0) + 6.46*x0_t**2 + 5.83*x0*sin(x0) + 1.79*x0_t*sin(x0) + -4.25*sin(x0)**2 + 0.24*x0_t*cos(x0)\n",
      "exp([-3.7483])\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49793243408203\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49773406982422\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4975471496582\n",
      "\n",
      "\n",
      "Stage  73\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.497344970703125\n",
      "expression length:\t 9\n",
      "Result stage 75: 4.25*x0 + -4.48*x0_t + -1.96*sin(x0) + 6.67*cos(x0) + 6.42*x0_t**2 + 5.79*x0*sin(x0) + 1.75*x0_t*sin(x0) + -4.22*sin(x0)**2 + 0.19*x0_t*cos(x0)\n",
      "exp([-3.7049])\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49715042114258\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.496944427490234\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49674606323242\n",
      "\n",
      "\n",
      "Stage  74\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.496543884277344\n",
      "expression length:\t 9\n",
      "Result stage 76: 4.22*x0 + -4.44*x0_t + -1.92*sin(x0) + 6.63*cos(x0) + 6.38*x0_t**2 + 5.74*x0*sin(x0) + 1.71*x0_t*sin(x0) + -4.18*sin(x0)**2 + 0.15*x0_t*cos(x0)\n",
      "exp([-3.6614])\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.496337890625\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49613571166992\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49592590332031\n",
      "\n",
      "\n",
      "Stage  75\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49571228027344\n",
      "expression length:\t 9\n",
      "Result stage 77: 4.18*x0 + -4.4*x0_t + -1.88*sin(x0) + 6.6*cos(x0) + 6.33*x0_t**2 + 5.69*x0*sin(x0) + 1.67*x0_t*sin(x0) + -4.14*sin(x0)**2 + 0.11*x0_t*cos(x0)\n",
      "exp([-3.6168])\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49549865722656\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49528503417969\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49507141113281\n",
      "\n",
      "\n",
      "Stage  76\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49485397338867\n",
      "expression length:\t 8\n",
      "Result stage 78: 4.14*x0 + -4.36*x0_t + -1.83*sin(x0) + 6.57*cos(x0) + 6.29*x0_t**2 + 5.65*x0*sin(x0) + 1.63*x0_t*sin(x0) + -4.1*sin(x0)**2\n",
      "exp([-3.5721])\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.494632720947266\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.494415283203125\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49418640136719\n",
      "\n",
      "\n",
      "Stage  77\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49396514892578\n",
      "expression length:\t 8\n",
      "Result stage 79: 4.1*x0 + -4.32*x0_t + -1.79*sin(x0) + 6.53*cos(x0) + 6.25*x0_t**2 + 5.6*x0*sin(x0) + 1.59*x0_t*sin(x0) + -4.06*sin(x0)**2\n",
      "exp([-3.5274])\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.493743896484375\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.493507385253906\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49325942993164\n",
      "\n",
      "\n",
      "Stage  78\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.493019104003906\n",
      "expression length:\t 8\n",
      "Result stage 80: 4.06*x0 + -4.28*x0_t + -1.75*sin(x0) + 6.5*cos(x0) + 6.21*x0_t**2 + 5.55*x0*sin(x0) + 1.55*x0_t*sin(x0) + -4.02*sin(x0)**2\n",
      "exp([-3.4824])\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.492774963378906\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.492530822753906\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49228286743164\n",
      "\n",
      "\n",
      "Stage  79\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.492034912109375\n",
      "expression length:\t 8\n",
      "Result stage 81: 4.02*x0 + -4.24*x0_t + -1.71*sin(x0) + 6.46*cos(x0) + 6.17*x0_t**2 + 5.51*x0*sin(x0) + 1.51*x0_t*sin(x0) + -3.99*sin(x0)**2\n",
      "exp([-3.4372])\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49177932739258\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49152374267578\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49127197265625\n",
      "\n",
      "\n",
      "Stage  80\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49102020263672\n",
      "expression length:\t 8\n",
      "Result stage 82: 3.98*x0 + -4.21*x0_t + -1.67*sin(x0) + 6.43*cos(x0) + 6.13*x0_t**2 + 5.46*x0*sin(x0) + 1.47*x0_t*sin(x0) + -3.95*sin(x0)**2\n",
      "exp([-3.3922])\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49076461791992\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.49050521850586\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4902458190918\n",
      "\n",
      "\n",
      "Stage  81\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48998260498047\n",
      "expression length:\t 8\n",
      "Result stage 83: 3.94*x0 + -4.17*x0_t + -1.63*sin(x0) + 6.4*cos(x0) + 6.08*x0_t**2 + 5.41*x0*sin(x0) + 1.43*x0_t*sin(x0) + -3.91*sin(x0)**2\n",
      "exp([-3.3472])\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.489715576171875\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48944091796875\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48916244506836\n",
      "\n",
      "\n",
      "Stage  82\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48887634277344\n",
      "expression length:\t 8\n",
      "Result stage 84: 3.9*x0 + -4.13*x0_t + -1.59*sin(x0) + 6.37*cos(x0) + 6.04*x0_t**2 + 5.37*x0*sin(x0) + 1.39*x0_t*sin(x0) + -3.87*sin(x0)**2\n",
      "exp([-3.3021])\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.488590240478516\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.488304138183594\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48801040649414\n",
      "\n",
      "\n",
      "Stage  83\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48771667480469\n",
      "expression length:\t 8\n",
      "Result stage 85: 3.86*x0 + -4.09*x0_t + -1.55*sin(x0) + 6.33*cos(x0) + 6.0*x0_t**2 + 5.32*x0*sin(x0) + 1.35*x0_t*sin(x0) + -3.84*sin(x0)**2\n",
      "exp([-3.2559])\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48741912841797\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48712158203125\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.486820220947266\n",
      "\n",
      "\n",
      "Stage  84\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48651123046875\n",
      "expression length:\t 8\n",
      "Result stage 86: 3.82*x0 + -4.05*x0_t + -1.51*sin(x0) + 6.3*cos(x0) + 5.96*x0_t**2 + 5.27*x0*sin(x0) + 1.31*x0_t*sin(x0) + -3.8*sin(x0)**2\n",
      "exp([-3.2094])\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48619842529297\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.485870361328125\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48554229736328\n",
      "\n",
      "\n",
      "Stage  85\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48521041870117\n",
      "expression length:\t 8\n",
      "Result stage 87: 3.78*x0 + -4.01*x0_t + -1.47*sin(x0) + 6.27*cos(x0) + 5.92*x0_t**2 + 5.22*x0*sin(x0) + 1.27*x0_t*sin(x0) + -3.76*sin(x0)**2\n",
      "exp([-3.1624])\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4848747253418\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48453903198242\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48419952392578\n",
      "\n",
      "\n",
      "Stage  86\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.483848571777344\n",
      "expression length:\t 8\n",
      "Result stage 88: 3.74*x0 + -3.97*x0_t + -1.42*sin(x0) + 6.24*cos(x0) + 5.87*x0_t**2 + 5.18*x0*sin(x0) + 1.23*x0_t*sin(x0) + -3.72*sin(x0)**2\n",
      "exp([-3.115])\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48349380493164\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48313522338867\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.482765197753906\n",
      "\n",
      "\n",
      "Stage  87\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48238754272461\n",
      "expression length:\t 8\n",
      "Result stage 89: 3.7*x0 + -3.93*x0_t + -1.38*sin(x0) + 6.21*cos(x0) + 5.83*x0_t**2 + 5.13*x0*sin(x0) + 1.19*x0_t*sin(x0) + -3.69*sin(x0)**2\n",
      "exp([-3.0675])\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48200988769531\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48162841796875\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48123550415039\n",
      "\n",
      "\n",
      "Stage  88\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4808464050293\n",
      "expression length:\t 8\n",
      "Result stage 90: 3.66*x0 + -3.89*x0_t + -1.34*sin(x0) + 6.18*cos(x0) + 5.79*x0_t**2 + 5.07*x0*sin(x0) + 1.15*x0_t*sin(x0) + -3.65*sin(x0)**2\n",
      "exp([-3.02])\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48044967651367\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.48005676269531\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47964096069336\n",
      "\n",
      "\n",
      "Stage  89\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47923278808594\n",
      "expression length:\t 8\n",
      "Result stage 91: 3.62*x0 + -3.85*x0_t + -1.3*sin(x0) + 6.15*cos(x0) + 5.75*x0_t**2 + 5.02*x0*sin(x0) + 1.11*x0_t*sin(x0) + -3.61*sin(x0)**2\n",
      "exp([-2.9722])\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47881317138672\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.478389739990234\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.477962493896484\n",
      "\n",
      "\n",
      "Stage  90\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47753143310547\n",
      "expression length:\t 8\n",
      "Result stage 92: 3.58*x0 + -3.81*x0_t + -1.26*sin(x0) + 6.12*cos(x0) + 5.7*x0_t**2 + 4.97*x0*sin(x0) + 1.07*x0_t*sin(x0) + -3.58*sin(x0)**2\n",
      "exp([-2.9244])\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47709274291992\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47663879394531\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.476173400878906\n",
      "\n",
      "\n",
      "Stage  91\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.475704193115234\n",
      "expression length:\t 8\n",
      "Result stage 93: 3.54*x0 + -3.77*x0_t + -1.22*sin(x0) + 6.09*cos(x0) + 5.66*x0_t**2 + 4.92*x0*sin(x0) + 1.03*x0_t*sin(x0) + -3.54*sin(x0)**2\n",
      "exp([-2.8764])\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47522735595703\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47475051879883\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.474266052246094\n",
      "\n",
      "\n",
      "Stage  92\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.473777770996094\n",
      "expression length:\t 8\n",
      "Result stage 94: 3.5*x0 + -3.73*x0_t + -1.18*sin(x0) + 6.06*cos(x0) + 5.61*x0_t**2 + 4.87*x0*sin(x0) + 0.99*x0_t*sin(x0) + -3.5*sin(x0)**2\n",
      "exp([-2.8281])\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4732780456543\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4727783203125\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47226333618164\n",
      "\n",
      "\n",
      "Stage  93\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47175216674805\n",
      "expression length:\t 8\n",
      "Result stage 95: 3.46*x0 + -3.69*x0_t + -1.14*sin(x0) + 6.04*cos(x0) + 5.57*x0_t**2 + 4.82*x0*sin(x0) + 0.95*x0_t*sin(x0) + -3.46*sin(x0)**2\n",
      "exp([-2.7796])\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.471229553222656\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.470699310302734\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.47016525268555\n",
      "\n",
      "\n",
      "Stage  94\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4696159362793\n",
      "expression length:\t 8\n",
      "Result stage 96: 3.42*x0 + -3.65*x0_t + -1.1*sin(x0) + 6.01*cos(x0) + 5.53*x0_t**2 + 4.77*x0*sin(x0) + 0.91*x0_t*sin(x0) + -3.43*sin(x0)**2\n",
      "exp([-2.7307])\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.469051361083984\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.46846389770508\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.46786880493164\n",
      "\n",
      "\n",
      "Stage  95\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.46727752685547\n",
      "expression length:\t 8\n",
      "Result stage 97: 3.38*x0 + -3.61*x0_t + -1.05*sin(x0) + 5.98*cos(x0) + 5.48*x0_t**2 + 4.71*x0*sin(x0) + 0.87*x0_t*sin(x0) + -3.39*sin(x0)**2\n",
      "exp([-2.6817])\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.46666717529297\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.46604537963867\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.465415954589844\n",
      "\n",
      "\n",
      "Stage  96\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.46478271484375\n",
      "expression length:\t 8\n",
      "Result stage 98: 3.34*x0 + -3.57*x0_t + -1.01*sin(x0) + 5.96*cos(x0) + 5.44*x0_t**2 + 4.66*x0*sin(x0) + 0.83*x0_t*sin(x0) + -3.35*sin(x0)**2\n",
      "exp([-2.6324])\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.46413803100586\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.46348190307617\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.462825775146484\n",
      "\n",
      "\n",
      "Stage  97\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4621467590332\n",
      "expression length:\t 8\n",
      "Result stage 99: 3.3*x0 + -3.53*x0_t + -0.97*sin(x0) + 5.93*cos(x0) + 5.39*x0_t**2 + 4.61*x0*sin(x0) + 0.79*x0_t*sin(x0) + -3.32*sin(x0)**2\n",
      "exp([-2.5829])\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.461463928222656\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.46076583862305\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.46005630493164\n",
      "\n",
      "\n",
      "Stage  98\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4593505859375\n",
      "expression length:\t 8\n",
      "Result stage 100: 3.26*x0 + -3.49*x0_t + -0.93*sin(x0) + 5.91*cos(x0) + 5.35*x0_t**2 + 4.55*x0*sin(x0) + 0.74*x0_t*sin(x0) + -3.28*sin(x0)**2\n",
      "exp([-2.5331])\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.458614349365234\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.457881927490234\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.45712661743164\n",
      "\n",
      "\n",
      "Stage  99\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.45635223388672\n",
      "expression length:\t 8\n",
      "Result stage 101: 3.22*x0 + -3.45*x0_t + -0.88*sin(x0) + 5.88*cos(x0) + 5.31*x0_t**2 + 4.5*x0*sin(x0) + 0.7*x0_t*sin(x0) + -3.25*sin(x0)**2\n",
      "exp([-2.4826])\n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.45555114746094\n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.45474624633789\n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.45392608642578\n",
      "\n",
      "\n",
      "Stage  100\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.453094482421875\n",
      "expression length:\t 8\n",
      "Result stage 102: 3.18*x0 + -3.41*x0_t + -0.84*sin(x0) + 5.86*cos(x0) + 5.26*x0_t**2 + 4.44*x0*sin(x0) + 0.66*x0_t*sin(x0) + -3.21*sin(x0)**2\n",
      "exp([-2.432])\n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.45224380493164\n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.451377868652344\n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.45049285888672\n",
      "\n",
      "\n",
      "Stage  101\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.44959259033203\n",
      "expression length:\t 8\n",
      "Result stage 103: 3.14*x0 + -3.37*x0_t + -0.8*sin(x0) + 5.84*cos(x0) + 5.22*x0_t**2 + 4.38*x0*sin(x0) + 0.62*x0_t*sin(x0) + -3.17*sin(x0)**2\n",
      "exp([-2.3807])\n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.44866180419922\n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.447715759277344\n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.44674301147461\n",
      "\n",
      "\n",
      "Stage  102\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.44575500488281\n",
      "expression length:\t 8\n",
      "Result stage 104: 3.1*x0 + -3.33*x0_t + -0.75*sin(x0) + 5.82*cos(x0) + 5.17*x0_t**2 + 4.32*x0*sin(x0) + 0.58*x0_t*sin(x0) + -3.14*sin(x0)**2\n",
      "exp([-2.3286])\n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.444740295410156\n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.44370651245117\n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.44266128540039\n",
      "\n",
      "\n",
      "Stage  103\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.44159698486328\n",
      "expression length:\t 8\n",
      "Result stage 105: 3.06*x0 + -3.29*x0_t + -0.71*sin(x0) + 5.79*cos(x0) + 5.12*x0_t**2 + 4.26*x0*sin(x0) + 0.53*x0_t*sin(x0) + -3.1*sin(x0)**2\n",
      "exp([-2.2745])\n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.44050979614258\n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.43941116333008\n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.43829345703125\n",
      "\n",
      "\n",
      "Stage  104\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.43715286254883\n",
      "expression length:\t 8\n",
      "Result stage 106: 3.02*x0 + -3.25*x0_t + -0.67*sin(x0) + 5.77*cos(x0) + 5.08*x0_t**2 + 4.2*x0*sin(x0) + 0.49*x0_t*sin(x0) + -3.07*sin(x0)**2\n",
      "exp([-2.2195])\n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.43599319458008\n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.434814453125\n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.43360137939453\n",
      "\n",
      "\n",
      "Stage  105\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.43238067626953\n",
      "expression length:\t 8\n",
      "Result stage 107: 2.98*x0 + -3.21*x0_t + -0.62*sin(x0) + 5.75*cos(x0) + 5.03*x0_t**2 + 4.14*x0*sin(x0) + 0.45*x0_t*sin(x0) + -3.03*sin(x0)**2\n",
      "exp([-2.1637])\n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.43112564086914\n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.42982864379883\n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.428497314453125\n",
      "\n",
      "\n",
      "Stage  106\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.42713165283203\n",
      "expression length:\t 8\n",
      "Result stage 108: 2.93*x0 + -3.17*x0_t + -0.58*sin(x0) + 5.73*cos(x0) + 4.98*x0_t**2 + 4.08*x0*sin(x0) + 0.41*x0_t*sin(x0) + -3.0*sin(x0)**2\n",
      "exp([-2.1057])\n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.42573165893555\n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.424312591552734\n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.42285919189453\n",
      "\n",
      "\n",
      "Stage  107\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4213752746582\n",
      "expression length:\t 8\n",
      "Result stage 109: 2.89*x0 + -3.13*x0_t + -0.54*sin(x0) + 5.71*cos(x0) + 4.94*x0_t**2 + 4.01*x0*sin(x0) + 0.37*x0_t*sin(x0) + -2.97*sin(x0)**2\n",
      "exp([-2.0462])\n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.41985321044922\n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.41830825805664\n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.416709899902344\n",
      "\n",
      "\n",
      "Stage  108\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.41508102416992\n",
      "expression length:\t 8\n",
      "Result stage 110: 2.85*x0 + -3.09*x0_t + -0.5*sin(x0) + 5.69*cos(x0) + 4.89*x0_t**2 + 3.95*x0*sin(x0) + 0.32*x0_t*sin(x0) + -2.93*sin(x0)**2\n",
      "exp([-1.9853])\n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.41339111328125\n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.411651611328125\n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.40988540649414\n",
      "\n",
      "\n",
      "Stage  109\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.4080696105957\n",
      "expression length:\t 8\n",
      "Result stage 111: 2.81*x0 + -3.05*x0_t + -0.45*sin(x0) + 5.67*cos(x0) + 4.84*x0_t**2 + 3.89*x0*sin(x0) + 0.28*x0_t*sin(x0) + -2.9*sin(x0)**2\n",
      "exp([-1.9225])\n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.406192779541016\n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.404258728027344\n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.40226745605469\n",
      "\n",
      "\n",
      "Stage  110\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.400203704833984\n",
      "expression length:\t 8\n",
      "Result stage 112: 2.77*x0 + -3.01*x0_t + -0.41*sin(x0) + 5.66*cos(x0) + 4.79*x0_t**2 + 3.82*x0*sin(x0) + 0.24*x0_t*sin(x0) + -2.86*sin(x0)**2\n",
      "exp([-1.8572])\n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.39806365966797\n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.3958740234375\n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.393611907958984\n",
      "\n",
      "\n",
      "Stage  111\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.391273498535156\n",
      "expression length:\t 8\n",
      "Result stage 113: 2.73*x0 + -2.97*x0_t + -0.37*sin(x0) + 5.64*cos(x0) + 4.74*x0_t**2 + 3.76*x0*sin(x0) + 0.2*x0_t*sin(x0) + -2.83*sin(x0)**2\n",
      "exp([-1.789])\n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.38887023925781\n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.3863639831543\n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.3837890625\n",
      "\n",
      "\n",
      "Stage  112\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.381107330322266\n",
      "expression length:\t 8\n",
      "Result stage 114: 2.69*x0 + -2.93*x0_t + -0.32*sin(x0) + 5.62*cos(x0) + 4.69*x0_t**2 + 3.69*x0*sin(x0) + 0.15*x0_t*sin(x0) + -2.8*sin(x0)**2\n",
      "exp([-1.7176])\n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.378326416015625\n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.37541961669922\n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.372371673583984\n",
      "\n",
      "\n",
      "Stage  113\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.36919403076172\n",
      "expression length:\t 8\n",
      "Result stage 115: 2.65*x0 + -2.89*x0_t + -0.28*sin(x0) + 5.61*cos(x0) + 4.64*x0_t**2 + 3.62*x0*sin(x0) + 0.11*x0_t*sin(x0) + -2.77*sin(x0)**2\n",
      "exp([-1.6418])\n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.36589813232422\n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.362483978271484\n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.35891342163086\n",
      "\n",
      "\n",
      "Stage  114\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.35518264770508\n",
      "expression length:\t 7\n",
      "Result stage 116: 2.61*x0 + -2.85*x0_t + -0.24*sin(x0) + 5.6*cos(x0) + 4.58*x0_t**2 + 3.55*x0*sin(x0) + -2.74*sin(x0)**2\n",
      "exp([-1.5605])\n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.35127639770508\n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.34717559814453\n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.34283447265625\n",
      "\n",
      "\n",
      "Stage  115\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.33826446533203\n",
      "expression length:\t 7\n",
      "Result stage 117: 2.57*x0 + -2.81*x0_t + -0.2*sin(x0) + 5.59*cos(x0) + 4.52*x0_t**2 + 3.48*x0*sin(x0) + -2.71*sin(x0)**2\n",
      "exp([-1.4727])\n",
      "\n",
      "\n",
      "Stage  116\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.33340835571289\n",
      "\n",
      "\n",
      "Stage  116\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.328250885009766\n",
      "\n",
      "\n",
      "Stage  116\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.32276916503906\n",
      "\n",
      "\n",
      "Stage  116\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.31695556640625\n",
      "expression length:\t 7\n",
      "Result stage 118: 2.53*x0 + -2.77*x0_t + -0.16*sin(x0) + 5.58*cos(x0) + 4.46*x0_t**2 + 3.41*x0*sin(x0) + -2.68*sin(x0)**2\n",
      "exp([-1.376])\n",
      "\n",
      "\n",
      "Stage  117\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.31076431274414\n",
      "\n",
      "\n",
      "Stage  117\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.30415725708008\n",
      "\n",
      "\n",
      "Stage  117\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.297054290771484\n",
      "\n",
      "\n",
      "Stage  117\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.28934860229492\n",
      "expression length:\t 7\n",
      "Result stage 119: 2.48*x0 + -2.73*x0_t + -0.11*sin(x0) + 5.58*cos(x0) + 4.4*x0_t**2 + 3.33*x0*sin(x0) + -2.65*sin(x0)**2\n",
      "exp([-1.2671])\n",
      "\n",
      "\n",
      "Stage  118\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.281005859375\n",
      "\n",
      "\n",
      "Stage  118\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.27183532714844\n",
      "\n",
      "\n",
      "Stage  118\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.26179504394531\n",
      "\n",
      "\n",
      "Stage  118\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.25061798095703\n",
      "expression length:\t 6\n",
      "Result stage 120: 2.44*x0 + -2.69*x0_t + 5.57*cos(x0) + 4.34*x0_t**2 + 3.25*x0*sin(x0) + -2.63*sin(x0)**2\n",
      "exp([-1.1396])\n",
      "\n",
      "\n",
      "Stage  119\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.23811340332031\n",
      "\n",
      "\n",
      "Stage  119\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.22408676147461\n",
      "\n",
      "\n",
      "Stage  119\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.20796203613281\n",
      "\n",
      "\n",
      "Stage  119\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.18899917602539\n",
      "expression length:\t 6\n",
      "Result stage 121: 2.4*x0 + -2.65*x0_t + 5.58*cos(x0) + 4.27*x0_t**2 + 3.16*x0*sin(x0) + -2.6*sin(x0)**2\n",
      "exp([-0.9802])\n",
      "\n",
      "\n",
      "Stage  120\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.16623306274414\n",
      "\n",
      "\n",
      "Stage  120\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.13833236694336\n",
      "\n",
      "\n",
      "Stage  120\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.102806091308594\n",
      "\n",
      "\n",
      "Stage  120\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  47.05488204956055\n",
      "expression length:\t 6\n",
      "Result stage 122: 2.36*x0 + -2.61*x0_t + 5.59*cos(x0) + 4.19*x0_t**2 + 3.06*x0*sin(x0) + -2.58*sin(x0)**2\n",
      "exp([-0.7472])\n",
      "\n",
      "\n",
      "Stage  121\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  46.983917236328125\n",
      "\n",
      "\n",
      "Stage  121\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  46.8593635559082\n",
      "\n",
      "\n",
      "Stage  121\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  46.51724624633789\n",
      "\n",
      "\n",
      "Stage  121\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  31.072296142578125\n",
      "expression length:\t 6\n",
      "Result stage 123: 2.16*x0 + -2.57*x0_t + 5.79*cos(x0) + 4.04*x0_t**2 + 2.76*x0*sin(x0) + -2.63*sin(x0)**2\n",
      "exp([0.3003])\n",
      "\n",
      "\n",
      "Stage  122\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  25.997610092163086\n",
      "\n",
      "\n",
      "Stage  122\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  23.193832397460938\n",
      "\n",
      "\n",
      "Stage  122\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  21.500125885009766\n",
      "\n",
      "\n",
      "Stage  122\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  20.27718162536621\n",
      "expression length:\t 6\n",
      "Result stage 124: 0.78*x0 + -2.53*x0_t + 6.74*cos(x0) + 3.38*x0_t**2 + 1.88*x0*sin(x0) + -2.59*sin(x0)**2\n",
      "exp([0.282])\n",
      "\n",
      "\n",
      "Stage  123\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  19.222352981567383\n",
      "\n",
      "\n",
      "Stage  123\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  18.169767379760742\n",
      "\n",
      "\n",
      "Stage  123\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  16.99222755432129\n",
      "\n",
      "\n",
      "Stage  123\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  15.529428482055664\n",
      "expression length:\t 6\n",
      "Result stage 125: 0.37*x0 + -2.49*x0_t + 7.31*cos(x0) + 2.28*x0_t**2 + 1.45*x0*sin(x0) + -2.39*sin(x0)**2\n",
      "exp([0.2239])\n",
      "\n",
      "\n",
      "Stage  124\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  13.45157527923584\n",
      "\n",
      "\n",
      "Stage  124\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  9.663192749023438\n",
      "\n",
      "\n",
      "Stage  124\n",
      "Epoch 150/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  1.144126534461975\n",
      "\n",
      "\n",
      "Stage  124\n",
      "Epoch 200/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.8416386246681213\n",
      "expression length:\t 6\n",
      "Result stage 126: 0.14*x0 + -2.45*x0_t + 7.72*cos(x0) + 0.38*x0_t**2 + 1.22*x0*sin(x0) + -2.08*sin(x0)**2\n",
      "exp([-0.0039])\n",
      "\n",
      "\n",
      "Stage  125\n",
      "Epoch 50/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.641373336315155\n",
      "\n",
      "\n",
      "Stage  125\n",
      "Epoch 100/200\n",
      "Learning rate :  1e-05\n",
      "Average loss :  0.5025649666786194\n",
      "\n",
      "\n",
      "Stage  125\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.4449532926082611\n",
      "\n",
      "\n",
      "Stage  125\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.42643243074417114\n",
      "expression length:\t 5\n",
      "Result stage 127: -2.43*x0_t + 7.74*cos(x0) + 0.37*x0_t**2 + 1.31*x0*sin(x0) + -1.81*sin(x0)**2\n",
      "exp([-0.0005])\n",
      "\n",
      "\n",
      "Stage  126\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.4068045914173126\n",
      "\n",
      "\n",
      "Stage  126\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.3914985656738281\n",
      "\n",
      "\n",
      "Stage  126\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.3771919310092926\n",
      "\n",
      "\n",
      "Stage  126\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.36393001675605774\n",
      "expression length:\t 5\n",
      "Result stage 128: -2.42*x0_t + 7.75*cos(x0) + 0.36*x0_t**2 + 1.33*x0*sin(x0) + -1.74*sin(x0)**2\n",
      "exp([-0.0005])\n",
      "\n",
      "\n",
      "Stage  127\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.35075652599334717\n",
      "\n",
      "\n",
      "Stage  127\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.3394700586795807\n",
      "\n",
      "\n",
      "Stage  127\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.3289946913719177\n",
      "\n",
      "\n",
      "Stage  127\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.31925374269485474\n",
      "expression length:\t 5\n",
      "Result stage 129: -2.41*x0_t + 7.75*cos(x0) + 0.36*x0_t**2 + 1.34*x0*sin(x0) + -1.68*sin(x0)**2\n",
      "exp([-0.0004])\n",
      "\n",
      "\n",
      "Stage  128\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.3095471262931824\n",
      "\n",
      "\n",
      "Stage  128\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.301157146692276\n",
      "\n",
      "\n",
      "Stage  128\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2934327721595764\n",
      "\n",
      "\n",
      "Stage  128\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2862705588340759\n",
      "expression length:\t 5\n",
      "Result stage 130: -2.41*x0_t + 7.76*cos(x0) + 0.36*x0_t**2 + 1.35*x0*sin(x0) + -1.62*sin(x0)**2\n",
      "exp([-0.0003])\n",
      "\n",
      "\n",
      "Stage  129\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2791541516780853\n",
      "\n",
      "\n",
      "Stage  129\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.27302542328834534\n",
      "\n",
      "\n",
      "Stage  129\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2673667371273041\n",
      "\n",
      "\n",
      "Stage  129\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.26209333539009094\n",
      "expression length:\t 5\n",
      "Result stage 131: -2.4*x0_t + 7.77*cos(x0) + 0.36*x0_t**2 + 1.35*x0*sin(x0) + -1.57*sin(x0)**2\n",
      "exp([-0.0003])\n",
      "\n",
      "\n",
      "Stage  130\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2568909227848053\n",
      "\n",
      "\n",
      "Stage  130\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.25236114859580994\n",
      "\n",
      "\n",
      "Stage  130\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.24812331795692444\n",
      "\n",
      "\n",
      "Stage  130\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.24423909187316895\n",
      "expression length:\t 5\n",
      "Result stage 132: -2.39*x0_t + 7.77*cos(x0) + 0.36*x0_t**2 + 1.35*x0*sin(x0) + -1.53*sin(x0)**2\n",
      "exp([-0.0002])\n",
      "\n",
      "\n",
      "Stage  131\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2403995543718338\n",
      "\n",
      "\n",
      "Stage  131\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.23700058460235596\n",
      "\n",
      "\n",
      "Stage  131\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.233816996216774\n",
      "\n",
      "\n",
      "Stage  131\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2308652549982071\n",
      "expression length:\t 5\n",
      "Result stage 133: -2.38*x0_t + 7.78*cos(x0) + 0.36*x0_t**2 + 1.35*x0*sin(x0) + -1.49*sin(x0)**2\n",
      "exp([-0.0002])\n",
      "\n",
      "\n",
      "Stage  132\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2279224842786789\n",
      "\n",
      "\n",
      "Stage  132\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.22533191740512848\n",
      "\n",
      "\n",
      "Stage  132\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.22287631034851074\n",
      "\n",
      "\n",
      "Stage  132\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.22056075930595398\n",
      "expression length:\t 5\n",
      "Result stage 134: -2.38*x0_t + 7.78*cos(x0) + 0.36*x0_t**2 + 1.34*x0*sin(x0) + -1.46*sin(x0)**2\n",
      "exp([-1.e-04])\n",
      "\n",
      "\n",
      "Stage  133\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.21820582449436188\n",
      "\n",
      "\n",
      "Stage  133\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.21613238751888275\n",
      "\n",
      "\n",
      "Stage  133\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2141686975955963\n",
      "\n",
      "\n",
      "Stage  133\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2123226523399353\n",
      "expression length:\t 5\n",
      "Result stage 135: -2.37*x0_t + 7.79*cos(x0) + 0.36*x0_t**2 + 1.33*x0*sin(x0) + -1.43*sin(x0)**2\n",
      "exp([-1.e-04])\n",
      "\n",
      "\n",
      "Stage  134\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.21043498814105988\n",
      "\n",
      "\n",
      "Stage  134\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2087336629629135\n",
      "\n",
      "\n",
      "Stage  134\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2071014791727066\n",
      "\n",
      "\n",
      "Stage  134\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.20554107427597046\n",
      "expression length:\t 5\n",
      "Result stage 136: -2.36*x0_t + 7.79*cos(x0) + 0.36*x0_t**2 + 1.32*x0*sin(x0) + -1.4*sin(x0)**2\n",
      "exp([-1.e-04])\n",
      "\n",
      "\n",
      "Stage  135\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.2039755880832672\n",
      "\n",
      "\n",
      "Stage  135\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.20254774391651154\n",
      "\n",
      "\n",
      "Stage  135\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.20118145644664764\n",
      "\n",
      "\n",
      "Stage  135\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.19986407458782196\n",
      "expression length:\t 5\n",
      "Result stage 137: -2.35*x0_t + 7.8*cos(x0) + 0.36*x0_t**2 + 1.31*x0*sin(x0) + -1.38*sin(x0)**2\n",
      "exp([-0.])\n",
      "\n",
      "\n",
      "Stage  136\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.19850192964076996\n",
      "\n",
      "\n",
      "Stage  136\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.19726471602916718\n",
      "\n",
      "\n",
      "Stage  136\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1960974931716919\n",
      "\n",
      "\n",
      "Stage  136\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.19496379792690277\n",
      "expression length:\t 5\n",
      "Result stage 138: -2.35*x0_t + 7.8*cos(x0) + 0.36*x0_t**2 + 1.29*x0*sin(x0) + -1.36*sin(x0)**2\n",
      "exp([-0.])\n",
      "\n",
      "\n",
      "Stage  137\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.19378575682640076\n",
      "\n",
      "\n",
      "Stage  137\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.19269543886184692\n",
      "\n",
      "\n",
      "Stage  137\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.19162045419216156\n",
      "\n",
      "\n",
      "Stage  137\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.19055111706256866\n",
      "expression length:\t 5\n",
      "Result stage 139: -2.34*x0_t + 7.8*cos(x0) + 0.36*x0_t**2 + 1.28*x0*sin(x0) + -1.33*sin(x0)**2\n",
      "exp([-0.])\n",
      "\n",
      "\n",
      "Stage  138\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.18943892419338226\n",
      "\n",
      "\n",
      "Stage  138\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1884104311466217\n",
      "\n",
      "\n",
      "Stage  138\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.18740391731262207\n",
      "\n",
      "\n",
      "Stage  138\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1864176094532013\n",
      "expression length:\t 5\n",
      "Result stage 140: -2.33*x0_t + 7.8*cos(x0) + 0.36*x0_t**2 + 1.26*x0*sin(x0) + -1.31*sin(x0)**2\n",
      "exp([-0.])\n",
      "\n",
      "\n",
      "Stage  139\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.18540717661380768\n",
      "\n",
      "\n",
      "Stage  139\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.18446972966194153\n",
      "\n",
      "\n",
      "Stage  139\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1835445761680603\n",
      "\n",
      "\n",
      "Stage  139\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.18262936174869537\n",
      "expression length:\t 5\n",
      "Result stage 141: -2.32*x0_t + 7.81*cos(x0) + 0.36*x0_t**2 + 1.25*x0*sin(x0) + -1.29*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  140\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.18167805671691895\n",
      "\n",
      "\n",
      "Stage  140\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.18080009520053864\n",
      "\n",
      "\n",
      "Stage  140\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.17993183434009552\n",
      "\n",
      "\n",
      "Stage  140\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1790691465139389\n",
      "expression length:\t 5\n",
      "Result stage 142: -2.32*x0_t + 7.81*cos(x0) + 0.36*x0_t**2 + 1.23*x0*sin(x0) + -1.28*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  141\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.17816369235515594\n",
      "\n",
      "\n",
      "Stage  141\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1773112416267395\n",
      "\n",
      "\n",
      "Stage  141\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.17647303640842438\n",
      "\n",
      "\n",
      "Stage  141\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.17564484477043152\n",
      "expression length:\t 5\n",
      "Result stage 143: -2.31*x0_t + 7.81*cos(x0) + 0.36*x0_t**2 + 1.21*x0*sin(x0) + -1.26*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  142\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.17478488385677338\n",
      "\n",
      "\n",
      "Stage  142\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.17398743331432343\n",
      "\n",
      "\n",
      "Stage  142\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.17319613695144653\n",
      "\n",
      "\n",
      "Stage  142\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.17241157591342926\n",
      "expression length:\t 5\n",
      "Result stage 144: -2.3*x0_t + 7.81*cos(x0) + 0.36*x0_t**2 + 1.2*x0*sin(x0) + -1.24*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  143\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1715850681066513\n",
      "\n",
      "\n",
      "Stage  143\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.17080552875995636\n",
      "\n",
      "\n",
      "Stage  143\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.17002706229686737\n",
      "\n",
      "\n",
      "Stage  143\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.16925467550754547\n",
      "expression length:\t 5\n",
      "Result stage 145: -2.3*x0_t + 7.81*cos(x0) + 0.36*x0_t**2 + 1.18*x0*sin(x0) + -1.23*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  144\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1684419959783554\n",
      "\n",
      "\n",
      "Stage  144\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1676774024963379\n",
      "\n",
      "\n",
      "Stage  144\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.16692574322223663\n",
      "\n",
      "\n",
      "Stage  144\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1661818027496338\n",
      "expression length:\t 5\n",
      "Result stage 146: -2.29*x0_t + 7.82*cos(x0) + 0.36*x0_t**2 + 1.16*x0*sin(x0) + -1.21*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  145\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1653946191072464\n",
      "\n",
      "\n",
      "Stage  145\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1646520346403122\n",
      "\n",
      "\n",
      "Stage  145\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1639096736907959\n",
      "\n",
      "\n",
      "Stage  145\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1631709337234497\n",
      "expression length:\t 5\n",
      "Result stage 147: -2.28*x0_t + 7.82*cos(x0) + 0.37*x0_t**2 + 1.14*x0*sin(x0) + -1.19*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  146\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.16240061819553375\n",
      "\n",
      "\n",
      "Stage  146\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.16168096661567688\n",
      "\n",
      "\n",
      "Stage  146\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1609647125005722\n",
      "\n",
      "\n",
      "Stage  146\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.16024872660636902\n",
      "expression length:\t 5\n",
      "Result stage 148: -2.27*x0_t + 7.82*cos(x0) + 0.37*x0_t**2 + 1.13*x0*sin(x0) + -1.18*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  147\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15949155390262604\n",
      "\n",
      "\n",
      "Stage  147\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15877483785152435\n",
      "\n",
      "\n",
      "Stage  147\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15806901454925537\n",
      "\n",
      "\n",
      "Stage  147\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15736894309520721\n",
      "expression length:\t 5\n",
      "Result stage 149: -2.27*x0_t + 7.82*cos(x0) + 0.37*x0_t**2 + 1.11*x0*sin(x0) + -1.16*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  148\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15663091838359833\n",
      "\n",
      "\n",
      "Stage  148\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15593892335891724\n",
      "\n",
      "\n",
      "Stage  148\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15524941682815552\n",
      "\n",
      "\n",
      "Stage  148\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15455923974514008\n",
      "expression length:\t 5\n",
      "Result stage 150: -2.26*x0_t + 7.83*cos(x0) + 0.37*x0_t**2 + 1.09*x0*sin(x0) + -1.15*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  149\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1538301557302475\n",
      "\n",
      "\n",
      "Stage  149\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1531452089548111\n",
      "\n",
      "\n",
      "Stage  149\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15246286988258362\n",
      "\n",
      "\n",
      "Stage  149\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15178316831588745\n",
      "expression length:\t 5\n",
      "Result stage 151: -2.25*x0_t + 7.83*cos(x0) + 0.37*x0_t**2 + 1.07*x0*sin(x0) + -1.13*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  150\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1510665863752365\n",
      "\n",
      "\n",
      "Stage  150\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.15039612352848053\n",
      "\n",
      "\n",
      "Stage  150\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.14972980320453644\n",
      "\n",
      "\n",
      "Stage  150\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1490694284439087\n",
      "expression length:\t 5\n",
      "Result stage 152: -2.24*x0_t + 7.83*cos(x0) + 0.37*x0_t**2 + 1.06*x0*sin(x0) + -1.12*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  151\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.14837995171546936\n",
      "\n",
      "\n",
      "Stage  151\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.14773331582546234\n",
      "\n",
      "\n",
      "Stage  151\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1470891535282135\n",
      "\n",
      "\n",
      "Stage  151\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.14644771814346313\n",
      "expression length:\t 5\n",
      "Result stage 153: -2.24*x0_t + 7.83*cos(x0) + 0.37*x0_t**2 + 1.04*x0*sin(x0) + -1.1*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  152\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1457715928554535\n",
      "\n",
      "\n",
      "Stage  152\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.14513641595840454\n",
      "\n",
      "\n",
      "Stage  152\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.14450344443321228\n",
      "\n",
      "\n",
      "Stage  152\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1438763588666916\n",
      "expression length:\t 5\n",
      "Result stage 154: -2.23*x0_t + 7.84*cos(x0) + 0.37*x0_t**2 + 1.02*x0*sin(x0) + -1.09*sin(x0)**2\n",
      "exp([0.])\n",
      "\n",
      "\n",
      "Stage  153\n",
      "Epoch 50/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.14320990443229675\n",
      "\n",
      "\n",
      "Stage  153\n",
      "Epoch 100/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.14257334172725677\n",
      "\n",
      "\n",
      "Stage  153\n",
      "Epoch 150/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1419404298067093\n",
      "\n",
      "\n",
      "Stage  153\n",
      "Epoch 200/200\n",
      "Learning rate :  2e-06\n",
      "Average loss :  0.1413092464208603\n",
      "expression length:\t 5\n",
      "Result stage 155: -2.22*x0_t + 7.84*cos(x0) + 0.37*x0_t**2 + 1.0*x0*sin(x0) + -1.08*sin(x0)**2\n",
      "exp([-0.])\n"
     ]
    }
   ],
   "source": [
    "for stage in range(500):\n",
    "\n",
    "    #Redefine computation after thresholding\n",
    "    Zeta, Eta, Delta = LagrangianLibraryTensor(X,Xdot,expr,states,states_dot,device,scaling=False)\n",
    "    Eta = Eta.to(device)\n",
    "    Zeta = Zeta.to(device)\n",
    "    Delta = Delta.to(device)\n",
    "\n",
    "    #Training\n",
    "    Epoch = 200\n",
    "    i = 1\n",
    "    lr = 1e-5\n",
    "    if(stage==1):\n",
    "        lam = 0\n",
    "    else:\n",
    "        lam = 0.1\n",
    "    temp = 1000\n",
    "    while(i<=Epoch):   \n",
    "        xi_L , c, prevxi_L, prec, lossitem, q= training_loop(xi_L,c,prevxi_L,prec,Zeta,Eta,Delta,Xdot,t,500,lr,lam)\n",
    "        if i %50 == 0:\n",
    "            print(\"\\n\")\n",
    "            print(\"Stage \",stage)\n",
    "            print(\"Epoch \"+str(i) + \"/\" + str(Epoch))\n",
    "            print(\"Learning rate : \", lr)\n",
    "            print(\"Average loss : \" , lossitem)\n",
    "        temp = lossitem\n",
    "        if temp <= 0.5:\n",
    "            lr = 2e-6\n",
    "        if(temp <=1e-6):\n",
    "            break\n",
    "        i+=1\n",
    "    \n",
    "    ## Thresholding small indices ##\n",
    "    threshold = 1e-1\n",
    "    surv_index = ((torch.abs(xi_L) >= threshold)).nonzero(as_tuple=True)[0].detach().cpu().numpy()\n",
    "    expr = np.array(expr)[surv_index].tolist()\n",
    "    xi_L =xi_L[surv_index].clone().detach().requires_grad_(True)\n",
    "    prevxi_L = xi_L.clone().detach()\n",
    "\n",
    "    ## obtaining analytical model\n",
    "    xi_Lcpu = np.around(xi_L.detach().cpu().numpy(),decimals=2)\n",
    "    c_cpu = c.detach().cpu().numpy()\n",
    "    L = HL.generateExpression(xi_Lcpu,expr)\n",
    "    print(\"expression length:\\t\",len(xi_L))\n",
    "    print(\"Result stage \" + str(stage+2) + \":\" , L)\n",
    "    print('exp({}*t)'.format(c_cpu))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
